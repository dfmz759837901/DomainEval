{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "wls_prediction_std", "method_path": "../srcdata/Computation/statsmodels/statsmodels/sandbox/regression/predstd.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\n\n\ndef wls_prediction_std(res, exog=None, weights=None, alpha=0.05): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\ndef wls_prediction_std(res, exog=None, weights=None, alpha=0.05):\n    \"\"\"calculate standard deviation and confidence interval for prediction\n\n    applies to WLS and OLS, not to general GLS,\n    that is independently but not identically distributed observations\n\n    Parameters\n    ----------\n    res : regression result instance\n        results of WLS or OLS regression required attributes see notes\n    exog : array_like (optional)\n        exogenous variables for points to predict\n    weights : scalar or array_like (optional)\n        weights as defined for WLS (inverse of variance of observation)\n    alpha : float (default: alpha = 0.05)\n        confidence level for two-sided hypothesis\n\n    Returns\n    -------\n    predstd : array_like, 1d\n        standard error of prediction\n        same length as rows of exog\n    interval_l, interval_u : array_like\n        lower und upper confidence bounds\n\n    Notes\n    -----\n    The result instance needs to have at least the following\n    res.model.predict() : predicted values or\n    res.fittedvalues : values used in estimation\n    res.cov_params() : covariance matrix of parameter estimates\n\n    If exog is 1d, then it is interpreted as one observation,\n    i.e. a row vector.\n\n    testing status: not compared with other packages\n\n    References\n    ----------\n\n    Greene p.111 for OLS, extended to WLS by analogy\n\n    \"\"\"\n    covb = res.cov_params()\n    if exog is None:\n        exog = res.model.exog\n        predicted = res.fittedvalues\n        if weights is None:\n            weights = res.model.weights\n    else:\n        exog = np.atleast_2d(exog)\n        if covb.shape[1] != exog.shape[1]:\n            raise ValueError('wrong shape of exog')\n        predicted = res.model.predict(res.params, exog)\n        if weights is None:\n            weights = 1.0\n        else:\n            weights = np.asarray(weights)\n            if weights.size > 1 and len(weights) != exog.shape[0]:\n                raise ValueError('weights and exog do not have matching shape')\n    predvar = res.mse_resid / weights + (exog * np.dot(covb, exog.T).T).sum(1)\n    predstd = np.sqrt(predvar)\n    tppf = stats.t.isf(alpha / 2.0, res.df_resid)\n    interval_u = predicted + tppf * predstd\n    interval_l = predicted - tppf * predstd\n    return predstd, interval_l, interval_u"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "load_pandas", "method_path": "../srcdata/Computation/statsmodels/statsmodels/datasets/macrodata/data.py", "method_code_mask": "from statsmodels.datasets import utils as du\n\n\ndef load_pandas(): [MASK]\n", "response": "from statsmodels.datasets import utils as du\ndef load_pandas():\n    data = _get_data()\n    return du.Dataset(data=data, names=list(data.columns))"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "local_fdr", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/multitest.py", "method_code_mask": "import numpy as np\nfrom statsmodels.stats._knockoff import RegressionFDR\nimport gc\nimport warnings\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import families\nfrom statsmodels.regression.linear_model import OLS\nfrom scipy.stats.distributions import norm\nfrom scipy.optimize import minimize\n\n\ndef local_fdr(zscores, null_proportion=1.0, null_pdf=None, deg=7, nbins=30,\n    alpha=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.stats._knockoff import RegressionFDR\nimport gc\nimport warnings\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import families\nfrom statsmodels.regression.linear_model import OLS\nfrom scipy.stats.distributions import norm\nfrom scipy.optimize import minimize\ndef local_fdr(zscores, null_proportion=1.0, null_pdf=None, deg=7, nbins=30,\n    alpha=0):\n    \"\"\"\n    Calculate local FDR values for a list of Z-scores.\n\n    Parameters\n    ----------\n    zscores : array_like\n        A vector of Z-scores\n    null_proportion : float\n        The assumed proportion of true null hypotheses\n    null_pdf : function mapping reals to positive reals\n        The density of null Z-scores; if None, use standard normal\n    deg : int\n        The maximum exponent in the polynomial expansion of the\n        density of non-null Z-scores\n    nbins : int\n        The number of bins for estimating the marginal density\n        of Z-scores.\n    alpha : float\n        Use Poisson ridge regression with parameter alpha to estimate\n        the density of non-null Z-scores.\n\n    Returns\n    -------\n    fdr : array_like\n        A vector of FDR values\n\n    References\n    ----------\n    B Efron (2008).  Microarrays, Empirical Bayes, and the Two-Groups\n    Model.  Statistical Science 23:1, 1-22.\n\n    Examples\n    --------\n    Basic use (the null Z-scores are taken to be standard normal):\n\n    >>> from statsmodels.stats.multitest import local_fdr\n    >>> import numpy as np\n    >>> zscores = np.random.randn(30)\n    >>> fdr = local_fdr(zscores)\n\n    Use a Gaussian null distribution estimated from the data:\n\n    >>> null = EmpiricalNull(zscores)\n    >>> fdr = local_fdr(zscores, null_pdf=null.pdf)\n    \"\"\"\n    from statsmodels.genmod.generalized_linear_model import GLM\n    from statsmodels.genmod.generalized_linear_model import families\n    from statsmodels.regression.linear_model import OLS\n    minz = min(zscores)\n    maxz = max(zscores)\n    bins = np.linspace(minz, maxz, nbins)\n    zhist = np.histogram(zscores, bins)[0]\n    zbins = (bins[:-1] + bins[1:]) / 2\n    dmat = np.vander(zbins, deg + 1)\n    sd = dmat.std(0)\n    ii = sd > 1e-08\n    dmat[:, ii] /= sd[ii]\n    start = OLS(np.log(1 + zhist), dmat).fit().params\n    if alpha > 0:\n        md = GLM(zhist, dmat, family=families.Poisson()).fit_regularized(L1_wt\n            =0, alpha=alpha, start_params=start)\n    else:\n        md = GLM(zhist, dmat, family=families.Poisson()).fit(start_params=start\n            )\n    dmat_full = np.vander(zscores, deg + 1)\n    dmat_full[:, ii] /= sd[ii]\n    fz = md.predict(dmat_full) / (len(zscores) * (bins[1] - bins[0]))\n    if null_pdf is None:\n        f0 = np.exp(-0.5 * zscores ** 2) / np.sqrt(2 * np.pi)\n    else:\n        f0 = null_pdf(zscores)\n    fdr = null_proportion * f0 / fz\n    fdr = np.clip(fdr, 0, 1)\n    return fdr"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "variance_inflation_factor", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/outliers_influence.py", "method_code_mask": "import warnings\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.python import lzip\nfrom collections import defaultdict\nimport numpy as np\nfrom statsmodels.graphics._regressionplots_doc import _plot_influence_doc\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.tools import maybe_unwrap_results\nfrom scipy import stats\nfrom pandas import DataFrame\nfrom statsmodels.graphics.regressionplots import _influence_plot\nfrom statsmodels.graphics import utils\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.sandbox.tools.cross_val import LeaveOneOut\nfrom copy import deepcopy\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.iolib.table import default_html_fmt\nfrom statsmodels.iolib.tableformatting import fmt_base\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\n\n\ndef variance_inflation_factor(exog, exog_idx): [MASK]\n", "response": "import warnings\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.python import lzip\nfrom collections import defaultdict\nimport numpy as np\nfrom statsmodels.graphics._regressionplots_doc import _plot_influence_doc\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.tools import maybe_unwrap_results\nfrom scipy import stats\nfrom pandas import DataFrame\nfrom statsmodels.graphics.regressionplots import _influence_plot\nfrom statsmodels.graphics import utils\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.sandbox.tools.cross_val import LeaveOneOut\nfrom copy import deepcopy\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.iolib.table import default_html_fmt\nfrom statsmodels.iolib.tableformatting import fmt_base\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\ndef variance_inflation_factor(exog, exog_idx):\n    \"\"\"\n    Variance inflation factor, VIF, for one exogenous variable\n\n    The variance inflation factor is a measure for the increase of the\n    variance of the parameter estimates if an additional variable, given by\n    exog_idx is added to the linear regression. It is a measure for\n    multicollinearity of the design matrix, exog.\n\n    One recommendation is that if VIF is greater than 5, then the explanatory\n    variable given by exog_idx is highly collinear with the other explanatory\n    variables, and the parameter estimates will have large standard errors\n    because of this.\n\n    Parameters\n    ----------\n    exog : {ndarray, DataFrame}\n        design matrix with all explanatory variables, as for example used in\n        regression\n    exog_idx : int\n        index of the exogenous variable in the columns of exog\n\n    Returns\n    -------\n    float\n        variance inflation factor\n\n    Notes\n    -----\n    This function does not save the auxiliary regression.\n\n    See Also\n    --------\n    xxx : class for regression diagnostics  TODO: does not exist yet\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Variance_inflation_factor\n    \"\"\"\n    k_vars = exog.shape[1]\n    exog = np.asarray(exog)\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n    vif = 1.0 / (1.0 - r_squared_i)\n    return vif"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "reset_ramsey", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/outliers_influence.py", "method_code_mask": "import warnings\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.python import lzip\nfrom collections import defaultdict\nimport numpy as np\nfrom statsmodels.graphics._regressionplots_doc import _plot_influence_doc\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.tools import maybe_unwrap_results\nfrom scipy import stats\nfrom pandas import DataFrame\nfrom statsmodels.graphics.regressionplots import _influence_plot\nfrom statsmodels.graphics import utils\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.sandbox.tools.cross_val import LeaveOneOut\nfrom copy import deepcopy\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.iolib.table import default_html_fmt\nfrom statsmodels.iolib.tableformatting import fmt_base\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\n\n\ndef reset_ramsey(res, degree=5): [MASK]\n", "response": "import warnings\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.python import lzip\nfrom collections import defaultdict\nimport numpy as np\nfrom statsmodels.graphics._regressionplots_doc import _plot_influence_doc\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.tools import maybe_unwrap_results\nfrom scipy import stats\nfrom pandas import DataFrame\nfrom statsmodels.graphics.regressionplots import _influence_plot\nfrom statsmodels.graphics import utils\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.sandbox.tools.cross_val import LeaveOneOut\nfrom copy import deepcopy\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.iolib.table import default_html_fmt\nfrom statsmodels.iolib.tableformatting import fmt_base\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\ndef reset_ramsey(res, degree=5):\n    \"\"\"Ramsey's RESET specification test for linear models\n\n    This is a general specification test, for additional non-linear effects\n    in a model.\n\n    Parameters\n    ----------\n    degree : int\n        Maximum power to include in the RESET test.  Powers 0 and 1 are\n        excluded, so that degree tests powers 2, ..., degree of the fitted\n        values.\n\n    Notes\n    -----\n    The test fits an auxiliary OLS regression where the design matrix, exog,\n    is augmented by powers 2 to degree of the fitted values. Then it performs\n    an F-test whether these additional terms are significant.\n\n    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\n    indicates that there might be additional non-linear effects in the model\n    and that the linear model is mis-specified.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Ramsey_RESET_test\n    \"\"\"\n    order = degree + 1\n    k_vars = res.model.exog.shape[1]\n    norm_values = np.asarray(res.fittedvalues)\n    norm_values = norm_values / np.sqrt((norm_values ** 2).mean())\n    y_fitted_vander = np.vander(norm_values, order)[:, :-2]\n    exog = np.column_stack((res.model.exog, y_fitted_vander))\n    exog /= np.sqrt((exog ** 2).mean(0))\n    endog = res.model.endog / (res.model.endog ** 2).mean()\n    res_aux = OLS(endog, exog).fit()\n    r_matrix = np.eye(degree - 1, exog.shape[1], k_vars)\n    return res_aux.f_test(r_matrix)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "confint_poisson_2indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/rates.py", "method_code_mask": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\n\n\ndef confint_poisson_2indep(count1, exposure1, count2, exposure2, method=\n    'score', compare='ratio', alpha=0.05, method_mover='score'): [MASK]\n", "response": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\ndef confint_poisson_2indep(count1, exposure1, count2, exposure2, method=\n    'score', compare='ratio', alpha=0.05, method_mover='score'):\n    \"\"\"Confidence interval for ratio or difference of 2 indep poisson rates.\n\n    Parameters\n    ----------\n    count1 : int\n        Number of events in first sample.\n    exposure1 : float\n        Total exposure (time * subjects) in first sample.\n    count2 : int\n        Number of events in second sample.\n    exposure2 : float\n        Total exposure (time * subjects) in second sample.\n    method : string\n        Method for the test statistic and the p-value. Defaults to `'score'`.\n        see Notes.\n\n        ratio:\n\n        - 'wald': NOT YET, method W1A, wald test, variance based on observed\n          rates\n        - 'waldcc' :\n        - 'score': method W2A, score test, variance based on estimate under\n          the Null hypothesis\n        - 'wald-log': W3A, uses log-ratio, variance based on observed rates\n        - 'score-log' W4A, uses log-ratio, variance based on estimate under\n          the Null hypothesis\n        - 'sqrt': W5A, based on variance stabilizing square root transformation\n        - 'sqrtcc' :\n        - 'exact-cond': NOT YET, exact conditional test based on binomial\n          distribution\n          This uses ``binom_test`` which is minlike in the two-sided case.\n        - 'cond-midp': NOT YET, midpoint-pvalue of exact conditional test\n        - 'mover' :\n\n        diff:\n\n        - 'wald',\n        - 'waldccv'\n        - 'score'\n        - 'mover'\n\n    compare : {'diff', 'ratio'}\n        Default is \"ratio\".\n        If compare is `diff`, then the hypothesis test is for\n        diff = rate1 - rate2.\n        If compare is `ratio`, then the hypothesis test is for the\n        rate ratio defined by ratio = rate1 / rate2.\n    alternative : string\n        The alternative hypothesis, H1, has to be one of the following\n\n        - 'two-sided': H1: ratio of rates is not equal to ratio_null (default)\n        - 'larger' :   H1: ratio of rates is larger than ratio_null\n        - 'smaller' :  H1: ratio of rates is smaller than ratio_null\n\n    alpha : float in (0, 1)\n        Significance level, nominal coverage of the confidence interval is\n        1 - alpha.\n\n    Returns\n    -------\n    tuple (low, upp) : confidence limits.\n\n    \"\"\"\n    y1, n1, y2, n2 = map(np.asarray, [count1, exposure1, count2, exposure2])\n    rate1, rate2 = y1 / n1, y2 / n2\n    alpha = alpha / 2\n    if compare == 'ratio':\n        if method == 'score':\n            low, upp = _invert_test_confint_2indep(count1, exposure1,\n                count2, exposure2, alpha=alpha * 2, method='score', compare\n                ='ratio', method_start='waldcc')\n            ci = low, upp\n        elif method == 'wald-log':\n            crit = stats.norm.isf(alpha)\n            c = 0\n            center = (count1 + c) / (count2 + c) * n2 / n1\n            std = np.sqrt(1 / (count1 + c) + 1 / (count2 + c))\n            ci = center * np.exp(-crit * std), center * np.exp(crit * std)\n        elif method == 'score-log':\n            low, upp = _invert_test_confint_2indep(count1, exposure1,\n                count2, exposure2, alpha=alpha * 2, method='score-log',\n                compare='ratio', method_start='waldcc')\n            ci = low, upp\n        elif method == 'waldcc':\n            crit = stats.norm.isf(alpha)\n            center = (count1 + 0.5) / (count2 + 0.5) * n2 / n1\n            std = np.sqrt(1 / (count1 + 0.5) + 1 / (count2 + 0.5))\n            ci = center * np.exp(-crit * std), center * np.exp(crit * std)\n        elif method == 'sqrtcc':\n            crit = stats.norm.isf(alpha)\n            center = np.sqrt((count1 + 0.5) * (count2 + 0.5))\n            std = 0.5 * np.sqrt(count1 + 0.5 + count2 + 0.5 - 0.25 * crit)\n            denom = count2 + 0.5 - 0.25 * crit ** 2\n            low_sqrt = (center - crit * std) / denom\n            upp_sqrt = (center + crit * std) / denom\n            ci = low_sqrt ** 2, upp_sqrt ** 2\n        elif method == 'mover':\n            method_p = method_mover\n            ci1 = confint_poisson(y1, n1, method=method_p, alpha=2 * alpha)\n            ci2 = confint_poisson(y2, n2, method=method_p, alpha=2 * alpha)\n            ci = _mover_confint(rate1, rate2, ci1, ci2, contrast='ratio')\n        else:\n            raise ValueError(f'method \"{method}\" not recognized')\n        ci = np.maximum(ci[0], 0), ci[1]\n    elif compare == 'diff':\n        if method in ['wald']:\n            crit = stats.norm.isf(alpha)\n            center = rate1 - rate2\n            half = crit * np.sqrt(rate1 / n1 + rate2 / n2)\n            ci = center - half, center + half\n        elif method in ['waldccv']:\n            crit = stats.norm.isf(alpha)\n            center = rate1 - rate2\n            std = np.sqrt((count1 + 0.5) / n1 ** 2 + (count2 + 0.5) / n2 ** 2)\n            half = crit * std\n            ci = center - half, center + half\n        elif method == 'score':\n            low, upp = _invert_test_confint_2indep(count1, exposure1,\n                count2, exposure2, alpha=alpha * 2, method='score', compare\n                ='diff', method_start='waldccv')\n            ci = low, upp\n        elif method == 'mover':\n            method_p = method_mover\n            ci1 = confint_poisson(y1, n1, method=method_p, alpha=2 * alpha)\n            ci2 = confint_poisson(y2, n2, method=method_p, alpha=2 * alpha)\n            ci = _mover_confint(rate1, rate2, ci1, ci2, contrast='diff')\n        else:\n            raise ValueError(f'method \"{method}\" not recognized')\n    else:\n        raise NotImplementedError('\"compare\" needs to be ratio or diff')\n    return ci"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "linear_lm", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/diagnostic.py", "method_code_mask": "from statsmodels.compat.pandas import deprecate_kwarg\nfrom collections.abc import Iterable\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import RegressionResultsWrapper\nfrom statsmodels.stats._adnorm import anderson_statistic\nfrom statsmodels.stats._adnorm import normal_ad\nfrom statsmodels.stats._lilliefors import kstest_exponential\nfrom statsmodels.stats._lilliefors import kstest_fit\nfrom statsmodels.stats._lilliefors import kstest_normal\nfrom statsmodels.stats._lilliefors import lilliefors\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.multivariate.pca import PCA\nfrom scipy.spatial.distance import cdist\n\n\ndef linear_lm(resid, exog, func=None): [MASK]\n", "response": "from statsmodels.compat.pandas import deprecate_kwarg\nfrom collections.abc import Iterable\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import RegressionResultsWrapper\nfrom statsmodels.stats._adnorm import anderson_statistic\nfrom statsmodels.stats._adnorm import normal_ad\nfrom statsmodels.stats._lilliefors import kstest_exponential\nfrom statsmodels.stats._lilliefors import kstest_fit\nfrom statsmodels.stats._lilliefors import kstest_normal\nfrom statsmodels.stats._lilliefors import lilliefors\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.multivariate.pca import PCA\nfrom scipy.spatial.distance import cdist\ndef linear_lm(resid, exog, func=None):\n    \"\"\"\n    Lagrange multiplier test for linearity against functional alternative\n\n    # TODO: Remove the restriction\n    limitations: Assumes currently that the first column is integer.\n    Currently it does not check whether the transformed variables contain NaNs,\n    for example log of negative number.\n\n    Parameters\n    ----------\n    resid : ndarray\n        residuals of a regression\n    exog : ndarray\n        exogenous variables for which linearity is tested\n    func : callable, default None\n        If func is None, then squares are used. func needs to take an array\n        of exog and return an array of transformed variables.\n\n    Returns\n    -------\n    lm : float\n       Lagrange multiplier test statistic\n    lm_pval : float\n       p-value of Lagrange multiplier tes\n    ftest : ContrastResult instance\n       the results from the F test variant of this test\n\n    Notes\n    -----\n    Written to match Gretl's linearity test. The test runs an auxiliary\n    regression of the residuals on the combined original and transformed\n    regressors. The Null hypothesis is that the linear specification is\n    correct.\n    \"\"\"\n    if func is None:\n\n        def func(x):\n            return np.power(x, 2)\n    exog = np.asarray(exog)\n    exog_aux = np.column_stack((exog, func(exog[:, 1:])))\n    nobs, k_vars = exog.shape\n    ls = OLS(resid, exog_aux).fit()\n    ftest = ls.f_test(np.eye(k_vars - 1, k_vars * 2 - 1, k_vars))\n    lm = nobs * ls.rsquared\n    lm_pval = stats.chi2.sf(lm, k_vars - 1)\n    return lm, lm_pval, ftest"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "burg", "method_path": "../srcdata/Computation/statsmodels/statsmodels/regression/linear_model.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nfrom typing import Literal\nfrom collections.abc import Sequence\nimport warnings\nimport numpy as np\nfrom scipy import optimize\nfrom scipy import stats\nfrom scipy.linalg import cholesky\nfrom scipy.linalg import toeplitz\nfrom scipy.linalg.lapack import dtrtri\nimport statsmodels.base.model as base\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.emplike.elregress import _ELRegOpts\nfrom statsmodels.regression._prediction import PredictionResults\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.decorators import cache_writable\nfrom statsmodels.tools.sm_exceptions import InvalidTestWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import pinv_extended\nfrom statsmodels.tools.typing import Float64Array\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import string_like\nfrom scipy.stats.distributions import norm\nfrom statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.base.elastic_net import RegularizedResultsWrapper\nfrom statsmodels.base.elastic_net import fit_elasticnet\nfrom statsmodels.tsa.stattools import levinson_durbin_pacf\nfrom statsmodels.tsa.stattools import pacf_burg\nfrom statsmodels.tools.eval_measures import aicc\nfrom statsmodels.tools.eval_measures import hqic\nfrom numpy.linalg import inv\nimport statsmodels.stats.sandwich_covariance as sw\nfrom statsmodels.base.covtype import descriptions\nfrom statsmodels.base.covtype import normalize_cov_type\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.stats.stattools import jarque_bera\nfrom statsmodels.stats.stattools import omni_normtest\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib import summary2\nfrom statsmodels.stats.outliers_influence import OLSInfluence\nfrom statsmodels.stats.outliers_influence import outlier_test\n\n\ndef burg(endog, order=1, demean=True): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nfrom typing import Literal\nfrom collections.abc import Sequence\nimport warnings\nimport numpy as np\nfrom scipy import optimize\nfrom scipy import stats\nfrom scipy.linalg import cholesky\nfrom scipy.linalg import toeplitz\nfrom scipy.linalg.lapack import dtrtri\nimport statsmodels.base.model as base\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.emplike.elregress import _ELRegOpts\nfrom statsmodels.regression._prediction import PredictionResults\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.decorators import cache_writable\nfrom statsmodels.tools.sm_exceptions import InvalidTestWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import pinv_extended\nfrom statsmodels.tools.typing import Float64Array\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import string_like\nfrom scipy.stats.distributions import norm\nfrom statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.base.elastic_net import RegularizedResultsWrapper\nfrom statsmodels.base.elastic_net import fit_elasticnet\nfrom statsmodels.tsa.stattools import levinson_durbin_pacf\nfrom statsmodels.tsa.stattools import pacf_burg\nfrom statsmodels.tools.eval_measures import aicc\nfrom statsmodels.tools.eval_measures import hqic\nfrom numpy.linalg import inv\nimport statsmodels.stats.sandwich_covariance as sw\nfrom statsmodels.base.covtype import descriptions\nfrom statsmodels.base.covtype import normalize_cov_type\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.stats.stattools import jarque_bera\nfrom statsmodels.stats.stattools import omni_normtest\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib import summary2\nfrom statsmodels.stats.outliers_influence import OLSInfluence\nfrom statsmodels.stats.outliers_influence import outlier_test\ndef burg(endog, order=1, demean=True):\n    \"\"\"\n    Compute Burg's AP(p) parameter estimator.\n\n    Parameters\n    ----------\n    endog : array_like\n        The endogenous variable.\n    order : int, optional\n        Order of the AR.  Default is 1.\n    demean : bool, optional\n        Flag indicating to subtract the mean from endog before estimation.\n\n    Returns\n    -------\n    rho : ndarray\n        The AR(p) coefficients computed using Burg's algorithm.\n    sigma2 : float\n        The estimate of the residual variance.\n\n    See Also\n    --------\n    yule_walker : Estimate AR parameters using the Yule-Walker method.\n\n    Notes\n    -----\n    AR model estimated includes a constant that is estimated using the sample\n    mean (see [1]_). This value is not reported.\n\n    References\n    ----------\n    .. [1] Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series\n        and forecasting. Springer.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> from statsmodels.datasets.sunspots import load\n    >>> data = load()\n    >>> rho, sigma2 = sm.regression.linear_model.burg(data.endog, order=4)\n\n    >>> rho\n    array([ 1.30934186, -0.48086633, -0.20185982,  0.05501941])\n    >>> sigma2\n    271.2467306963966\n    \"\"\"\n    from statsmodels.tsa.stattools import levinson_durbin_pacf, pacf_burg\n    endog = np.squeeze(np.asarray(endog))\n    if endog.ndim != 1:\n        raise ValueError('endog must be 1-d or squeezable to 1-d.')\n    order = int(order)\n    if order < 1:\n        raise ValueError('order must be an integer larger than 1')\n    if demean:\n        endog = endog - endog.mean()\n    pacf, sigma = pacf_burg(endog, order, demean=demean)\n    ar, _ = levinson_durbin_pacf(pacf)\n    return ar, sigma[-1]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "corr_nearest", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/correlation_tools.py", "method_code_mask": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\n\n\ndef corr_nearest(corr, threshold=1e-15, n_fact=100): [MASK]\n", "response": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\ndef corr_nearest(corr, threshold=1e-15, n_fact=100):\n    \"\"\"\n    Find the nearest correlation matrix that is positive semi-definite.\n\n    The function iteratively adjust the correlation matrix by clipping the\n    eigenvalues of a difference matrix. The diagonal elements are set to one.\n\n    Parameters\n    ----------\n    corr : ndarray, (k, k)\n        initial correlation matrix\n    threshold : float\n        clipping threshold for smallest eigenvalue, see Notes\n    n_fact : int or float\n        factor to determine the maximum number of iterations. The maximum\n        number of iterations is the integer part of the number of columns in\n        the correlation matrix times n_fact.\n\n    Returns\n    -------\n    corr_new : ndarray, (optional)\n        corrected correlation matrix\n\n    Notes\n    -----\n    The smallest eigenvalue of the corrected correlation matrix is\n    approximately equal to the ``threshold``.\n    If the threshold=0, then the smallest eigenvalue of the correlation matrix\n    might be negative, but zero within a numerical error, for example in the\n    range of -1e-16.\n\n    Assumes input correlation matrix is symmetric.\n\n    Stops after the first step if correlation matrix is already positive\n    semi-definite or positive definite, so that smallest eigenvalue is above\n    threshold. In this case, the returned array is not the original, but\n    is equal to it within numerical precision.\n\n    See Also\n    --------\n    corr_clipped\n    cov_nearest\n\n    \"\"\"\n    k_vars = corr.shape[0]\n    if k_vars != corr.shape[1]:\n        raise ValueError('matrix is not square')\n    diff = np.zeros(corr.shape)\n    x_new = corr.copy()\n    diag_idx = np.arange(k_vars)\n    for ii in range(int(len(corr) * n_fact)):\n        x_adj = x_new - diff\n        x_psd, clipped = clip_evals(x_adj, value=threshold)\n        if not clipped:\n            x_new = x_psd\n            break\n        diff = x_psd - x_adj\n        x_new = x_psd.copy()\n        x_new[diag_idx, diag_idx] = 1\n    else:\n        warnings.warn(iteration_limit_doc, IterationLimitWarning)\n    return x_new"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "cov_nearest", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/correlation_tools.py", "method_code_mask": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\n\n\ndef cov_nearest(cov, method='clipped', threshold=1e-15, n_fact=100,\n    return_all=False): [MASK]\n", "response": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\ndef cov_nearest(cov, method='clipped', threshold=1e-15, n_fact=100,\n    return_all=False):\n    \"\"\"\n    Find the nearest covariance matrix that is positive (semi-) definite\n\n    This leaves the diagonal, i.e. the variance, unchanged\n\n    Parameters\n    ----------\n    cov : ndarray, (k,k)\n        initial covariance matrix\n    method : str\n        if \"clipped\", then the faster but less accurate ``corr_clipped`` is\n        used.if \"nearest\", then ``corr_nearest`` is used\n    threshold : float\n        clipping threshold for smallest eigen value, see Notes\n    n_fact : int or float\n        factor to determine the maximum number of iterations in\n        ``corr_nearest``. See its doc string\n    return_all : bool\n        if False (default), then only the covariance matrix is returned.\n        If True, then correlation matrix and standard deviation are\n        additionally returned.\n\n    Returns\n    -------\n    cov_ : ndarray\n        corrected covariance matrix\n    corr_ : ndarray, (optional)\n        corrected correlation matrix\n    std_ : ndarray, (optional)\n        standard deviation\n\n\n    Notes\n    -----\n    This converts the covariance matrix to a correlation matrix. Then, finds\n    the nearest correlation matrix that is positive semidefinite and converts\n    it back to a covariance matrix using the initial standard deviation.\n\n    The smallest eigenvalue of the intermediate correlation matrix is\n    approximately equal to the ``threshold``.\n    If the threshold=0, then the smallest eigenvalue of the correlation matrix\n    might be negative, but zero within a numerical error, for example in the\n    range of -1e-16.\n\n    Assumes input covariance matrix is symmetric.\n\n    See Also\n    --------\n    corr_nearest\n    corr_clipped\n    \"\"\"\n    from statsmodels.stats.moment_helpers import cov2corr, corr2cov\n    cov_, std_ = cov2corr(cov, return_std=True)\n    if method == 'clipped':\n        corr_ = corr_clipped(cov_, threshold=threshold)\n    else:\n        corr_ = corr_nearest(cov_, threshold=threshold, n_fact=n_fact)\n    cov_ = corr2cov(corr_, std_)\n    if return_all:\n        return cov_, corr_, std_\n    else:\n        return cov_"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "array_like", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/validation/validation.py", "method_code_mask": "from typing import Any\nfrom typing import Optional\nfrom collections.abc import Mapping\nimport numpy as np\nimport pandas as pd\n\n\ndef array_like(obj, name, dtype=np.double, ndim=1, maxdim=None, shape=None,\n    order=None, contiguous=False, optional=False, writeable=True): [MASK]\n", "response": "from typing import Any\nfrom typing import Optional\nfrom collections.abc import Mapping\nimport numpy as np\nimport pandas as pd\ndef array_like(obj, name, dtype=np.double, ndim=1, maxdim=None, shape=None,\n    order=None, contiguous=False, optional=False, writeable=True):\n    \"\"\"\n    Convert array-like to a ndarray and check conditions\n\n    Parameters\n    ----------\n    obj : array_like\n         An array, any object exposing the array interface, an object whose\n        __array__ method returns an array, or any (nested) sequence.\n    name : str\n        Name of the variable to use in exceptions\n    dtype : {None, numpy.dtype, str}\n        Required dtype. Default is double. If None, does not change the dtype\n        of obj (if present) or uses NumPy to automatically detect the dtype\n    ndim : {int, None}\n        Required number of dimensions of obj. If None, no check is performed.\n        If the number of dimensions of obj is less than ndim, additional axes\n        are inserted on the right. See examples.\n    maxdim : {int, None}\n        Maximum allowed dimension.  Use ``maxdim`` instead of ``ndim`` when\n        inputs are allowed to have ndim 1, 2, ..., or maxdim.\n    shape : {tuple[int], None}\n        Required shape obj.  If None, no check is performed. Partially\n        restricted shapes can be checked using None. See examples.\n    order : {'C', 'F', None}\n        Order of the array\n    contiguous : bool\n        Ensure that the array's data is contiguous with order ``order``\n    optional : bool\n        Flag indicating whether None is allowed\n    writeable : bool\n        Whether to ensure the returned array is writeable\n\n    Returns\n    -------\n    ndarray\n        The converted input.\n\n    Examples\n    --------\n    Convert a list or pandas series to an array\n    >>> import pandas as pd\n    >>> x = [0, 1, 2, 3]\n    >>> a = array_like(x, 'x', ndim=1)\n    >>> a.shape\n    (4,)\n\n    >>> a = array_like(pd.Series(x), 'x', ndim=1)\n    >>> a.shape\n    (4,)\n\n    >>> type(a.orig)\n    pandas.core.series.Series\n\n    Squeezes singleton dimensions when required\n    >>> x = np.array(x).reshape((4, 1))\n    >>> a = array_like(x, 'x', ndim=1)\n    >>> a.shape\n    (4,)\n\n    Right-appends when required size is larger than actual\n    >>> x = [0, 1, 2, 3]\n    >>> a = array_like(x, 'x', ndim=2)\n    >>> a.shape\n    (4, 1)\n\n    Check only the first and last dimension of the input\n    >>> x = np.arange(4*10*4).reshape((4, 10, 4))\n    >>> y = array_like(x, 'x', ndim=3, shape=(4, None, 4))\n\n    Check only the first two dimensions\n    >>> z = array_like(x, 'x', ndim=3, shape=(4, 10))\n\n    Raises ValueError if constraints are not satisfied\n    >>> z = array_like(x, 'x', ndim=2)\n    Traceback (most recent call last):\n     ...\n    ValueError: x is required to have ndim 2 but has ndim 3\n\n    >>> z = array_like(x, 'x', shape=(10, 4, 4))\n    Traceback (most recent call last):\n     ...\n    ValueError: x is required to have shape (10, 4, 4) but has shape (4, 10, 4)\n\n    >>> z = array_like(x, 'x', shape=(None, 4, 4))\n    Traceback (most recent call last):\n     ...\n    ValueError: x is required to have shape (*, 4, 4) but has shape (4, 10, 4)\n    \"\"\"\n    if optional and obj is None:\n        return None\n    reqs = ['W'] if writeable else []\n    if order == 'C' or contiguous:\n        reqs += ['C']\n    elif order == 'F':\n        reqs += ['F']\n    arr = np.require(obj, dtype=dtype, requirements=reqs)\n    if maxdim is not None:\n        if arr.ndim > maxdim:\n            msg = f'{name} must have ndim <= {maxdim}'\n            raise ValueError(msg)\n    elif ndim is not None:\n        if arr.ndim > ndim:\n            arr = _right_squeeze(arr, stop_dim=ndim)\n        elif arr.ndim < ndim:\n            arr = np.reshape(arr, arr.shape + (1,) * (ndim - arr.ndim))\n        if arr.ndim != ndim:\n            msg = '{0} is required to have ndim {1} but has ndim {2}'\n            raise ValueError(msg.format(name, ndim, arr.ndim))\n    if shape is not None:\n        for actual, req in zip(arr.shape, shape):\n            if req is not None and actual != req:\n                req_shape = str(shape).replace('None, ', '*, ')\n                msg = '{0} is required to have shape {1} but has shape {2}'\n                raise ValueError(msg.format(name, req_shape, arr.shape))\n    return arr"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_project_correlation_factors", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/correlation_tools.py", "method_code_mask": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\n\n\ndef _project_correlation_factors(X): [MASK]\n", "response": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\ndef _project_correlation_factors(X):\n    \"\"\"\n    Project a matrix into the domain of matrices whose row-wise sums\n    of squares are less than or equal to 1.\n\n    The input matrix is modified in-place.\n    \"\"\"\n    nm = np.sqrt((X * X).sum(1))\n    ii = np.flatnonzero(nm > 1)\n    if len(ii) > 0:\n        X[ii, :] /= nm[ii][:, None]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "corr_clipped", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/correlation_tools.py", "method_code_mask": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\n\n\ndef corr_clipped(corr, threshold=1e-15): [MASK]\n", "response": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\ndef corr_clipped(corr, threshold=1e-15):\n    \"\"\"\n    Find a near correlation matrix that is positive semi-definite\n\n    This function clips the eigenvalues, replacing eigenvalues smaller than\n    the threshold by the threshold. The new matrix is normalized, so that the\n    diagonal elements are one.\n    Compared to corr_nearest, the distance between the original correlation\n    matrix and the positive definite correlation matrix is larger, however,\n    it is much faster since it only computes eigenvalues once.\n\n    Parameters\n    ----------\n    corr : ndarray, (k, k)\n        initial correlation matrix\n    threshold : float\n        clipping threshold for smallest eigenvalue, see Notes\n\n    Returns\n    -------\n    corr_new : ndarray, (optional)\n        corrected correlation matrix\n\n\n    Notes\n    -----\n    The smallest eigenvalue of the corrected correlation matrix is\n    approximately equal to the ``threshold``. In examples, the\n    smallest eigenvalue can be by a factor of 10 smaller than the threshold,\n    e.g. threshold 1e-8 can result in smallest eigenvalue in the range\n    between 1e-9 and 1e-8.\n    If the threshold=0, then the smallest eigenvalue of the correlation matrix\n    might be negative, but zero within a numerical error, for example in the\n    range of -1e-16.\n\n    Assumes input correlation matrix is symmetric. The diagonal elements of\n    returned correlation matrix is set to ones.\n\n    If the correlation matrix is already positive semi-definite given the\n    threshold, then the original correlation matrix is returned.\n\n    ``cov_clipped`` is 40 or more times faster than ``cov_nearest`` in simple\n    example, but has a slightly larger approximation error.\n\n    See Also\n    --------\n    corr_nearest\n    cov_nearest\n\n    \"\"\"\n    x_new, clipped = clip_evals(corr, value=threshold)\n    if not clipped:\n        return corr\n    x_std = np.sqrt(np.diag(x_new))\n    x_new = x_new / x_std / x_std[:, None]\n    return x_new"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "fdrcorrection_twostage", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/multitest.py", "method_code_mask": "import numpy as np\nfrom statsmodels.stats._knockoff import RegressionFDR\nimport gc\nimport warnings\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import families\nfrom statsmodels.regression.linear_model import OLS\nfrom scipy.stats.distributions import norm\nfrom scipy.optimize import minimize\n\n\ndef fdrcorrection_twostage(pvals, alpha=0.05, method='bky', maxiter=1, iter\n    =None, is_sorted=False): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.stats._knockoff import RegressionFDR\nimport gc\nimport warnings\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import families\nfrom statsmodels.regression.linear_model import OLS\nfrom scipy.stats.distributions import norm\nfrom scipy.optimize import minimize\ndef fdrcorrection_twostage(pvals, alpha=0.05, method='bky', maxiter=1, iter\n    =None, is_sorted=False):\n    \"\"\"(iterated) two stage linear step-up procedure with estimation of number of true\n    hypotheses\n\n    Benjamini, Krieger and Yekuteli, procedure in Definition 6\n\n    Parameters\n    ----------\n    pvals : array_like\n        set of p-values of the individual tests.\n    alpha : float\n        error rate\n    method : {'bky', 'bh')\n        see Notes for details\n\n        * 'bky' - implements the procedure in Definition 6 of Benjamini, Krieger\n           and Yekuteli 2006\n        * 'bh' - the two stage method of Benjamini and Hochberg\n\n    maxiter : int or bool\n        Maximum number of iterations.\n        maxiter=1 (default) corresponds to the two stage method.\n        maxiter=-1 corresponds to full iterations which is maxiter=len(pvals).\n        maxiter=0 uses only a single stage fdr correction using a 'bh' or 'bky'\n        prior fraction of assumed true hypotheses.\n        Boolean maxiter is allowed for backwards compatibility with the\n        deprecated ``iter`` keyword.\n        maxiter=False is two-stage fdr (maxiter=1)\n        maxiter=True is full iteration (maxiter=-1 or maxiter=len(pvals))\n\n        .. versionadded:: 0.14\n\n            Replacement for ``iter`` with additional features.\n\n    iter : bool\n        ``iter`` is deprecated use ``maxiter`` instead.\n        If iter is True, then only one iteration step is used, this is the\n        two-step method.\n        If iter is False, then iterations are stopped at convergence which\n        occurs in a finite number of steps (at most len(pvals) steps).\n\n        .. deprecated:: 0.14\n\n            Use ``maxiter`` instead of ``iter``.\n\n    Returns\n    -------\n    rejected : ndarray, bool\n        True if a hypothesis is rejected, False if not\n    pvalue-corrected : ndarray\n        pvalues adjusted for multiple hypotheses testing to limit FDR\n    m0 : int\n        ntest - rej, estimated number of true (not rejected) hypotheses\n    alpha_stages : list of floats\n        A list of alphas that have been used at each stage\n\n    Notes\n    -----\n    The returned corrected p-values are specific to the given alpha, they\n    cannot be used for a different alpha.\n\n    The returned corrected p-values are from the last stage of the fdr_bh\n    linear step-up procedure (fdrcorrection0 with method='indep') corrected\n    for the estimated fraction of true hypotheses.\n    This means that the rejection decision can be obtained with\n    ``pval_corrected <= alpha``, where ``alpha`` is the original significance\n    level.\n    (Note: This has changed from earlier versions (<0.5.0) of statsmodels.)\n\n    BKY described several other multi-stage methods, which would be easy to implement.\n    However, in their simulation the simple two-stage method (with iter=False) was the\n    most robust to the presence of positive correlation\n\n    TODO: What should be returned?\n\n    \"\"\"\n    pvals = np.asarray(pvals)\n    if iter is not None:\n        import warnings\n        msg = 'iter keyword is deprecated, use maxiter keyword instead.'\n        warnings.warn(msg, FutureWarning)\n    if iter is False:\n        maxiter = 1\n    elif iter is True or maxiter in [-1, None]:\n        maxiter = len(pvals)\n    if not is_sorted:\n        pvals_sortind = np.argsort(pvals)\n        pvals = np.take(pvals, pvals_sortind)\n    ntests = len(pvals)\n    if method == 'bky':\n        fact = 1.0 + alpha\n        alpha_prime = alpha / fact\n    elif method == 'bh':\n        fact = 1.0\n        alpha_prime = alpha\n    else:\n        raise ValueError(\"only 'bky' and 'bh' are available as method\")\n    alpha_stages = [alpha_prime]\n    rej, pvalscorr = fdrcorrection(pvals, alpha=alpha_prime, method='indep',\n        is_sorted=True)\n    r1 = rej.sum()\n    if r1 == 0 or r1 == ntests:\n        reject = rej\n        pvalscorr *= fact\n        ri = r1\n    else:\n        ri_old = ri = r1\n        ntests0 = ntests\n        for it in range(maxiter):\n            ntests0 = 1.0 * ntests - ri_old\n            alpha_star = alpha_prime * ntests / ntests0\n            alpha_stages.append(alpha_star)\n            rej, pvalscorr = fdrcorrection(pvals, alpha=alpha_star, method=\n                'indep', is_sorted=True)\n            ri = rej.sum()\n            if it >= maxiter - 1 or ri == ri_old:\n                break\n            elif ri < ri_old:\n                raise RuntimeError(' oops - should not be here')\n            ri_old = ri\n        pvalscorr *= ntests0 * 1.0 / ntests\n        if method == 'bky':\n            pvalscorr *= 1.0 + alpha\n    pvalscorr[pvalscorr > 1] = 1\n    if not is_sorted:\n        pvalscorr_ = np.empty_like(pvalscorr)\n        pvalscorr_[pvals_sortind] = pvalscorr\n        del pvalscorr\n        reject = np.empty_like(rej)\n        reject[pvals_sortind] = rej\n        return reject, pvalscorr_, ntests - ri, alpha_stages\n    else:\n        return rej, pvalscorr, ntests - ri, alpha_stages"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_design_knockoff_equi", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/_knockoff.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nfrom statsmodels.iolib import summary2\n\n\ndef _design_knockoff_equi(exog): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nfrom statsmodels.iolib import summary2\ndef _design_knockoff_equi(exog):\n    \"\"\"\n    Construct an equivariant design matrix for knockoff analysis.\n\n    Follows the 'equi-correlated knockoff approach of equation 2.4 in\n    Barber and Candes.\n\n    Constructs a pair of design matrices exogs, exogn such that exogs\n    is a scaled/centered version of the input matrix exog, exogn is\n    another matrix of the same shape with cov(exogn) = cov(exogs), and\n    the covariances between corresponding columns of exogn and exogs\n    are as small as possible.\n    \"\"\"\n    nobs, nvar = exog.shape\n    if nobs < 2 * nvar:\n        msg = 'The equivariant knockoff can ony be used when n >= 2*p'\n        raise ValueError(msg)\n    xnm = np.sum(exog ** 2, 0)\n    xnm = np.sqrt(xnm)\n    exog = exog / xnm\n    xcov = np.dot(exog.T, exog)\n    ev, _ = np.linalg.eig(xcov)\n    evmin = np.min(ev)\n    sl = min(2 * evmin, 1)\n    sl = sl * np.ones(nvar)\n    exogn = _get_knmat(exog, xcov, sl)\n    return exog, exogn, sl"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "load_pandas", "method_path": "../srcdata/Computation/statsmodels/statsmodels/datasets/longley/data.py", "method_code_mask": "from statsmodels.datasets import utils as du\n\n\ndef load_pandas(): [MASK]\n", "response": "from statsmodels.datasets import utils as du\ndef load_pandas():\n    \"\"\"\n    Load the Longley data and return a Dataset class.\n\n    Returns\n    -------\n    Dataset\n        See DATASET_PROPOSAL.txt for more information.\n    \"\"\"\n    data = _get_data()\n    return du.process_pandas(data, endog_idx=0)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "normal_power", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/power.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom scipy import stats\nfrom scipy import optimize\nfrom scipy import special\nfrom statsmodels.tools.rootfinding import brentq_expanding\nfrom collections import defaultdict\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nfrom statsmodels.tools.sm_exceptions import convergence_doc\nfrom statsmodels.graphics import utils\nfrom statsmodels.graphics.plottools import rainbow\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.gof import chisquare_power\n\n\ndef normal_power(effect_size, nobs, alpha, alternative='two-sided', sigma=1.0\n    ): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom scipy import stats\nfrom scipy import optimize\nfrom scipy import special\nfrom statsmodels.tools.rootfinding import brentq_expanding\nfrom collections import defaultdict\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nfrom statsmodels.tools.sm_exceptions import convergence_doc\nfrom statsmodels.graphics import utils\nfrom statsmodels.graphics.plottools import rainbow\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.gof import chisquare_power\ndef normal_power(effect_size, nobs, alpha, alternative='two-sided', sigma=1.0):\n    \"\"\"Calculate power of a normal distributed test statistic\n\n    This is an generalization of `normal_power` when variance under Null and\n    Alternative differ.\n\n    Parameters\n    ----------\n    effect size : float\n        difference in the estimated means or statistics under the alternative\n        normalized by the standard deviation (without division by sqrt(nobs).\n    nobs : float or int\n        number of observations\n    alpha : float in interval (0,1)\n        significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        extra argument to choose whether the power is calculated for a\n        two-sided (default) or one sided test. The one-sided test can be\n        either 'larger', 'smaller'.\n    \"\"\"\n    d = effect_size\n    if alternative in ['two-sided', '2s']:\n        alpha_ = alpha / 2.0\n    elif alternative in ['smaller', 'larger']:\n        alpha_ = alpha\n    else:\n        raise ValueError(\"alternative has to be 'two-sided', 'larger' \" +\n            \"or 'smaller'\")\n    pow_ = 0\n    if alternative in ['two-sided', '2s', 'larger']:\n        crit = stats.norm.isf(alpha_)\n        pow_ = stats.norm.sf(crit - d * np.sqrt(nobs) / sigma)\n    if alternative in ['two-sided', '2s', 'smaller']:\n        crit = stats.norm.ppf(alpha_)\n        pow_ += stats.norm.cdf(crit - d * np.sqrt(nobs) / sigma)\n    return pow_"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "kernel_covariance", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/correlation_tools.py", "method_code_mask": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\n\n\ndef kernel_covariance(exog, loc, groups, kernel=None, bw=None): [MASK]\n", "response": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\ndef kernel_covariance(exog, loc, groups, kernel=None, bw=None):\n    \"\"\"\n    Use kernel averaging to estimate a multivariate covariance function.\n\n    The goal is to estimate a covariance function C(x, y) =\n    cov(Z(x), Z(y)) where x, y are vectors in R^p (e.g. representing\n    locations in time or space), and Z(.) represents a multivariate\n    process on R^p.\n\n    The data used for estimation can be observed at arbitrary values of the\n    position vector, and there can be multiple independent observations\n    from the process.\n\n    Parameters\n    ----------\n    exog : array_like\n        The rows of exog are realizations of the process obtained at\n        specified points.\n    loc : array_like\n        The rows of loc are the locations (e.g. in space or time) at\n        which the rows of exog are observed.\n    groups : array_like\n        The values of groups are labels for distinct independent copies\n        of the process.\n    kernel : MultivariateKernel instance, optional\n        An instance of MultivariateKernel, defaults to\n        GaussianMultivariateKernel.\n    bw : array_like or scalar\n        A bandwidth vector, or bandwidth multiplier.  If a 1d array, it\n        contains kernel bandwidths for each component of the process, and\n        must have length equal to the number of columns of exog.  If a scalar,\n        bw is a bandwidth multiplier used to adjust the default bandwidth; if\n        None, a default bandwidth is used.\n\n    Returns\n    -------\n    A real-valued function C(x, y) that returns an estimate of the covariance\n    between values of the process located at x and y.\n\n    References\n    ----------\n    .. [1] Genton M, W Kleiber (2015).  Cross covariance functions for\n        multivariate geostatics.  Statistical Science 30(2).\n        https://arxiv.org/pdf/1507.08017.pdf\n    \"\"\"\n    exog = np.asarray(exog)\n    loc = np.asarray(loc)\n    groups = np.asarray(groups)\n    if loc.ndim == 1:\n        loc = loc[:, None]\n    v = [exog.shape[0], loc.shape[0], len(groups)]\n    if min(v) != max(v):\n        msg = 'exog, loc, and groups must have the same number of rows'\n        raise ValueError(msg)\n    ix = {}\n    for i, g in enumerate(groups):\n        if g not in ix:\n            ix[g] = []\n        ix[g].append(i)\n    for g in ix.keys():\n        ix[g] = np.sort(ix[g])\n    if kernel is None:\n        kernel = GaussianMultivariateKernel()\n    if bw is None:\n        kernel.set_default_bw(loc)\n    elif np.isscalar(bw):\n        kernel.set_default_bw(loc, bwm=bw)\n    else:\n        kernel.set_bandwidth(bw)\n\n    def cov(x, y):\n        kx = kernel.call(x, loc)\n        ky = kernel.call(y, loc)\n        cm, cw = 0.0, 0.0\n        for g, ii in ix.items():\n            m = len(ii)\n            j1, j2 = np.indices((m, m))\n            j1 = ii[j1.flat]\n            j2 = ii[j2.flat]\n            w = kx[j1] * ky[j2]\n            cm += np.einsum('ij,ik,i->jk', exog[j1, :], exog[j2, :], w)\n            cw += w.sum()\n        if cw < 1e-10:\n            msg = ('Effective sample size is 0.  The bandwidth may be too ' +\n                'small, or you are outside the range of your data.')\n            warnings.warn(msg)\n            return np.nan * np.ones_like(cm)\n        return cm / cw\n    return cov"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "chisquare", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/gof.py", "method_code_mask": "from statsmodels.compat.python import lrange\nimport numpy as np\nfrom scipy import stats\n\n\ndef chisquare(f_obs, f_exp=None, value=0, ddof=0, return_basic=True): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nimport numpy as np\nfrom scipy import stats\ndef chisquare(f_obs, f_exp=None, value=0, ddof=0, return_basic=True):\n    \"\"\"chisquare goodness-of-fit test\n\n    The null hypothesis is that the distance between the expected distribution\n    and the observed frequencies is ``value``. The alternative hypothesis is\n    that the distance is larger than ``value``. ``value`` is normalized in\n    terms of effect size.\n\n    The standard chisquare test has the null hypothesis that ``value=0``, that\n    is the distributions are the same.\n\n\n    Notes\n    -----\n    The case with value greater than zero is similar to an equivalence test,\n    that the exact null hypothesis is replaced by an approximate hypothesis.\n    However, TOST \"reverses\" null and alternative hypothesis, while here the\n    alternative hypothesis is that the distance (divergence) is larger than a\n    threshold.\n\n    References\n    ----------\n    McLaren, ...\n    Drost,...\n\n    See Also\n    --------\n    powerdiscrepancy\n    scipy.stats.chisquare\n\n    \"\"\"\n    f_obs = np.asarray(f_obs)\n    n_bins = len(f_obs)\n    nobs = f_obs.sum(0)\n    if f_exp is None:\n        f_exp = np.empty(n_bins, float)\n        f_exp.fill(nobs / float(n_bins))\n    f_exp = np.asarray(f_exp, float)\n    chisq = ((f_obs - f_exp) ** 2 / f_exp).sum(0)\n    if value == 0:\n        pvalue = stats.chi2.sf(chisq, n_bins - 1 - ddof)\n    else:\n        pvalue = stats.ncx2.sf(chisq, n_bins - 1 - ddof, value ** 2 * nobs)\n    if return_basic:\n        return chisq, pvalue\n    else:\n        return chisq, pvalue"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "chisquare_effectsize", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/gof.py", "method_code_mask": "from statsmodels.compat.python import lrange\nimport numpy as np\nfrom scipy import stats\n\n\ndef chisquare_effectsize(probs0, probs1, correction=None, cohen=True, axis=0\n    ): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nimport numpy as np\nfrom scipy import stats\ndef chisquare_effectsize(probs0, probs1, correction=None, cohen=True, axis=0):\n    \"\"\"effect size for a chisquare goodness-of-fit test\n\n    Parameters\n    ----------\n    probs0 : array_like\n        probabilities or cell frequencies under the Null hypothesis\n    probs1 : array_like\n        probabilities or cell frequencies under the Alternative hypothesis\n        probs0 and probs1 need to have the same length in the ``axis`` dimension.\n        and broadcast in the other dimensions\n        Both probs0 and probs1 are normalized to add to one (in the ``axis``\n        dimension).\n    correction : None or tuple\n        If None, then the effect size is the chisquare statistic divide by\n        the number of observations.\n        If the correction is a tuple (nobs, df), then the effectsize is\n        corrected to have less bias and a smaller variance. However, the\n        correction can make the effectsize negative. In that case, the\n        effectsize is set to zero.\n        Pederson and Johnson (1990) as referenced in McLaren et all. (1994)\n    cohen : bool\n        If True, then the square root is returned as in the definition of the\n        effect size by Cohen (1977), If False, then the original effect size\n        is returned.\n    axis : int\n        If the probability arrays broadcast to more than 1 dimension, then\n        this is the axis over which the sums are taken.\n\n    Returns\n    -------\n    effectsize : float\n        effect size of chisquare test\n\n    \"\"\"\n    probs0 = np.asarray(probs0, float)\n    probs1 = np.asarray(probs1, float)\n    probs0 = probs0 / probs0.sum(axis)\n    probs1 = probs1 / probs1.sum(axis)\n    d2 = ((probs1 - probs0) ** 2 / probs0).sum(axis)\n    if correction is not None:\n        nobs, df = correction\n        diff = ((probs1 - probs0) / probs0).sum(axis)\n        d2 = np.maximum((d2 * nobs - diff - df) / (nobs - 1.0), 0)\n    if cohen:\n        return np.sqrt(d2)\n    else:\n        return d2"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "summary_col", "method_path": "../srcdata/Computation/statsmodels/statsmodels/iolib/summary2.py", "method_code_mask": "from statsmodels.compat.python import lzip\nimport datetime\nfrom functools import reduce\nimport re\nimport textwrap\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\n\ndef summary_col(results, float_format='%.4f', model_names=(), stars=False,\n    info_dict=None, regressor_order=(), drop_omitted=False, include_r2=True,\n    fixed_effects=None, fe_present='Yes', fe_absent=''): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nimport datetime\nfrom functools import reduce\nimport re\nimport textwrap\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\ndef summary_col(results, float_format='%.4f', model_names=(), stars=False,\n    info_dict=None, regressor_order=(), drop_omitted=False, include_r2=True,\n    fixed_effects=None, fe_present='Yes', fe_absent=''):\n    \"\"\"\n    Summarize multiple results instances side-by-side (coefs and SEs)\n\n    Parameters\n    ----------\n    results : statsmodels results instance or list of result instances\n    float_format : str, optional\n        float format for coefficients and standard errors\n        Default : '%.4f'\n    model_names : list[str], optional\n        Must have same length as the number of results. If the names are not\n        unique, a roman number will be appended to all model names\n    stars : bool\n        print significance stars\n    info_dict : dict, default None\n        dict of functions to be applied to results instances to retrieve\n        model info. To use specific information for different models, add a\n        (nested) info_dict with model name as the key.\n        Example: `info_dict = {\"N\":lambda x:(x.nobs), \"R2\": ..., \"OLS\":{\n        \"R2\":...}}` would only show `R2` for OLS regression models, but\n        additionally `N` for all other results.\n        Default : None (use the info_dict specified in\n        result.default_model_infos, if this property exists)\n    regressor_order : list[str], optional\n        list of names of the regressors in the desired order. All regressors\n        not specified will be appended to the end of the list.\n    drop_omitted : bool, optional\n        Includes regressors that are not specified in regressor_order. If\n        False, regressors not specified will be appended to end of the list.\n        If True, only regressors in regressor_order will be included.\n    include_r2 : bool, optional\n        Includes R2 and adjusted R2 in the summary table.\n    fixed_effects : list[str], optional\n        List of categorical variables for which to indicate presence of\n        fixed effects.\n    fe_present : str, optional\n        String to indicate the presence of fixed effects. Default is \"Yes\".\n    fe_absent : str, optional\n        String to indicate the absence of fixed effects. Default is empty\n        string.\n    \"\"\"\n    if not isinstance(results, list):\n        results = [results]\n    cols = [_col_params(x, stars=stars, float_format=float_format,\n        include_r2=include_r2) for x in results]\n    if model_names:\n        colnames = _make_unique(model_names)\n    else:\n        colnames = _make_unique([x.columns[0] for x in cols])\n    for i in range(len(cols)):\n        cols[i].columns = [colnames[i]]\n\n    def merg(x, y):\n        return x.merge(y, how='outer', right_index=True, left_index=True)\n    index = list(cols[0].index)\n    for col in cols[1:]:\n        for key in col.index:\n            if key not in index:\n                index.append(key)\n    for special in (('R-squared', ''), ('R-squared Adj.', '')):\n        if special in index:\n            index.remove(special)\n            index.insert(len(index), special)\n    summ = reduce(merg, cols)\n    summ = summ.reindex(index)\n    if regressor_order:\n        varnames = summ.index.get_level_values(0).tolist()\n        vc = pd.Series(varnames).value_counts()\n        varnames = vc.loc[vc == 2].index.tolist()\n        ordered = [x for x in regressor_order if x in varnames]\n        unordered = [x for x in varnames if x not in regressor_order]\n        new_order = ordered + unordered\n        other = [x for x in summ.index.get_level_values(0) if x not in\n            new_order]\n        new_order += other\n        if drop_omitted:\n            for uo in unordered:\n                new_order.remove(uo)\n        summ = summ.reindex(new_order, level=0)\n    idx = []\n    index = summ.index.get_level_values(0)\n    for i in range(0, index.shape[0], 2):\n        idx.append(index[i])\n        if i + 1 < index.shape[0] and index[i] == index[i + 1]:\n            idx.append('')\n        else:\n            idx.append(index[i + 1])\n    summ.index = idx\n    if fixed_effects:\n        if not info_dict:\n            info_dict = {}\n        for fe in fixed_effects:\n            info_dict[fe + ' FE'] = (lambda x, fe=fe, fe_present=fe_present,\n                fe_absent=fe_absent: fe_present if any(f'C({fe})' in param for\n                param in x.params.index) else fe_absent)\n    if info_dict:\n        cols = [_col_info(x, info_dict.get(x.model.__class__.__name__,\n            info_dict)) for x in results]\n    else:\n        cols = [_col_info(x, getattr(x, 'default_model_infos', None)) for x in\n            results]\n    for df, name in zip(cols, _make_unique([df.columns[0] for df in cols])):\n        df.columns = [name]\n    info = reduce(merg, cols)\n    dat = pd.DataFrame(np.vstack([summ, info]))\n    dat.columns = summ.columns\n    dat.index = pd.Index(summ.index.tolist() + info.index.tolist())\n    summ = dat\n    summ = summ.fillna('')\n    if fixed_effects:\n        index_series = pd.Series(summ.index, index=summ.index)\n        skip_flag = index_series.apply(lambda x: any(f'C({fe})' in x for fe in\n            fixed_effects))\n        skip_next_flag = skip_flag.shift(fill_value=False)\n        final_skip = skip_flag | skip_next_flag\n        summ = summ[~final_skip]\n        r_squared_rows = summ.index[summ.index.str.contains('R-squared')]\n        r_squared_section = summ.loc[r_squared_rows]\n        summ = summ.drop(index=r_squared_rows)\n        summ = pd.concat([summ, r_squared_section])\n    smry = Summary()\n    smry._merge_latex = True\n    smry.add_df(summ, header=True, align='l')\n    smry.add_text('Standard errors in parentheses.')\n    if stars:\n        smry.add_text('* p<.1, ** p<.05, ***p<.01')\n    return smry"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "zconfint", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/weightstats.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.tools.decorators import cache_readonly\nimport pandas as pd\nfrom statsmodels.iolib.summary import summary_params\n\n\ndef zconfint(x1, x2=None, value=0, alpha=0.05, alternative='two-sided',\n    usevar='pooled', ddof=1.0): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.tools.decorators import cache_readonly\nimport pandas as pd\nfrom statsmodels.iolib.summary import summary_params\ndef zconfint(x1, x2=None, value=0, alpha=0.05, alternative='two-sided',\n    usevar='pooled', ddof=1.0):\n    \"\"\"confidence interval based on normal distribution z-test\n\n    Parameters\n    ----------\n    x1 : array_like, 1-D or 2-D\n        first of the two independent samples, see notes for 2-D case\n    x2 : array_like, 1-D or 2-D\n        second of the two independent samples, see notes for 2-D case\n    value : float\n        In the one sample case, value is the mean of x1 under the Null\n        hypothesis.\n        In the two sample case, value is the difference between mean of x1 and\n        mean of x2 under the Null hypothesis. The test statistic is\n        `x1_mean - x2_mean - value`.\n    usevar : str, 'pooled'\n        Currently, only 'pooled' is implemented.\n        If ``pooled``, then the standard deviation of the samples is assumed to be\n        the same. see CompareMeans.ztest_ind for different options.\n    ddof : int\n        Degrees of freedom use in the calculation of the variance of the mean\n        estimate. In the case of comparing means this is one, however it can\n        be adjusted for testing other statistics (proportion, correlation)\n\n    Notes\n    -----\n    checked only for 1 sample case\n\n    usevar not implemented, is always pooled in two sample case\n\n    ``value`` shifts the confidence interval so it is centered at\n    `x1_mean - x2_mean - value`\n\n    See Also\n    --------\n    ztest\n    CompareMeans\n\n    \"\"\"\n    if usevar != 'pooled':\n        raise NotImplementedError('only usevar=\"pooled\" is implemented')\n    x1 = np.asarray(x1)\n    nobs1 = x1.shape[0]\n    x1_mean = x1.mean(0)\n    x1_var = x1.var(0)\n    if x2 is not None:\n        x2 = np.asarray(x2)\n        nobs2 = x2.shape[0]\n        x2_mean = x2.mean(0)\n        x2_var = x2.var(0)\n        var_pooled = nobs1 * x1_var + nobs2 * x2_var\n        var_pooled /= nobs1 + nobs2 - 2 * ddof\n        var_pooled *= 1.0 / nobs1 + 1.0 / nobs2\n    else:\n        var_pooled = x1_var / (nobs1 - ddof)\n        x2_mean = 0\n    std_diff = np.sqrt(var_pooled)\n    ci = _zconfint_generic(x1_mean - x2_mean - value, std_diff, alpha,\n        alternative)\n    return ci"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "ksstat", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/_lilliefors.py", "method_code_mask": "from functools import partial\nimport numpy as np\nfrom scipy import stats\nfrom statsmodels.tools.validation import string_like\n\n\ndef ksstat(x, cdf, alternative='two_sided', args=()): [MASK]\n", "response": "from functools import partial\nimport numpy as np\nfrom scipy import stats\nfrom statsmodels.tools.validation import string_like\ndef ksstat(x, cdf, alternative='two_sided', args=()):\n    \"\"\"\n    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit\n\n    This calculates the test statistic for a test of the distribution G(x) of\n    an observed variable against a given distribution F(x). Under the null\n    hypothesis the two distributions are identical, G(x)=F(x). The\n    alternative hypothesis can be either 'two_sided' (default), 'less'\n    or 'greater'. The KS test is only valid for continuous distributions.\n\n    Parameters\n    ----------\n    x : array_like, 1d\n        array of observations\n    cdf : str or callable\n        string: name of a distribution in scipy.stats\n        callable: function to evaluate cdf\n    alternative : 'two_sided' (default), 'less' or 'greater'\n        defines the alternative hypothesis (see explanation)\n    args : tuple, sequence\n        distribution parameters for call to cdf\n\n\n    Returns\n    -------\n    D : float\n        KS test statistic, either D, D+ or D-\n\n    See Also\n    --------\n    scipy.stats.kstest\n\n    Notes\n    -----\n\n    In the one-sided test, the alternative is that the empirical\n    cumulative distribution function of the random variable is \"less\"\n    or \"greater\" than the cumulative distribution function F(x) of the\n    hypothesis, G(x)<=F(x), resp. G(x)>=F(x).\n\n    In contrast to scipy.stats.kstest, this function only calculates the\n    statistic which can be used either as distance measure or to implement\n    case specific p-values.\n    \"\"\"\n    nobs = float(len(x))\n    if isinstance(cdf, str):\n        cdf = getattr(stats.distributions, cdf).cdf\n    elif hasattr(cdf, 'cdf'):\n        cdf = getattr(cdf, 'cdf')\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n    d_plus = (np.arange(1.0, nobs + 1) / nobs - cdfvals).max()\n    d_min = (cdfvals - np.arange(0.0, nobs) / nobs).max()\n    if alternative == 'greater':\n        return d_plus\n    elif alternative == 'less':\n        return d_min\n    return np.max([d_plus, d_min])"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "normal_sample_size_one_tail", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/power.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom scipy import stats\nfrom scipy import optimize\nfrom scipy import special\nfrom statsmodels.tools.rootfinding import brentq_expanding\nfrom collections import defaultdict\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nfrom statsmodels.tools.sm_exceptions import convergence_doc\nfrom statsmodels.graphics import utils\nfrom statsmodels.graphics.plottools import rainbow\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.gof import chisquare_power\n\n\ndef normal_sample_size_one_tail(diff, power, alpha, std_null=1.0,\n    std_alternative=None): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom scipy import stats\nfrom scipy import optimize\nfrom scipy import special\nfrom statsmodels.tools.rootfinding import brentq_expanding\nfrom collections import defaultdict\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nfrom statsmodels.tools.sm_exceptions import convergence_doc\nfrom statsmodels.graphics import utils\nfrom statsmodels.graphics.plottools import rainbow\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.gof import chisquare_power\ndef normal_sample_size_one_tail(diff, power, alpha, std_null=1.0,\n    std_alternative=None):\n    \"\"\"explicit sample size computation if only one tail is relevant\n\n    The sample size is based on the power in one tail assuming that the\n    alternative is in the tail where the test has power that increases\n    with sample size.\n    Use alpha/2 to compute the one tail approximation to the two-sided\n    test, i.e. consider only one tail of two-sided test.\n\n    Parameters\n    ----------\n    diff : float\n        difference in the estimated means or statistics under the alternative.\n    power : float in interval (0,1)\n        power of the test, e.g. 0.8, is one minus the probability of a type II\n        error. Power is the probability that the test correctly rejects the\n        Null Hypothesis if the Alternative Hypothesis is true.\n    alpha : float in interval (0,1)\n        significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n        Note: alpha is used for one tail. Use alpha/2 for two-sided\n        alternative.\n    std_null : float\n        standard deviation under the Null hypothesis without division by\n        sqrt(nobs)\n    std_alternative : float\n        standard deviation under the Alternative hypothesis without division\n        by sqrt(nobs). Defaults to None. If None, ``std_alternative`` is set\n        to the value of ``std_null``.\n\n    Returns\n    -------\n    nobs : float\n        Sample size to achieve (at least) the desired power.\n        If the minimum power is satisfied for all positive sample sizes, then\n        ``nobs`` will be zero. This will be the case when power <= alpha if\n        std_alternative is equal to std_null.\n\n    \"\"\"\n    if std_alternative is None:\n        std_alternative = std_null\n    crit_power = stats.norm.isf(power)\n    crit = stats.norm.isf(alpha)\n    n1 = (np.maximum(crit * std_null - crit_power * std_alternative, 0) / diff\n        ) ** 2\n    return n1"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "cov2corr", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/moment_helpers.py", "method_code_mask": "import numpy as np\nfrom scipy.special import comb\n\n\ndef cov2corr(cov, return_std=False): [MASK]\n", "response": "import numpy as np\nfrom scipy.special import comb\ndef cov2corr(cov, return_std=False):\n    \"\"\"\n    convert covariance matrix to correlation matrix\n\n    Parameters\n    ----------\n    cov : array_like, 2d\n        covariance matrix, see Notes\n\n    Returns\n    -------\n    corr : ndarray (subclass)\n        correlation matrix\n    return_std : bool\n        If this is true then the standard deviation is also returned.\n        By default only the correlation matrix is returned.\n\n    Notes\n    -----\n    This function does not convert subclasses of ndarrays. This requires that\n    division is defined elementwise. np.ma.array and np.matrix are allowed.\n    \"\"\"\n    cov = np.asanyarray(cov)\n    std_ = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std_, std_)\n    if return_std:\n        return corr, std_\n    else:\n        return corr"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "power_equivalence_poisson_2indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/rates.py", "method_code_mask": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\n\n\ndef power_equivalence_poisson_2indep(rate1, rate2, nobs1, low, upp,\n    nobs_ratio=1, exposure=1, alpha=0.05, dispersion=1, method_var='alt',\n    return_results=False): [MASK]\n", "response": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\ndef power_equivalence_poisson_2indep(rate1, rate2, nobs1, low, upp,\n    nobs_ratio=1, exposure=1, alpha=0.05, dispersion=1, method_var='alt',\n    return_results=False):\n    \"\"\"Power of equivalence test of ratio of 2 independent poisson rates.\n\n    Parameters\n    ----------\n    rate1 : float\n        Poisson rate for the first sample, treatment group, under the\n        alternative hypothesis.\n    rate2 : float\n        Poisson rate for the second sample, reference group, under the\n        alternative hypothesis.\n    nobs1 : float or int\n        Number of observations in sample 1.\n    low : float\n        Lower equivalence margin for the rate ratio, rate1 / rate2.\n    upp : float\n        Upper equivalence margin for the rate ratio, rate1 / rate2.\n    nobs_ratio : float\n        Sample size ratio, nobs2 = nobs_ratio * nobs1.\n    exposure : float\n        Exposure for each observation. Total exposure is nobs1 * exposure\n        and nobs2 * exposure.\n    alpha : float in interval (0,1)\n        Significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    value : float\n        Difference between rates 1 and 2 under the null hypothesis.\n    method_var : {\"score\", \"alt\"}\n        The variance of the test statistic for the null hypothesis given the\n        rates uder the alternative, can be either equal to the rates under the\n        alternative ``method_var=\"alt\"``, or estimated under the constrained\n        of the null hypothesis, ``method_var=\"score\"``.\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        Alternative hypothesis whether the power is calculated for a\n        two-sided (default) or one sided test. The one-sided test can be\n        either 'larger', 'smaller'.\n    return_results : bool\n        If true, then a results instance with extra information is returned,\n        otherwise only the computed power is returned.\n\n    Returns\n    -------\n    results : results instance or float\n        If return_results is False, then only the power is returned.\n        If return_results is True, then a results instance with the\n        information in attributes is returned.\n\n        power : float\n            Power of the test, e.g. 0.8, is one minus the probability of a\n            type II error. Power is the probability that the test correctly\n            rejects the Null Hypothesis if the Alternative Hypothesis is true.\n\n        Other attributes in results instance include :\n\n        std_null\n            standard error of difference under the null hypothesis (without\n            sqrt(nobs1))\n        std_alt\n            standard error of difference under the alternative hypothesis\n            (without sqrt(nobs1))\n\n    References\n    ----------\n    .. [1] Zhu, Haiyuan. 2017. \u201cSample Size Calculation for Comparing Two\n       Poisson or Negative Binomial Rates in Noninferiority or Equivalence\n       Trials.\u201d Statistics in Biopharmaceutical Research, March.\n       https://doi.org/10.1080/19466315.2016.1225594\n    .. [2] Zhu, Haiyuan, and Hassan Lakkis. 2014. \u201cSample Size Calculation for\n       Comparing Two Negative Binomial Rates.\u201d Statistics in Medicine 33 (3):\n       376\u201387. https://doi.org/10.1002/sim.5947.\n    .. [3] PASS documentation\n    \"\"\"\n    rate1, rate2, nobs1 = map(np.asarray, [rate1, rate2, nobs1])\n    nobs2 = nobs_ratio * nobs1\n    v1 = dispersion / exposure * (1 / rate1 + 1 / (nobs_ratio * rate2))\n    if method_var == 'alt':\n        v0_low = v0_upp = v1\n    elif method_var == 'score':\n        v0_low = dispersion / exposure * (1 + low * nobs_ratio) ** 2\n        v0_low /= low * nobs_ratio * (rate1 + nobs_ratio * rate2)\n        v0_upp = dispersion / exposure * (1 + upp * nobs_ratio) ** 2\n        v0_upp /= upp * nobs_ratio * (rate1 + nobs_ratio * rate2)\n    else:\n        raise NotImplementedError(f'method_var {method_var} not recognized')\n    es_low = np.log(rate1 / rate2) - np.log(low)\n    es_upp = np.log(rate1 / rate2) - np.log(upp)\n    std_null_low = np.sqrt(v0_low)\n    std_null_upp = np.sqrt(v0_upp)\n    std_alternative = np.sqrt(v1)\n    pow_ = _power_equivalence_het(es_low, es_upp, nobs2, alpha=alpha,\n        std_null_low=std_null_low, std_null_upp=std_null_upp,\n        std_alternative=std_alternative)\n    if return_results:\n        res = HolderTuple(power=pow_[0], power_margins=pow[1:],\n            std_null_low=std_null_low, std_null_upp=std_null_upp, std_alt=\n            std_alternative, nobs1=nobs1, nobs2=nobs2, nobs_ratio=\n            nobs_ratio, alpha=alpha, tuple_=('power',))\n        return res\n    else:\n        return pow_[0]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "mcnemar", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/contingency_tables.py", "method_code_mask": "import warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom statsmodels import iolib\nfrom statsmodels.tools import sm_exceptions\nfrom statsmodels.tools.decorators import cache_readonly\n\n\ndef mcnemar(table, exact=True, correction=True): [MASK]\n", "response": "import warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom statsmodels import iolib\nfrom statsmodels.tools import sm_exceptions\nfrom statsmodels.tools.decorators import cache_readonly\ndef mcnemar(table, exact=True, correction=True):\n    \"\"\"\n    McNemar test of homogeneity.\n\n    Parameters\n    ----------\n    table : array_like\n        A square contingency table.\n    exact : bool\n        If exact is true, then the binomial distribution will be used.\n        If exact is false, then the chisquare distribution will be\n        used, which is the approximation to the distribution of the\n        test statistic for large sample sizes.\n    correction : bool\n        If true, then a continuity correction is used for the chisquare\n        distribution (if exact is false.)\n\n    Returns\n    -------\n    A bunch with attributes:\n\n    statistic : float or int, array\n        The test statistic is the chisquare statistic if exact is\n        false. If the exact binomial distribution is used, then this\n        contains the min(n1, n2), where n1, n2 are cases that are zero\n        in one sample but one in the other sample.\n    pvalue : float or array\n        p-value of the null hypothesis of equal marginal distributions.\n\n    Notes\n    -----\n    This is a special case of Cochran's Q test, and of the homogeneity\n    test. The results when the chisquare distribution is used are\n    identical, except for continuity correction.\n    \"\"\"\n    table = _make_df_square(table)\n    table = np.asarray(table, dtype=np.float64)\n    n1, n2 = table[0, 1], table[1, 0]\n    if exact:\n        statistic = np.minimum(n1, n2)\n        int_sum = int(n1 + n2)\n        if int_sum != n1 + n2:\n            raise ValueError(\n                'exact can only be used with tables containing integers.')\n        pvalue = stats.binom.cdf(statistic, int_sum, 0.5) * 2\n        pvalue = np.minimum(pvalue, 1)\n    else:\n        corr = int(correction)\n        statistic = (np.abs(n1 - n2) - corr) ** 2 / (1.0 * (n1 + n2))\n        df = 1\n        pvalue = stats.chi2.sf(statistic, df)\n    b = _Bunch()\n    b.statistic = statistic\n    b.pvalue = pvalue\n    return b"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "power_poisson_diff_2indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/rates.py", "method_code_mask": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\n\n\ndef power_poisson_diff_2indep(rate1, rate2, nobs1, nobs_ratio=1, alpha=0.05,\n    value=0, method_var='score', alternative='two-sided', return_results=True\n    ): [MASK]\n", "response": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\ndef power_poisson_diff_2indep(rate1, rate2, nobs1, nobs_ratio=1, alpha=0.05,\n    value=0, method_var='score', alternative='two-sided', return_results=True):\n    \"\"\"Power of ztest for the difference between two independent poisson rates.\n\n    Parameters\n    ----------\n    rate1 : float\n        Poisson rate for the first sample, treatment group, under the\n        alternative hypothesis.\n    rate2 : float\n        Poisson rate for the second sample, reference group, under the\n        alternative hypothesis.\n    nobs1 : float or int\n        Number of observations in sample 1.\n    nobs_ratio : float\n        Sample size ratio, nobs2 = nobs_ratio * nobs1.\n    alpha : float in interval (0,1)\n        Significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    value : float\n        Difference between rates 1 and 2 under the null hypothesis.\n    method_var : {\"score\", \"alt\"}\n        The variance of the test statistic for the null hypothesis given the\n        rates uder the alternative, can be either equal to the rates under the\n        alternative ``method_var=\"alt\"``, or estimated under the constrained\n        of the null hypothesis, ``method_var=\"score\"``.\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        Alternative hypothesis whether the power is calculated for a\n        two-sided (default) or one sided test. The one-sided test can be\n        either 'larger', 'smaller'.\n    return_results : bool\n        If true, then a results instance with extra information is returned,\n        otherwise only the computed power is returned.\n\n    Returns\n    -------\n    results : results instance or float\n        If return_results is False, then only the power is returned.\n        If return_results is True, then a results instance with the\n        information in attributes is returned.\n\n        power : float\n            Power of the test, e.g. 0.8, is one minus the probability of a\n            type II error. Power is the probability that the test correctly\n            rejects the Null Hypothesis if the Alternative Hypothesis is true.\n\n        Other attributes in results instance include :\n\n        std_null\n            standard error of difference under the null hypothesis (without\n            sqrt(nobs1))\n        std_alt\n            standard error of difference under the alternative hypothesis\n            (without sqrt(nobs1))\n\n    References\n    ----------\n    .. [1] Stucke, Kathrin, and Meinhard Kieser. 2013. \u201cSample Size\n       Calculations for Noninferiority Trials with Poisson Distributed Count\n       Data.\u201d Biometrical Journal 55 (2): 203\u201316.\n       https://doi.org/10.1002/bimj.201200142.\n    .. [2] PASS manual chapter 436\n\n    \"\"\"\n    from statsmodels.stats.power import normal_power_het\n    rate1, rate2, nobs1 = map(np.asarray, [rate1, rate2, nobs1])\n    diff = rate1 - rate2\n    _, std_null, std_alt = _std_2poisson_power(rate1, rate2, nobs_ratio=\n        nobs_ratio, alpha=alpha, value=value, method_var=method_var)\n    pow_ = normal_power_het(diff - value, nobs1, alpha, std_null=std_null,\n        std_alternative=std_alt, alternative=alternative)\n    if return_results:\n        res = HolderTuple(power=pow_, rates_alt=(rate2 + diff, rate2),\n            std_null=std_null, std_alt=std_alt, nobs1=nobs1, nobs2=\n            nobs_ratio * nobs1, nobs_ratio=nobs_ratio, alpha=alpha, tuple_=\n            ('power',))\n        return res\n    else:\n        return pow_"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "corr_thresholded", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/correlation_tools.py", "method_code_mask": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\n\n\ndef corr_thresholded(data, minabs=None, max_elt=10000000.0): [MASK]\n", "response": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\ndef corr_thresholded(data, minabs=None, max_elt=10000000.0):\n    \"\"\"\n    Construct a sparse matrix containing the thresholded row-wise\n    correlation matrix from a data array.\n\n    Parameters\n    ----------\n    data : array_like\n        The data from which the row-wise thresholded correlation\n        matrix is to be computed.\n    minabs : non-negative real\n        The threshold value; correlation coefficients smaller in\n        magnitude than minabs are set to zero.  If None, defaults\n        to 1 / sqrt(n), see Notes for more information.\n\n    Returns\n    -------\n    cormat : sparse.coo_matrix\n        The thresholded correlation matrix, in COO format.\n\n    Notes\n    -----\n    This is an alternative to C = np.corrcoef(data); C \\\\*= (np.abs(C)\n    >= absmin), suitable for very tall data matrices.\n\n    If the data are jointly Gaussian, the marginal sampling\n    distributions of the elements of the sample correlation matrix are\n    approximately Gaussian with standard deviation 1 / sqrt(n).  The\n    default value of ``minabs`` is thus equal to 1 standard error, which\n    will set to zero approximately 68% of the estimated correlation\n    coefficients for which the population value is zero.\n\n    No intermediate matrix with more than ``max_elt`` values will be\n    constructed.  However memory use could still be high if a large\n    number of correlation values exceed `minabs` in magnitude.\n\n    The thresholded matrix is returned in COO format, which can easily\n    be converted to other sparse formats.\n\n    Examples\n    --------\n    Here X is a tall data matrix (e.g. with 100,000 rows and 50\n    columns).  The row-wise correlation matrix of X is calculated\n    and stored in sparse form, with all entries smaller than 0.3\n    treated as 0.\n\n    >>> import numpy as np\n    >>> np.random.seed(1234)\n    >>> b = 1.5 - np.random.rand(10, 1)\n    >>> x = np.random.randn(100,1).dot(b.T) + np.random.randn(100,10)\n    >>> cmat = corr_thresholded(x, 0.3)\n    \"\"\"\n    nrow, ncol = data.shape\n    if minabs is None:\n        minabs = 1.0 / float(ncol)\n    data = data.copy()\n    data -= data.mean(1)[:, None]\n    sd = data.std(1, ddof=1)\n    ii = np.flatnonzero(sd > 1e-05)\n    data[ii, :] /= sd[ii][:, None]\n    ii = np.flatnonzero(sd <= 1e-05)\n    data[ii, :] = 0\n    bs = int(np.floor(max_elt / nrow))\n    ipos_all, jpos_all, cor_values = [], [], []\n    ir = 0\n    while ir < nrow:\n        ir2 = min(data.shape[0], ir + bs)\n        cm = np.dot(data[ir:ir2, :], data.T) / (ncol - 1)\n        cma = np.abs(cm)\n        ipos, jpos = np.nonzero(cma >= minabs)\n        ipos_all.append(ipos + ir)\n        jpos_all.append(jpos)\n        cor_values.append(cm[ipos, jpos])\n        ir += bs\n    ipos = np.concatenate(ipos_all)\n    jpos = np.concatenate(jpos_all)\n    cor_values = np.concatenate(cor_values)\n    cmat = sparse.coo_matrix((cor_values, (ipos, jpos)), (nrow, nrow))\n    return cmat"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "power_negbin_ratio_2indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/rates.py", "method_code_mask": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\n\n\ndef power_negbin_ratio_2indep(rate1, rate2, nobs1, nobs_ratio=1, exposure=1,\n    value=1, alpha=0.05, dispersion=0.01, alternative='two-sided',\n    method_var='alt', return_results=True): [MASK]\n", "response": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\ndef power_negbin_ratio_2indep(rate1, rate2, nobs1, nobs_ratio=1, exposure=1,\n    value=1, alpha=0.05, dispersion=0.01, alternative='two-sided',\n    method_var='alt', return_results=True):\n    \"\"\"\n    Power of test of ratio of 2 independent negative binomial rates.\n\n    Parameters\n    ----------\n    rate1 : float\n        Poisson rate for the first sample, treatment group, under the\n        alternative hypothesis.\n    rate2 : float\n        Poisson rate for the second sample, reference group, under the\n        alternative hypothesis.\n    nobs1 : float or int\n        Number of observations in sample 1.\n    low : float\n        Lower equivalence margin for the rate ratio, rate1 / rate2.\n    upp : float\n        Upper equivalence margin for the rate ratio, rate1 / rate2.\n    nobs_ratio : float\n        Sample size ratio, nobs2 = nobs_ratio * nobs1.\n    exposure : float\n        Exposure for each observation. Total exposure is nobs1 * exposure\n        and nobs2 * exposure.\n    value : float\n        Rate ratio, rate1 / rate2, under the null hypothesis.\n    alpha : float in interval (0,1)\n        Significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    dispersion : float >= 0.\n        Dispersion parameter for Negative Binomial distribution.\n        The Poisson limiting case corresponds to ``dispersion=0``.\n    method_var : {\"score\", \"alt\"}\n        The variance of the test statistic for the null hypothesis given the\n        rates under the alternative, can be either equal to the rates under the\n        alternative ``method_var=\"alt\"``, or estimated under the constrained\n        of the null hypothesis, ``method_var=\"score\"``, or based on a moment\n        constrained estimate, ``method_var=\"ftotal\"``. see references.\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        Alternative hypothesis whether the power is calculated for a\n        two-sided (default) or one sided test. The one-sided test can be\n        either 'larger', 'smaller'.\n    return_results : bool\n        If true, then a results instance with extra information is returned,\n        otherwise only the computed power is returned.\n\n    Returns\n    -------\n    results : results instance or float\n        If return_results is False, then only the power is returned.\n        If return_results is True, then a results instance with the\n        information in attributes is returned.\n\n        power : float\n            Power of the test, e.g. 0.8, is one minus the probability of a\n            type II error. Power is the probability that the test correctly\n            rejects the Null Hypothesis if the Alternative Hypothesis is true.\n\n        Other attributes in results instance include :\n\n        std_null\n            standard error of difference under the null hypothesis (without\n            sqrt(nobs1))\n        std_alt\n            standard error of difference under the alternative hypothesis\n            (without sqrt(nobs1))\n\n    References\n    ----------\n    .. [1] Zhu, Haiyuan. 2017. \u201cSample Size Calculation for Comparing Two\n       Poisson or Negative Binomial Rates in Noninferiority or Equivalence\n       Trials.\u201d Statistics in Biopharmaceutical Research, March.\n       https://doi.org/10.1080/19466315.2016.1225594\n    .. [2] Zhu, Haiyuan, and Hassan Lakkis. 2014. \u201cSample Size Calculation for\n       Comparing Two Negative Binomial Rates.\u201d Statistics in Medicine 33 (3):\n       376\u201387. https://doi.org/10.1002/sim.5947.\n    .. [3] PASS documentation\n    \"\"\"\n    from statsmodels.stats.power import normal_power_het\n    rate1, rate2, nobs1 = map(np.asarray, [rate1, rate2, nobs1])\n    nobs2 = nobs_ratio * nobs1\n    v1 = (1 / rate1 + 1 / (nobs_ratio * rate2)) / exposure + (1 + nobs_ratio\n        ) / nobs_ratio * dispersion\n    if method_var == 'alt':\n        v0 = v1\n    elif method_var == 'ftotal':\n        v0 = (1 + value * nobs_ratio) ** 2 / (exposure * nobs_ratio * value *\n            (rate1 + nobs_ratio * rate2))\n        v0 += (1 + nobs_ratio) / nobs_ratio * dispersion\n    elif method_var == 'score':\n        v0 = _var_cmle_negbin(rate1, rate2, nobs_ratio, exposure=exposure,\n            value=value, dispersion=dispersion)[0]\n    else:\n        raise NotImplementedError(f'method_var {method_var} not recognized')\n    std_null = np.sqrt(v0)\n    std_alt = np.sqrt(v1)\n    es = np.log(rate1 / rate2) - np.log(value)\n    pow_ = normal_power_het(es, nobs1, alpha, std_null=std_null,\n        std_alternative=std_alt, alternative=alternative)\n    if return_results:\n        res = HolderTuple(power=pow_, std_null=std_null, std_alt=std_alt,\n            nobs1=nobs1, nobs2=nobs2, nobs_ratio=nobs_ratio, alpha=alpha,\n            tuple_=('power',))\n        return res\n    return pow_"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "power_equivalence_neginb_2indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/rates.py", "method_code_mask": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\n\n\ndef power_equivalence_neginb_2indep(rate1, rate2, nobs1, low, upp,\n    nobs_ratio=1, exposure=1, alpha=0.05, dispersion=0, method_var='alt',\n    return_results=False): [MASK]\n", "response": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\ndef power_equivalence_neginb_2indep(rate1, rate2, nobs1, low, upp,\n    nobs_ratio=1, exposure=1, alpha=0.05, dispersion=0, method_var='alt',\n    return_results=False):\n    \"\"\"\n    Power of equivalence test of ratio of 2 indep. negative binomial rates.\n\n    Parameters\n    ----------\n    rate1 : float\n        Poisson rate for the first sample, treatment group, under the\n        alternative hypothesis.\n    rate2 : float\n        Poisson rate for the second sample, reference group, under the\n        alternative hypothesis.\n    nobs1 : float or int\n        Number of observations in sample 1.\n    low : float\n        Lower equivalence margin for the rate ratio, rate1 / rate2.\n    upp : float\n        Upper equivalence margin for the rate ratio, rate1 / rate2.\n    nobs_ratio : float\n        Sample size ratio, nobs2 = nobs_ratio * nobs1.\n    alpha : float in interval (0,1)\n        Significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    dispersion : float >= 0.\n        Dispersion parameter for Negative Binomial distribution.\n        The Poisson limiting case corresponds to ``dispersion=0``.\n    method_var : {\"score\", \"alt\"}\n        The variance of the test statistic for the null hypothesis given the\n        rates under the alternative, can be either equal to the rates under the\n        alternative ``method_var=\"alt\"``, or estimated under the constrained\n        of the null hypothesis, ``method_var=\"score\"``, or based on a moment\n        constrained estimate, ``method_var=\"ftotal\"``. see references.\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        Alternative hypothesis whether the power is calculated for a\n        two-sided (default) or one sided test. The one-sided test can be\n        either 'larger', 'smaller'.\n    return_results : bool\n        If true, then a results instance with extra information is returned,\n        otherwise only the computed power is returned.\n\n    Returns\n    -------\n    results : results instance or float\n        If return_results is False, then only the power is returned.\n        If return_results is True, then a results instance with the\n        information in attributes is returned.\n\n        power : float\n            Power of the test, e.g. 0.8, is one minus the probability of a\n            type II error. Power is the probability that the test correctly\n            rejects the Null Hypothesis if the Alternative Hypothesis is true.\n\n        Other attributes in results instance include :\n\n        std_null\n            standard error of difference under the null hypothesis (without\n            sqrt(nobs1))\n        std_alt\n            standard error of difference under the alternative hypothesis\n            (without sqrt(nobs1))\n\n\n    References\n    ----------\n    .. [1] Zhu, Haiyuan. 2017. \u201cSample Size Calculation for Comparing Two\n       Poisson or Negative Binomial Rates in Noninferiority or Equivalence\n       Trials.\u201d Statistics in Biopharmaceutical Research, March.\n       https://doi.org/10.1080/19466315.2016.1225594\n    .. [2] Zhu, Haiyuan, and Hassan Lakkis. 2014. \u201cSample Size Calculation for\n       Comparing Two Negative Binomial Rates.\u201d Statistics in Medicine 33 (3):\n       376\u201387. https://doi.org/10.1002/sim.5947.\n    .. [3] PASS documentation\n    \"\"\"\n    rate1, rate2, nobs1 = map(np.asarray, [rate1, rate2, nobs1])\n    nobs2 = nobs_ratio * nobs1\n    v1 = (1 / rate2 + 1 / (nobs_ratio * rate1)) / exposure + (1 + nobs_ratio\n        ) / nobs_ratio * dispersion\n    if method_var == 'alt':\n        v0_low = v0_upp = v1\n    elif method_var == 'ftotal':\n        v0_low = (1 + low * nobs_ratio) ** 2 / (exposure * nobs_ratio * low *\n            (rate1 + nobs_ratio * rate2))\n        v0_low += (1 + nobs_ratio) / nobs_ratio * dispersion\n        v0_upp = (1 + upp * nobs_ratio) ** 2 / (exposure * nobs_ratio * upp *\n            (rate1 + nobs_ratio * rate2))\n        v0_upp += (1 + nobs_ratio) / nobs_ratio * dispersion\n    elif method_var == 'score':\n        v0_low = _var_cmle_negbin(rate1, rate2, nobs_ratio, exposure=\n            exposure, value=low, dispersion=dispersion)[0]\n        v0_upp = _var_cmle_negbin(rate1, rate2, nobs_ratio, exposure=\n            exposure, value=upp, dispersion=dispersion)[0]\n    else:\n        raise NotImplementedError(f'method_var {method_var} not recognized')\n    es_low = np.log(rate1 / rate2) - np.log(low)\n    es_upp = np.log(rate1 / rate2) - np.log(upp)\n    std_null_low = np.sqrt(v0_low)\n    std_null_upp = np.sqrt(v0_upp)\n    std_alternative = np.sqrt(v1)\n    pow_ = _power_equivalence_het(es_low, es_upp, nobs1, alpha=alpha,\n        std_null_low=std_null_low, std_null_upp=std_null_upp,\n        std_alternative=std_alternative)\n    if return_results:\n        res = HolderTuple(power=pow_[0], power_margins=pow[1:],\n            std_null_low=std_null_low, std_null_upp=std_null_upp, std_alt=\n            std_alternative, nobs1=nobs1, nobs2=nobs2, nobs_ratio=\n            nobs_ratio, alpha=alpha, tuple_=('power',))\n        return res\n    else:\n        return pow_[0]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/regression/tests/test_rolling.py", "method_code_mask": "from io import BytesIO\nfrom itertools import product\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_array_equal\nfrom statsmodels import tools\nfrom statsmodels.regression.linear_model import WLS\nfrom statsmodels.regression.rolling import RollingWLS\nfrom statsmodels.regression.rolling import RollingOLS\nimport matplotlib.pyplot as plt\n\n\ndef gen_data(nobs, nvar, const, pandas=False, missing=0.0, weights=False): [M\n    ASK]\n", "response": "from io import BytesIO\nfrom itertools import product\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_array_equal\nfrom statsmodels import tools\nfrom statsmodels.regression.linear_model import WLS\nfrom statsmodels.regression.rolling import RollingWLS\nfrom statsmodels.regression.rolling import RollingOLS\nimport matplotlib.pyplot as plt\ndef gen_data(nobs, nvar, const, pandas=False, missing=0.0, weights=False):\n    rs = np.random.RandomState(987499302)\n    x = rs.standard_normal((nobs, nvar))\n    cols = [f'x{i}' for i in range(nvar)]\n    if const:\n        x = tools.add_constant(x)\n        cols = ['const'] + cols\n    if missing > 0.0:\n        mask = rs.random_sample(x.shape) < missing\n        x[mask] = np.nan\n    if x.shape[1] > 1:\n        y = x[:, :-1].sum(1) + rs.standard_normal(nobs)\n    else:\n        y = x.sum(1) + rs.standard_normal(nobs)\n    w = rs.chisquare(5, y.shape[0]) / 5\n    if pandas:\n        idx = pd.date_range('12-31-1999', periods=nobs)\n        x = pd.DataFrame(x, index=idx, columns=cols)\n        y = pd.Series(y, index=idx, name='y')\n        w = pd.Series(w, index=idx, name='weights')\n    if not weights:\n        w = None\n    return y, x, w"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_calc_approx_inv_cov", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/regularized_covariance.py", "method_code_mask": "from statsmodels.regression.linear_model import OLS\nimport numpy as np\n\n\ndef _calc_approx_inv_cov(nodewise_row_l, nodewise_weight_l): [MASK]\n", "response": "from statsmodels.regression.linear_model import OLS\nimport numpy as np\ndef _calc_approx_inv_cov(nodewise_row_l, nodewise_weight_l):\n    \"\"\"calculates the approximate inverse covariance matrix\n\n    Parameters\n    ----------\n    nodewise_row_l : list\n        A list of array-like object where each object corresponds to\n        the nodewise_row values for the corresponding variable, should\n        be length p.\n    nodewise_weight_l : list\n        A list of scalars where each scalar corresponds to the nodewise_weight\n        value for the corresponding variable, should be length p.\n\n    Returns\n    ------\n    An array-like object, p x p matrix\n\n    Notes\n    -----\n\n    nwr = nodewise_row\n    nww = nodewise_weight\n\n    approx_inv_cov_j = - 1 / nww_j [nwr_j,1,...,1,...nwr_j,p]\n    \"\"\"\n    p = len(nodewise_weight_l)\n    approx_inv_cov = -np.eye(p)\n    for idx in range(p):\n        ind = list(range(p))\n        ind.pop(idx)\n        approx_inv_cov[idx, ind] = nodewise_row_l[idx]\n    approx_inv_cov *= -1 / nodewise_weight_l[:, None] ** 2\n    return approx_inv_cov"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "power_poisson_ratio_2indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/rates.py", "method_code_mask": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\n\n\ndef power_poisson_ratio_2indep(rate1, rate2, nobs1, nobs_ratio=1, exposure=\n    1, value=0, alpha=0.05, dispersion=1, alternative='smaller', method_var\n    ='alt', return_results=True): [MASK]\n", "response": "import numpy as np\nimport warnings\nfrom scipy import stats\nfrom scipy import optimize\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.stats._inference_tools import _mover_confint\nfrom statsmodels.stats import proportion\nfrom statsmodels.stats.power import normal_power_het\ndef power_poisson_ratio_2indep(rate1, rate2, nobs1, nobs_ratio=1, exposure=\n    1, value=0, alpha=0.05, dispersion=1, alternative='smaller', method_var\n    ='alt', return_results=True):\n    \"\"\"Power of test of ratio of 2 independent poisson rates.\n\n    This is based on Zhu and Zhu and Lakkis. It does not directly correspond\n    to `test_poisson_2indep`.\n\n    Parameters\n    ----------\n    rate1 : float\n        Poisson rate for the first sample, treatment group, under the\n        alternative hypothesis.\n    rate2 : float\n        Poisson rate for the second sample, reference group, under the\n        alternative hypothesis.\n    nobs1 : float or int\n        Number of observations in sample 1.\n    nobs_ratio : float\n        Sample size ratio, nobs2 = nobs_ratio * nobs1.\n    exposure : float\n        Exposure for each observation. Total exposure is nobs1 * exposure\n        and nobs2 * exposure.\n    alpha : float in interval (0,1)\n        Significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    value : float\n        Rate ratio, rate1 / rate2, under the null hypothesis.\n    dispersion : float\n        Dispersion coefficient for quasi-Poisson. Dispersion different from\n        one can capture over or under dispersion relative to Poisson\n        distribution.\n    method_var : {\"score\", \"alt\"}\n        The variance of the test statistic for the null hypothesis given the\n        rates under the alternative can be either equal to the rates under the\n        alternative ``method_var=\"alt\"``, or estimated under the constrained\n        of the null hypothesis, ``method_var=\"score\"``.\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        Alternative hypothesis whether the power is calculated for a\n        two-sided (default) or one sided test. The one-sided test can be\n        either 'larger', 'smaller'.\n    return_results : bool\n        If true, then a results instance with extra information is returned,\n        otherwise only the computed power is returned.\n\n    Returns\n    -------\n    results : results instance or float\n        If return_results is False, then only the power is returned.\n        If return_results is True, then a results instance with the\n        information in attributes is returned.\n\n        power : float\n            Power of the test, e.g. 0.8, is one minus the probability of a\n            type II error. Power is the probability that the test correctly\n            rejects the Null Hypothesis if the Alternative Hypothesis is true.\n\n        Other attributes in results instance include :\n\n        std_null\n            standard error of difference under the null hypothesis (without\n            sqrt(nobs1))\n        std_alt\n            standard error of difference under the alternative hypothesis\n            (without sqrt(nobs1))\n\n    References\n    ----------\n    .. [1] Zhu, Haiyuan. 2017. \u201cSample Size Calculation for Comparing Two\n       Poisson or Negative Binomial Rates in Noninferiority or Equivalence\n       Trials.\u201d Statistics in Biopharmaceutical Research, March.\n       https://doi.org/10.1080/19466315.2016.1225594\n    .. [2] Zhu, Haiyuan, and Hassan Lakkis. 2014. \u201cSample Size Calculation for\n       Comparing Two Negative Binomial Rates.\u201d Statistics in Medicine 33 (3):\n       376\u201387. https://doi.org/10.1002/sim.5947.\n    .. [3] PASS documentation\n    \"\"\"\n    from statsmodels.stats.power import normal_power_het\n    rate1, rate2, nobs1 = map(np.asarray, [rate1, rate2, nobs1])\n    nobs2 = nobs_ratio * nobs1\n    v1 = dispersion / exposure * (1 / rate1 + 1 / (nobs_ratio * rate2))\n    if method_var == 'alt':\n        v0 = v1\n    elif method_var == 'score':\n        v0 = dispersion / exposure * (1 + value / nobs_ratio) ** 2\n        v0 /= value / nobs_ratio * (rate1 + nobs_ratio * rate2)\n    else:\n        raise NotImplementedError(f'method_var {method_var} not recognized')\n    std_null = np.sqrt(v0)\n    std_alt = np.sqrt(v1)\n    es = np.log(rate1 / rate2) - np.log(value)\n    pow_ = normal_power_het(es, nobs1, alpha, std_null=std_null,\n        std_alternative=std_alt, alternative=alternative)\n    p_pooled = None\n    if return_results:\n        res = HolderTuple(power=pow_, p_pooled=p_pooled, std_null=std_null,\n            std_alt=std_alt, nobs1=nobs1, nobs2=nobs2, nobs_ratio=\n            nobs_ratio, alpha=alpha, tuple_=('power',))\n        return res\n    return pow_"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "confint_mvmean", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/multivariate.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.tools.validation import array_like\n\n\ndef confint_mvmean(data, lin_transf=None, alpha=0.5, simult=False): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.tools.validation import array_like\ndef confint_mvmean(data, lin_transf=None, alpha=0.5, simult=False):\n    \"\"\"Confidence interval for linear transformation of a multivariate mean\n\n    Either pointwise or simultaneous confidence intervals are returned.\n\n    Parameters\n    ----------\n    data : array_like\n        data with observations in rows and variables in columns\n    lin_transf : array_like or None\n        The linear transformation or contrast matrix for transforming the\n        vector of means. If this is None, then the identity matrix is used\n        which specifies the means themselves.\n    alpha : float in (0, 1)\n        confidence level for the confidence interval, commonly used is\n        alpha=0.05.\n    simult : bool\n        If ``simult`` is False (default), then the pointwise confidence\n        interval is returned.\n        Otherwise, a simultaneous confidence interval is returned.\n        Warning: additional simultaneous confidence intervals might be added\n        and the default for those might change.\n\n    Returns\n    -------\n    low : ndarray\n        lower confidence bound on the linear transformed\n    upp : ndarray\n        upper confidence bound on the linear transformed\n    values : ndarray\n        mean or their linear transformation, center of the confidence region\n\n    Notes\n    -----\n    Pointwise confidence interval is based on Johnson and Wichern\n    equation (5-21) page 224.\n\n    Simultaneous confidence interval is based on Johnson and Wichern\n    Result 5.3 page 225.\n    This looks like Sheffe simultaneous confidence intervals.\n\n    Bonferroni corrected simultaneous confidence interval might be added in\n    future\n\n    References\n    ----------\n    Johnson, Richard A., and Dean W. Wichern. 2007. Applied Multivariate\n    Statistical Analysis. 6th ed. Upper Saddle River, N.J: Pearson Prentice\n    Hall.\n    \"\"\"\n    x = np.asarray(data)\n    nobs, k_vars = x.shape\n    if lin_transf is None:\n        lin_transf = np.eye(k_vars)\n    mean = x.mean(0)\n    cov = np.cov(x, rowvar=False, ddof=0)\n    ci = confint_mvmean_fromstats(mean, cov, nobs, lin_transf=lin_transf,\n        alpha=alpha, simult=simult)\n    return ci"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "durbin_watson", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/stattools.py", "method_code_mask": "from scipy import stats\nimport numpy as np\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom warnings import warn\n\n\ndef durbin_watson(resids, axis=0): [MASK]\n", "response": "from scipy import stats\nimport numpy as np\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom warnings import warn\ndef durbin_watson(resids, axis=0):\n    \"\"\"\n    Calculates the Durbin-Watson statistic.\n\n    Parameters\n    ----------\n    resids : array_like\n        Data for which to compute the Durbin-Watson statistic. Usually\n        regression model residuals.\n    axis : int, optional\n        Axis to use if data has more than 1 dimension. Default is 0.\n\n    Returns\n    -------\n    dw : float, array_like\n        The Durbin-Watson statistic.\n\n    Notes\n    -----\n    The null hypothesis of the test is that there is no serial correlation\n    in the residuals.\n    The Durbin-Watson test statistic is defined as:\n\n    .. math::\n\n       \\\\sum_{t=2}^T((e_t - e_{t-1})^2)/\\\\sum_{t=1}^Te_t^2\n\n    The test statistic is approximately equal to 2*(1-r) where ``r`` is the\n    sample autocorrelation of the residuals. Thus, for r == 0, indicating no\n    serial correlation, the test statistic equals 2. This statistic will\n    always be between 0 and 4. The closer to 0 the statistic, the more\n    evidence for positive serial correlation. The closer to 4, the more\n    evidence for negative serial correlation.\n    \"\"\"\n    resids = np.asarray(resids)\n    diff_resids = np.diff(resids, 1, axis=axis)\n    dw = np.sum(diff_resids ** 2, axis=axis) / np.sum(resids ** 2, axis=axis)\n    return dw"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_contrast_pairs", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/contrast.py", "method_code_mask": "import numpy as np\nfrom scipy.stats import f as fdist\nfrom scipy.stats import t as student_t\nfrom scipy import stats\nfrom statsmodels.tools.tools import clean0\nfrom statsmodels.tools.tools import fullrank\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.iolib.summary import summary_params\nfrom statsmodels.iolib.summary import summary_params_frame\nimport statsmodels.sandbox.stats.multicomp as mc\n\n\ndef _contrast_pairs(k_params, k_level, idx_start): [MASK]\n", "response": "import numpy as np\nfrom scipy.stats import f as fdist\nfrom scipy.stats import t as student_t\nfrom scipy import stats\nfrom statsmodels.tools.tools import clean0\nfrom statsmodels.tools.tools import fullrank\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.iolib.summary import summary_params\nfrom statsmodels.iolib.summary import summary_params_frame\nimport statsmodels.sandbox.stats.multicomp as mc\ndef _contrast_pairs(k_params, k_level, idx_start):\n    \"\"\"create pairwise contrast for reference coding\n\n    currently not used,\n    using encoding contrast matrix is more general, but requires requires\n    factor information from patsy design_info.\n\n\n    Parameters\n    ----------\n    k_params : int\n        number of parameters\n    k_level : int\n        number of levels or categories (including reference case)\n    idx_start : int\n        Index of the first parameter of this factor. The restrictions on the\n        factor are inserted as a block in the full restriction matrix starting\n        at column with index `idx_start`.\n\n    Returns\n    -------\n    contrasts : ndarray\n        restriction matrix with k_params columns and number of rows equal to\n        the number of restrictions.\n    \"\"\"\n    k_level_m1 = k_level - 1\n    idx_pairs = np.triu_indices(k_level_m1, 1)\n    k = len(idx_pairs[0])\n    c_pairs = np.zeros((k, k_level_m1))\n    c_pairs[np.arange(k), idx_pairs[0]] = -1\n    c_pairs[np.arange(k), idx_pairs[1]] = 1\n    c_reference = np.eye(k_level_m1)\n    c = np.concatenate((c_reference, c_pairs), axis=0)\n    k_all = c.shape[0]\n    contrasts = np.zeros((k_all, k_params))\n    contrasts[:, idx_start:idx_start + k_level_m1] = c\n    return contrasts"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "qsturng", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/libqsturng/qsturng_.py", "method_code_mask": "from statsmodels.compat.python import lrange\nimport math\nimport scipy.stats\nimport numpy as np\nfrom scipy.optimize import fminbound\n\n\ndef qsturng(p, r, v): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nimport math\nimport scipy.stats\nimport numpy as np\nfrom scipy.optimize import fminbound\ndef qsturng(p, r, v):\n    \"\"\"Approximates the quantile p for a studentized range\n       distribution having v degrees of freedom and r samples\n       for probability p.\n\n    Parameters\n    ----------\n    p : (scalar, array_like)\n        The cumulative probability value\n        p >= .1 and p <=.999\n        (values under .5 are not recommended)\n    r : (scalar, array_like)\n        The number of samples\n        r >= 2 and r <= 200\n        (values over 200 are permitted but not recommended)\n    v : (scalar, array_like)\n        The sample degrees of freedom\n        if p >= .9:\n            v >=1 and v >= inf\n        else:\n            v >=2 and v >= inf\n\n    Returns\n    -------\n    q : (scalar, array_like)\n        approximation of the Studentized Range\n    \"\"\"\n    if all(map(_isfloat, [p, r, v])):\n        return _qsturng(p, r, v)\n    return _vqsturng(p, r, v)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "mcnemar", "method_path": "../srcdata/Computation/statsmodels/statsmodels/sandbox/stats/runs.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import comb\nimport warnings\nfrom statsmodels.tools.validation import array_like\n\n\ndef mcnemar(x, y=None, exact=True, correction=True): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import comb\nimport warnings\nfrom statsmodels.tools.validation import array_like\ndef mcnemar(x, y=None, exact=True, correction=True):\n    \"\"\"McNemar test\n\n    Parameters\n    ----------\n    x, y : array_like\n        two paired data samples. If y is None, then x can be a 2 by 2\n        contingency table. x and y can have more than one dimension, then\n        the results are calculated under the assumption that axis zero\n        contains the observation for the samples.\n    exact : bool\n        If exact is true, then the binomial distribution will be used.\n        If exact is false, then the chisquare distribution will be used, which\n        is the approximation to the distribution of the test statistic for\n        large sample sizes.\n    correction : bool\n        If true, then a continuity correction is used for the chisquare\n        distribution (if exact is false.)\n\n    Returns\n    -------\n    stat : float or int, array\n        The test statistic is the chisquare statistic if exact is false. If the\n        exact binomial distribution is used, then this contains the min(n1, n2),\n        where n1, n2 are cases that are zero in one sample but one in the other\n        sample.\n\n    pvalue : float or array\n        p-value of the null hypothesis of equal effects.\n\n    Notes\n    -----\n    This is a special case of Cochran's Q test. The results when the chisquare\n    distribution is used are identical, except for continuity correction.\n\n    \"\"\"\n    warnings.warn('Deprecated, use stats.TableSymmetry instead', FutureWarning)\n    x = np.asarray(x)\n    if y is None and x.shape[0] == x.shape[1]:\n        if x.shape[0] != 2:\n            raise ValueError('table needs to be 2 by 2')\n        n1, n2 = x[1, 0], x[0, 1]\n    else:\n        n1 = np.sum(x < y, 0)\n        n2 = np.sum(x > y, 0)\n    if exact:\n        stat = np.minimum(n1, n2)\n        pval = stats.binom.cdf(stat, n1 + n2, 0.5) * 2\n        pval = np.minimum(pval, 1)\n    else:\n        corr = int(correction)\n        stat = (np.abs(n1 - n2) - corr) ** 2 / (1.0 * (n1 + n2))\n        df = 1\n        pval = stats.chi2.sf(stat, df)\n    return stat, pval"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "trimboth", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/robust_compare.py", "method_code_mask": "import numbers\nimport numpy as np\nimport statsmodels.stats.weightstats as smws\n\n\ndef trimboth(a, proportiontocut, axis=0): [MASK]\n", "response": "import numbers\nimport numpy as np\nimport statsmodels.stats.weightstats as smws\ndef trimboth(a, proportiontocut, axis=0):\n    \"\"\"\n    Slices off a proportion of items from both ends of an array.\n\n    Slices off the passed proportion of items from both ends of the passed\n    array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n    rightmost 10% of scores).  You must pre-sort the array if you want\n    'proper' trimming.  Slices off less if proportion results in a\n    non-integer slice index (i.e., conservatively slices off\n    `proportiontocut`).\n\n    Parameters\n    ----------\n    a : array_like\n        Data to trim.\n    proportiontocut : float or int\n        Proportion of data to trim at each end.\n    axis : int or None\n        Axis along which the observations are trimmed. The default is to trim\n        along axis=0. If axis is None then the array will be flattened before\n        trimming.\n\n    Returns\n    -------\n    out : array-like\n        Trimmed version of array `a`.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.arange(20)\n    >>> b = stats.trimboth(a, 0.1)\n    >>> b.shape\n    (16,)\n\n    \"\"\"\n    a = np.asarray(a)\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut\n    if lowercut >= uppercut:\n        raise ValueError('Proportion too big.')\n    sl = [slice(None)] * a.ndim\n    sl[axis] = slice(lowercut, uppercut)\n    return a[tuple(sl)]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_calc_nodewise_weight", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/regularized_covariance.py", "method_code_mask": "from statsmodels.regression.linear_model import OLS\nimport numpy as np\n\n\ndef _calc_nodewise_weight(exog, nodewise_row, idx, alpha): [MASK]\n", "response": "from statsmodels.regression.linear_model import OLS\nimport numpy as np\ndef _calc_nodewise_weight(exog, nodewise_row, idx, alpha):\n    \"\"\"calculates the nodewise_weightvalue for the idxth variable, used to\n    estimate approx_inv_cov.\n\n    Parameters\n    ----------\n    exog : array_like\n        The weighted design matrix for the current partition.\n    nodewise_row : array_like\n        The nodewise_row values for the current variable.\n    idx : scalar\n        Index of the current variable\n    alpha : scalar or array_like\n        The penalty weight.  If a scalar, the same penalty weight\n        applies to all variables in the model.  If a vector, it\n        must have the same length as `params`, and contains a\n        penalty weight for each coefficient.\n\n    Returns\n    -------\n    A scalar\n\n    Notes\n    -----\n\n    nodewise_weight_i = sqrt(1/n ||exog,i - exog_-i nodewise_row||_2^2\n                             + alpha ||nodewise_row||_1)\n    \"\"\"\n    n, p = exog.shape\n    ind = list(range(p))\n    ind.pop(idx)\n    if not np.isscalar(alpha):\n        alpha = alpha[ind]\n    d = np.linalg.norm(exog[:, idx] - exog[:, ind].dot(nodewise_row)) ** 2\n    d = np.sqrt(d / n + alpha * np.linalg.norm(nodewise_row, 1))\n    return d"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "psturng", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/libqsturng/qsturng_.py", "method_code_mask": "from statsmodels.compat.python import lrange\nimport math\nimport scipy.stats\nimport numpy as np\nfrom scipy.optimize import fminbound\n\n\ndef psturng(q, r, v): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nimport math\nimport scipy.stats\nimport numpy as np\nfrom scipy.optimize import fminbound\ndef psturng(q, r, v):\n    \"\"\"Evaluates the probability from 0 to q for a studentized\n       range having v degrees of freedom and r samples.\n\n    Parameters\n    ----------\n    q : (scalar, array_like)\n        quantile value of Studentized Range\n        q >= 0.\n    r : (scalar, array_like)\n        The number of samples\n        r >= 2 and r <= 200\n        (values over 200 are permitted but not recommended)\n    v : (scalar, array_like)\n        The sample degrees of freedom\n        if p >= .9:\n            v >=1 and v >= inf\n        else:\n            v >=2 and v >= inf\n\n    Returns\n    -------\n    p : (scalar, array_like)\n        1. - area from zero to q under the Studentized Range\n        distribution. When v == 1, p is bound between .001\n        and .1, when v > 1, p is bound between .001 and .9.\n        Values between .5 and .9 are 1st order appoximations.\n    \"\"\"\n    if all(map(_isfloat, [q, r, v])):\n        return _psturng(q, r, v)\n    return _vpsturng(q, r, v)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_calc_nodewise_row", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/regularized_covariance.py", "method_code_mask": "from statsmodels.regression.linear_model import OLS\nimport numpy as np\n\n\ndef _calc_nodewise_row(exog, idx, alpha): [MASK]\n", "response": "from statsmodels.regression.linear_model import OLS\nimport numpy as np\ndef _calc_nodewise_row(exog, idx, alpha):\n    \"\"\"calculates the nodewise_row values for the idxth variable, used to\n    estimate approx_inv_cov.\n\n    Parameters\n    ----------\n    exog : array_like\n        The weighted design matrix for the current partition.\n    idx : scalar\n        Index of the current variable.\n    alpha : scalar or array_like\n        The penalty weight.  If a scalar, the same penalty weight\n        applies to all variables in the model.  If a vector, it\n        must have the same length as `params`, and contains a\n        penalty weight for each coefficient.\n\n    Returns\n    -------\n    An array-like object of length p-1\n\n    Notes\n    -----\n\n    nodewise_row_i = arg min 1/(2n) ||exog_i - exog_-i gamma||_2^2\n                             + alpha ||gamma||_1\n    \"\"\"\n    p = exog.shape[1]\n    ind = list(range(p))\n    ind.pop(idx)\n    if not np.isscalar(alpha):\n        alpha = alpha[ind]\n    tmod = OLS(exog[:, idx], exog[:, ind])\n    nodewise_row = tmod.fit_regularized(alpha=alpha).params\n    return nodewise_row"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "trim_mean", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/robust_compare.py", "method_code_mask": "import numbers\nimport numpy as np\nimport statsmodels.stats.weightstats as smws\n\n\ndef trim_mean(a, proportiontocut, axis=0): [MASK]\n", "response": "import numbers\nimport numpy as np\nimport statsmodels.stats.weightstats as smws\ndef trim_mean(a, proportiontocut, axis=0):\n    \"\"\"\n    Return mean of array after trimming observations from both tails.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' and 'rightmost' 10% of\n    scores. Slices off LESS if proportion results in a non-integer slice\n    index (i.e., conservatively slices off `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array\n    proportiontocut : float\n        Fraction to cut off at each tail of the sorted observations.\n    axis : int or None\n        Axis along which the trimmed means are computed. The default is axis=0.\n        If axis is None then the trimmed mean will be computed for the\n        flattened array.\n\n    Returns\n    -------\n    trim_mean : ndarray\n        Mean of trimmed array.\n\n    \"\"\"\n    newa = trimboth(np.sort(a, axis), proportiontocut, axis=axis)\n    return np.mean(newa, axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "confint_mvmean_fromstats", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/multivariate.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.tools.validation import array_like\n\n\ndef confint_mvmean_fromstats(mean, cov, nobs, lin_transf=None, alpha=0.05,\n    simult=False): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.tools.validation import array_like\ndef confint_mvmean_fromstats(mean, cov, nobs, lin_transf=None, alpha=0.05,\n    simult=False):\n    \"\"\"Confidence interval for linear transformation of a multivariate mean\n\n    Either pointwise or simultaneous confidence intervals are returned.\n    Data is provided in the form of summary statistics, mean, cov, nobs.\n\n    Parameters\n    ----------\n    mean : ndarray\n    cov : ndarray\n    nobs : int\n    lin_transf : array_like or None\n        The linear transformation or contrast matrix for transforming the\n        vector of means. If this is None, then the identity matrix is used\n        which specifies the means themselves.\n    alpha : float in (0, 1)\n        confidence level for the confidence interval, commonly used is\n        alpha=0.05.\n    simult : bool\n        If simult is False (default), then pointwise confidence interval is\n        returned.\n        Otherwise, a simultaneous confidence interval is returned.\n        Warning: additional simultaneous confidence intervals might be added\n        and the default for those might change.\n\n    Notes\n    -----\n    Pointwise confidence interval is based on Johnson and Wichern\n    equation (5-21) page 224.\n\n    Simultaneous confidence interval is based on Johnson and Wichern\n    Result 5.3 page 225.\n    This looks like Sheffe simultaneous confidence intervals.\n\n    Bonferroni corrected simultaneous confidence interval might be added in\n    future\n\n    References\n    ----------\n    Johnson, Richard A., and Dean W. Wichern. 2007. Applied Multivariate\n    Statistical Analysis. 6th ed. Upper Saddle River, N.J: Pearson Prentice\n    Hall.\n\n    \"\"\"\n    mean = np.asarray(mean)\n    cov = np.asarray(cov)\n    c = np.atleast_2d(lin_transf)\n    k_vars = len(mean)\n    if simult is False:\n        values = c.dot(mean)\n        quad_form = (c * cov.dot(c.T).T).sum(1)\n        df = nobs - 1\n        t_critval = stats.t.isf(alpha / 2, df)\n        ci_diff = np.sqrt(quad_form / df) * t_critval\n        low = values - ci_diff\n        upp = values + ci_diff\n    else:\n        values = c.dot(mean)\n        quad_form = (c * cov.dot(c.T).T).sum(1)\n        factor = (nobs - 1) * k_vars / (nobs - k_vars) / nobs\n        df = k_vars, nobs - k_vars\n        f_critval = stats.f.isf(alpha, df[0], df[1])\n        ci_diff = np.sqrt(factor * quad_form * f_critval)\n        low = values - ci_diff\n        upp = values + ci_diff\n    return low, upp, values"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "robust_skewness", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/stattools.py", "method_code_mask": "from scipy import stats\nimport numpy as np\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom warnings import warn\n\n\ndef robust_skewness(y, axis=0): [MASK]\n", "response": "from scipy import stats\nimport numpy as np\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom warnings import warn\ndef robust_skewness(y, axis=0):\n    \"\"\"\n    Calculates the four skewness measures in Kim & White\n\n    Parameters\n    ----------\n    y : array_like\n        Data to compute use in the estimator.\n    axis : int or None, optional\n        Axis along which the skewness measures are computed.  If `None`, the\n        entire array is used.\n\n    Returns\n    -------\n    sk1 : ndarray\n          The standard skewness estimator.\n    sk2 : ndarray\n          Skewness estimator based on quartiles.\n    sk3 : ndarray\n          Skewness estimator based on mean-median difference, standardized by\n          absolute deviation.\n    sk4 : ndarray\n          Skewness estimator based on mean-median difference, standardized by\n          standard deviation.\n\n    Notes\n    -----\n    The robust skewness measures are defined\n\n    .. math::\n\n        SK_{2}=\\\\frac{\\\\left(q_{.75}-q_{.5}\\\\right)\n        -\\\\left(q_{.5}-q_{.25}\\\\right)}{q_{.75}-q_{.25}}\n\n    .. math::\n\n        SK_{3}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}\n        {\\\\hat{E}\\\\left[\\\\left|y-\\\\hat{\\\\mu}\\\\right|\\\\right]}\n\n    .. math::\n\n        SK_{4}=\\\\frac{\\\\mu-\\\\hat{q}_{0.5}}{\\\\hat{\\\\sigma}}\n\n    .. [*] Tae-Hwan Kim and Halbert White, \"On more robust estimation of\n       skewness and kurtosis,\" Finance Research Letters, vol. 1, pp. 56-73,\n       March 2004.\n    \"\"\"\n    if axis is None:\n        y = y.ravel()\n        axis = 0\n    y = np.sort(y, axis)\n    q1, q2, q3 = np.percentile(y, [25.0, 50.0, 75.0], axis=axis)\n    mu = y.mean(axis)\n    shape = y.size,\n    if axis is not None:\n        shape = list(mu.shape)\n        shape.insert(axis, 1)\n        shape = tuple(shape)\n    mu_b = np.reshape(mu, shape)\n    q2_b = np.reshape(q2, shape)\n    sigma = np.sqrt(np.mean((y - mu_b) ** 2, axis))\n    sk1 = stats.skew(y, axis=axis)\n    sk2 = (q1 + q3 - 2.0 * q2) / (q3 - q1)\n    sk3 = (mu - q2) / np.mean(abs(y - q2_b), axis=axis)\n    sk4 = (mu - q2) / sigma\n    return sk1, sk2, sk3, sk4"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "wellek_to_f2", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/oneway.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\n\n\ndef wellek_to_f2(eps, n_groups): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\ndef wellek_to_f2(eps, n_groups):\n    \"\"\"Convert Wellek's effect size (sqrt) to Cohen's f-squared\n\n    This computes the following effect size :\n\n       f2 = 1 / n_groups * eps**2\n\n    Parameters\n    ----------\n    eps : float or ndarray\n        Wellek's effect size used in anova equivalence test\n    n_groups : int\n        Number of groups in oneway comparison\n\n    Returns\n    -------\n    f2 : effect size Cohen's f-squared\n\n    \"\"\"\n    f2 = 1 / n_groups * eps ** 2\n    return f2"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "simulate_power_equivalence_oneway", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/oneway.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\n\n\ndef simulate_power_equivalence_oneway(means, nobs, equiv_margin, vars_=None,\n    k_mc=1000, trim_frac=0, options_var=None, margin_type='f2'): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\ndef simulate_power_equivalence_oneway(means, nobs, equiv_margin, vars_=None,\n    k_mc=1000, trim_frac=0, options_var=None, margin_type='f2'):\n    \"\"\"Simulate Power for oneway equivalence test (Wellek's Anova)\n\n    This function is experimental and written to evaluate asymptotic power\n    function. This function will change without backwards compatibility\n    constraints. The only part that is stable is `pvalue` attribute in results.\n\n    Effect size for equivalence margin\n\n    \"\"\"\n    if options_var is None:\n        options_var = ['unequal', 'equal', 'bf']\n    if vars_ is not None:\n        stds = np.sqrt(vars_)\n    else:\n        stds = np.ones(len(means))\n    nobs_mean = nobs.mean()\n    n_groups = len(nobs)\n    res_mc = []\n    f_mc = []\n    reject_mc = []\n    other_mc = []\n    for _ in range(k_mc):\n        y0, y1, y2, y3 = (m + std * np.random.randn(n) for n, m, std in zip\n            (nobs, means, stds))\n        res_i = []\n        f_i = []\n        reject_i = []\n        other_i = []\n        for uv in options_var:\n            res0 = anova_oneway([y0, y1, y2, y3], use_var=uv, trim_frac=\n                trim_frac)\n            f_stat = res0.statistic\n            res1 = equivalence_oneway_generic(f_stat, n_groups, nobs.sum(),\n                equiv_margin, res0.df, alpha=0.05, margin_type=margin_type)\n            res_i.append(res1.pvalue)\n            es_wellek = f_stat * (n_groups - 1) / nobs_mean\n            f_i.append(es_wellek)\n            reject_i.append(res1.reject)\n            other_i.extend([res1.crit_f, res1.crit_es, res1.power_zero])\n        res_mc.append(res_i)\n        f_mc.append(f_i)\n        reject_mc.append(reject_i)\n        other_mc.append(other_i)\n    f_mc = np.asarray(f_mc)\n    other_mc = np.asarray(other_mc)\n    res_mc = np.asarray(res_mc)\n    reject_mc = np.asarray(reject_mc)\n    res = Holder(f_stat=f_mc, other=other_mc, pvalue=res_mc, reject=reject_mc)\n    return res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "proportion_effectsize", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/proportion.py", "method_code_mask": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\n\n\ndef proportion_effectsize(prop1, prop2, method='normal'): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\ndef proportion_effectsize(prop1, prop2, method='normal'):\n    \"\"\"\n    Effect size for a test comparing two proportions\n\n    for use in power function\n\n    Parameters\n    ----------\n    prop1, prop2 : float or array_like\n        The proportion value(s).\n\n    Returns\n    -------\n    es : float or ndarray\n        effect size for (transformed) prop1 - prop2\n\n    Notes\n    -----\n    only method='normal' is implemented to match pwr.p2.test\n    see http://www.statmethods.net/stats/power.html\n\n    Effect size for `normal` is defined as ::\n\n        2 * (arcsin(sqrt(prop1)) - arcsin(sqrt(prop2)))\n\n    I think other conversions to normality can be used, but I need to check.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> sm.stats.proportion_effectsize(0.5, 0.4)\n    0.20135792079033088\n    >>> sm.stats.proportion_effectsize([0.3, 0.4, 0.5], 0.4)\n    array([-0.21015893,  0.        ,  0.20135792])\n\n    \"\"\"\n    if method != 'normal':\n        raise ValueError('only \"normal\" is implemented')\n    es = 2 * (np.arcsin(np.sqrt(prop1)) - np.arcsin(np.sqrt(prop2)))\n    return es"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "medcouple", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/stattools.py", "method_code_mask": "from scipy import stats\nimport numpy as np\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom warnings import warn\n\n\ndef medcouple(y, axis=0): [MASK]\n", "response": "from scipy import stats\nimport numpy as np\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom warnings import warn\ndef medcouple(y, axis=0):\n    \"\"\"\n    Calculate the medcouple robust measure of skew.\n\n    Parameters\n    ----------\n    y : array_like\n        Data to compute use in the estimator.\n    axis : {int, None}\n        Axis along which the medcouple statistic is computed.  If `None`, the\n        entire array is used.\n\n    Returns\n    -------\n    mc : ndarray\n        The medcouple statistic with the same shape as `y`, with the specified\n        axis removed.\n\n    Notes\n    -----\n    The current algorithm requires a O(N**2) memory allocations, and so may\n    not work for very large arrays (N>10000).\n\n    .. [*] M. Hubert and E. Vandervieren, \"An adjusted boxplot for skewed\n       distributions\" Computational Statistics & Data Analysis, vol. 52, pp.\n       5186-5201, August 2008.\n    \"\"\"\n    y = np.asarray(y, dtype=np.double)\n    if axis is None:\n        return _medcouple_1d(y.ravel())\n    return np.apply_along_axis(_medcouple_1d, axis, y)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "samplesize_confint_proportion", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/proportion.py", "method_code_mask": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\n\n\ndef samplesize_confint_proportion(proportion, half_length, alpha=0.05,\n    method='normal'): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\ndef samplesize_confint_proportion(proportion, half_length, alpha=0.05,\n    method='normal'):\n    \"\"\"\n    Find sample size to get desired confidence interval length\n\n    Parameters\n    ----------\n    proportion : float in (0, 1)\n        proportion or quantile\n    half_length : float in (0, 1)\n        desired half length of the confidence interval\n    alpha : float in (0, 1)\n        significance level, default 0.05,\n        coverage of the two-sided interval is (approximately) ``1 - alpha``\n    method : str in ['normal']\n        method to use for confidence interval,\n        currently only normal approximation\n\n    Returns\n    -------\n    n : float\n        sample size to get the desired half length of the confidence interval\n\n    Notes\n    -----\n    this is mainly to store the formula.\n    possible application: number of replications in bootstrap samples\n\n    \"\"\"\n    q_ = proportion\n    if method == 'normal':\n        n = q_ * (1 - q_) / (half_length / stats.norm.isf(alpha / 2.0)) ** 2\n    else:\n        raise NotImplementedError('only \"normal\" is available')\n    return n"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_confint_riskratio_koopman", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/proportion.py", "method_code_mask": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\n\n\ndef _confint_riskratio_koopman(count1, nobs1, count2, nobs2, alpha=0.05,\n    correction=True): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\ndef _confint_riskratio_koopman(count1, nobs1, count2, nobs2, alpha=0.05,\n    correction=True):\n    \"\"\"\n    Score confidence interval for ratio or proportions, Koopman/Nam\n\n    signature not consistent with other functions\n\n    When correction is True, then the small sample correction nobs / (nobs - 1)\n    by Miettinen/Nurminen is used.\n    \"\"\"\n    x0, x1, n0, n1 = count2, count1, nobs2, nobs1\n    x = x0 + x1\n    n = n0 + n1\n    z = stats.norm.isf(alpha / 2) ** 2\n    if correction:\n        z *= n / (n - 1)\n    a1 = n0 * (n0 * n * x1 + n1 * (n0 + x1) * z)\n    a2 = -n0 * (n0 * n1 * x + 2 * n * x0 * x1 + n1 * (n0 + x0 + 2 * x1) * z)\n    a3 = 2 * n0 * n1 * x0 * x + n * x0 * x0 * x1 + n0 * n1 * x * z\n    a4 = -n1 * x0 * x0 * x\n    p_roots_ = np.sort(np.roots([a1, a2, a3, a4]))\n    p_roots = p_roots_[:2][::-1]\n    ci = (1 - (n1 - x1) * (1 - p_roots) / (x0 + n1 - n * p_roots)) / p_roots\n    res = Holder()\n    res.confint = ci\n    res._p_roots = p_roots_\n    return res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "ztost", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/weightstats.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.tools.decorators import cache_readonly\nimport pandas as pd\nfrom statsmodels.iolib.summary import summary_params\n\n\ndef ztost(x1, low, upp, x2=None, usevar='pooled', ddof=1.0): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom statsmodels.tools.decorators import cache_readonly\nimport pandas as pd\nfrom statsmodels.iolib.summary import summary_params\ndef ztost(x1, low, upp, x2=None, usevar='pooled', ddof=1.0):\n    \"\"\"Equivalence test based on normal distribution\n\n    Parameters\n    ----------\n    x1 : array_like\n        one sample or first sample for 2 independent samples\n    low, upp : float\n        equivalence interval low < m1 - m2 < upp\n    x1 : array_like or None\n        second sample for 2 independent samples test. If None, then a\n        one-sample test is performed.\n    usevar : str, 'pooled'\n        If `pooled`, then the standard deviation of the samples is assumed to be\n        the same. Only `pooled` is currently implemented.\n\n    Returns\n    -------\n    pvalue : float\n        pvalue of the non-equivalence test\n    t1, pv1 : tuple of floats\n        test statistic and pvalue for lower threshold test\n    t2, pv2 : tuple of floats\n        test statistic and pvalue for upper threshold test\n\n    Notes\n    -----\n    checked only for 1 sample case\n\n    \"\"\"\n    tt1 = ztest(x1, x2, alternative='larger', usevar=usevar, value=low,\n        ddof=ddof)\n    tt2 = ztest(x1, x2, alternative='smaller', usevar=usevar, value=upp,\n        ddof=ddof)\n    return np.maximum(tt1[1], tt2[1]), tt1, tt2"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "power_equivalence_oneway", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/oneway.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\n\n\ndef power_equivalence_oneway(f2_alt, equiv_margin, nobs_t, n_groups=None,\n    df=None, alpha=0.05, margin_type='f2'): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\ndef power_equivalence_oneway(f2_alt, equiv_margin, nobs_t, n_groups=None,\n    df=None, alpha=0.05, margin_type='f2'):\n    \"\"\"\n    Power of  oneway equivalence test\n\n    Parameters\n    ----------\n    f2_alt : float\n        Effect size, squared Cohen's f, under the alternative.\n    equiv_margin : float\n        Equivalence margin in terms of effect size. Effect size can be chosen\n        with `margin_type`. default is squared Cohen's f.\n    nobs_t : ndarray\n        Total number of observations summed over all groups.\n    n_groups : int\n        Number of groups in oneway comparison. If margin_type is \"wellek\",\n        then either ``n_groups`` or ``df`` has to be given.\n    df : tuple\n        Degrees of freedom for F distribution,\n        ``df = (n_groups - 1, nobs_t - n_groups)``\n    alpha : float in (0, 1)\n        Significance level for the hypothesis test.\n    margin_type : \"f2\" or \"wellek\"\n        Type of effect size used for equivalence margin, either squared\n        Cohen's f or Wellek's psi. Default is \"f2\".\n\n    Returns\n    -------\n    pow_alt : float\n        Power of the equivalence test at given equivalence effect size under\n        the alternative.\n    \"\"\"\n    if df is None:\n        if n_groups is None:\n            raise ValueError('either df or n_groups has to be provided')\n        df = n_groups - 1, nobs_t - n_groups\n    if f2_alt == 0:\n        f2_alt = 1e-13\n    if margin_type in ['f2', 'fsqu', 'fsquared']:\n        f2_null = equiv_margin\n    elif margin_type == 'wellek':\n        if n_groups is None:\n            raise ValueError(\n                'If margin_type is wellek, then n_groups has to be provided')\n        nobs_mean = nobs_t / n_groups\n        f2_null = nobs_mean * equiv_margin ** 2 / nobs_t\n        f2_alt = nobs_mean * f2_alt ** 2 / nobs_t\n    else:\n        raise ValueError('`margin_type` should be \"f2\" or \"wellek\"')\n    crit_f_margin = ncf_ppf(alpha, df[0], df[1], nobs_t * f2_null)\n    pwr_alt = ncf_cdf(crit_f_margin, df[0], df[1], nobs_t * f2_alt)\n    return pwr_alt"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "binom_tost_reject_interval", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/proportion.py", "method_code_mask": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\n\n\ndef binom_tost_reject_interval(low, upp, nobs, alpha=0.05): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\ndef binom_tost_reject_interval(low, upp, nobs, alpha=0.05):\n    \"\"\"\n    Rejection region for binomial TOST\n\n    The interval includes the end points,\n    `reject` if and only if `r_low <= x <= r_upp`.\n\n    The interval might be empty with `r_upp < r_low`.\n\n    Parameters\n    ----------\n    low, upp : floats\n        lower and upper limit of equivalence region\n    nobs : int\n        the number of trials or observations.\n\n    Returns\n    -------\n    x_low, x_upp : float\n        lower and upper bound of rejection region\n\n    \"\"\"\n    x_low = stats.binom.isf(alpha, nobs, low) + 1\n    x_upp = stats.binom.ppf(alpha, nobs, upp) - 1\n    return x_low, x_upp"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "confint_effectsize_oneway", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/oneway.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\n\n\ndef confint_effectsize_oneway(f_stat, df, alpha=0.05, nobs=None): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\ndef confint_effectsize_oneway(f_stat, df, alpha=0.05, nobs=None):\n    \"\"\"\n    Confidence interval for effect size in oneway anova for F distribution\n\n    This does not yet handle non-negativity constraint on nc.\n    Currently only two-sided alternative is supported.\n\n    Parameters\n    ----------\n    f_stat : float\n    df : tuple\n        degrees of freedom ``df = (df1, df2)`` where\n\n        - df1 : numerator degrees of freedom, number of constraints\n        - df2 : denominator degrees of freedom, df_resid\n\n    alpha : float, default 0.05\n    nobs : int, default None\n\n    Returns\n    -------\n    Holder\n        Class with effect size and confidence attributes\n\n    Notes\n    -----\n    The confidence interval for the noncentrality parameter is obtained by\n    inverting the cdf of the noncentral F distribution. Confidence intervals\n    for other effect sizes are computed by endpoint transformation.\n\n\n    R package ``effectsize`` does not compute the confidence intervals in the\n    same way. Their confidence intervals can be replicated with\n\n    >>> ci_nc = confint_noncentrality(f_stat, df1, df2, alpha=0.1)\n    >>> ci_es = smo._fstat2effectsize(ci_nc / df1, df1, df2)\n\n    See Also\n    --------\n    confint_noncentrality\n    \"\"\"\n    df1, df2 = df\n    if nobs is None:\n        nobs = df1 + df2 + 1\n    ci_nc = confint_noncentrality(f_stat, df, alpha=alpha)\n    ci_f2 = ci_nc / nobs\n    ci_res = convert_effectsize_fsqu(f2=ci_f2)\n    ci_res.ci_omega2 = (ci_f2 - df1 / df2) / (ci_f2 + 1 + 1 / df2)\n    ci_res.ci_nc = ci_nc\n    ci_res.ci_f = np.sqrt(ci_res.f2)\n    ci_res.ci_eta = np.sqrt(ci_res.eta2)\n    ci_res.ci_f_corrected = np.sqrt(ci_res.f2 * (df1 + 1) / df1)\n    return ci_res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_confint_riskratio_paired_nam", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/proportion.py", "method_code_mask": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\n\n\ndef _confint_riskratio_paired_nam(table, alpha=0.05): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\ndef _confint_riskratio_paired_nam(table, alpha=0.05):\n    \"\"\"\n    Confidence interval for marginal risk ratio for matched pairs\n\n    need full table\n\n             success fail  marginal\n    success    x11    x10  x1.\n    fail       x01    x00  x0.\n    marginal   x.1    x.0   n\n\n    The confidence interval is for the ratio p1 / p0 where\n    p1 = x1. / n and\n    p0 - x.1 / n\n    Todo: rename p1 to pa and p2 to pb, so we have a, b for treatment and\n    0, 1 for success/failure\n\n    current namings follow Nam 2009\n\n    status\n    testing:\n    compared to example in Nam 2009\n    internal polynomial coefficients in calculation correspond at around\n        4 decimals\n    confidence interval agrees only at 2 decimals\n\n    \"\"\"\n    x11, x10, x01, x00 = np.ravel(table)\n    n = np.sum(table)\n    p10, p01 = x10 / n, x01 / n\n    p1 = (x11 + x10) / n\n    p0 = (x11 + x01) / n\n    q00 = 1 - x00 / n\n    z2 = stats.norm.isf(alpha / 2) ** 2\n    g1 = (n * p0 + z2 / 2) * p0\n    g2 = -(2 * n * p1 * p0 + z2 * q00)\n    g3 = (n * p1 + z2 / 2) * p1\n    a0 = g1 ** 2 - (z2 * p0 / 2) ** 2\n    a1 = 2 * g1 * g2\n    a2 = g2 ** 2 + 2 * g1 * g3 + z2 ** 2 * (p1 * p0 - 2 * p10 * p01) / 2\n    a3 = 2 * g2 * g3\n    a4 = g3 ** 2 - (z2 * p1 / 2) ** 2\n    p_roots = np.sort(np.roots([a0, a1, a2, a3, a4]))\n    ci = [p_roots.min(), p_roots.max()]\n    res = Holder()\n    res.confint = ci\n    res.p = p1, p0\n    res._p_roots = p_roots\n    return res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "confint_noncentrality", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/oneway.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\n\n\ndef confint_noncentrality(f_stat, df, alpha=0.05, alternative='two-sided'): [M\n    ASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\ndef confint_noncentrality(f_stat, df, alpha=0.05, alternative='two-sided'):\n    \"\"\"\n    Confidence interval for noncentrality parameter in F-test\n\n    This does not yet handle non-negativity constraint on nc.\n    Currently only two-sided alternative is supported.\n\n    Parameters\n    ----------\n    f_stat : float\n    df : tuple\n        degrees of freedom ``df = (df1, df2)`` where\n\n        - df1 : numerator degrees of freedom, number of constraints\n        - df2 : denominator degrees of freedom, df_resid\n\n    alpha : float, default 0.05\n    alternative : {\"two-sided\"}\n        Other alternatives have not been implements.\n\n    Returns\n    -------\n    float\n        The end point of the confidence interval.\n\n    Notes\n    -----\n    The algorithm inverts the cdf of the noncentral F distribution with\n    respect to the noncentrality parameters.\n    See Steiger 2004 and references cited in it.\n\n    References\n    ----------\n    .. [1] Steiger, James H. 2004. \u201cBeyond the F Test: Effect Size Confidence\n       Intervals and Tests of Close Fit in the Analysis of Variance and\n       Contrast Analysis.\u201d Psychological Methods 9 (2): 164\u201382.\n       https://doi.org/10.1037/1082-989X.9.2.164.\n\n    See Also\n    --------\n    confint_effectsize_oneway\n    \"\"\"\n    df1, df2 = df\n    if alternative in ['two-sided', '2s', 'ts']:\n        alpha1s = alpha / 2\n        ci = ncfdtrinc(df1, df2, [1 - alpha1s, alpha1s], f_stat)\n    else:\n        raise NotImplementedError\n    return ci"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_noncentrality_f", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/effect_size.py", "method_code_mask": "import numpy as np\nfrom scipy import special\nfrom statsmodels.stats.base import Holder\n\n\ndef _noncentrality_f(f_stat, df1, df2, alpha=0.05): [MASK]\n", "response": "import numpy as np\nfrom scipy import special\nfrom statsmodels.stats.base import Holder\ndef _noncentrality_f(f_stat, df1, df2, alpha=0.05):\n    \"\"\"noncentrality parameter for f statistic\n\n    `nc` is zero-truncated umvue\n\n    Parameters\n    ----------\n    fstat : float\n        f-statistic, for example from a hypothesis test\n        df : int or float\n        Degrees of freedom\n    alpha : float in (0, 1)\n        Significance level for the confidence interval, covarage is 1 - alpha.\n\n    Returns\n    -------\n    HolderTuple\n        The main attributes are\n\n        - ``nc`` : estimate of noncentrality parameter\n        - ``confint`` : lower and upper bound of confidence interval for `nc``\n\n        Other attributes are estimates for nc by different methods.\n\n    References\n    ----------\n    .. [1] Kubokawa, T., C.P. Robert, and A.K.Md.E. Saleh. 1993. \u201cEstimation of\n       Noncentrality Parameters.\u201d Canadian Journal of Statistics 21 (1): 45\u201357.\n       https://doi.org/10.2307/3315657.\n    \"\"\"\n    alpha_half = alpha / 2\n    x_s = f_stat * df1 / df2\n    nc_umvue = (df2 - 2) * x_s - df1\n    nc = np.maximum(nc_umvue, 0)\n    nc_krs = np.maximum(nc_umvue, x_s * 2 * (df2 - 1) / (df1 + 2))\n    nc_median = special.ncfdtrinc(df1, df2, 0.5, f_stat)\n    ci = special.ncfdtrinc(df1, df2, [1 - alpha_half, alpha_half], f_stat)\n    res = Holder(nc=nc, confint=ci, nc_umvue=nc_umvue, nc_krs=nc_krs,\n        nc_median=nc_median, name=\n        'Noncentrality for F-distributed random variable')\n    return res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "power_proportions_2indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/proportion.py", "method_code_mask": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\n\n\ndef power_proportions_2indep(diff, prop2, nobs1, ratio=1, alpha=0.05, value\n    =0, alternative='two-sided', return_results=True): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\ndef power_proportions_2indep(diff, prop2, nobs1, ratio=1, alpha=0.05, value\n    =0, alternative='two-sided', return_results=True):\n    \"\"\"\n    Power for ztest that two independent proportions are equal\n\n    This assumes that the variance is based on the pooled proportion\n    under the null and the non-pooled variance under the alternative\n\n    Parameters\n    ----------\n    diff : float\n        difference between proportion 1 and 2 under the alternative\n    prop2 : float\n        proportion for the reference case, prop2, proportions for the\n        first case will be computed using p2 and diff\n        p1 = p2 + diff\n    nobs1 : float or int\n        number of observations in sample 1\n    ratio : float\n        sample size ratio, nobs2 = ratio * nobs1\n    alpha : float in interval (0,1)\n        Significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    value : float\n        currently only `value=0`, i.e. equality testing, is supported\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        Alternative hypothesis whether the power is calculated for a\n        two-sided (default) or one sided test. The one-sided test can be\n        either 'larger', 'smaller'.\n    return_results : bool\n        If true, then a results instance with extra information is returned,\n        otherwise only the computed power is returned.\n\n    Returns\n    -------\n    results : results instance or float\n        If return_results is True, then a results instance with the\n        information in attributes is returned.\n        If return_results is False, then only the power is returned.\n\n        power : float\n            Power of the test, e.g. 0.8, is one minus the probability of a\n            type II error. Power is the probability that the test correctly\n            rejects the Null Hypothesis if the Alternative Hypothesis is true.\n\n        Other attributes in results instance include :\n\n        p_pooled\n            pooled proportion, used for std_null\n        std_null\n            standard error of difference under the null hypothesis (without\n            sqrt(nobs1))\n        std_alt\n            standard error of difference under the alternative hypothesis\n            (without sqrt(nobs1))\n    \"\"\"\n    from statsmodels.stats.power import normal_power_het\n    p_pooled, std_null, std_alt = _std_2prop_power(diff, prop2, ratio=ratio,\n        alpha=alpha, value=value)\n    pow_ = normal_power_het(diff, nobs1, alpha, std_null=std_null,\n        std_alternative=std_alt, alternative=alternative)\n    if return_results:\n        res = Holder(power=pow_, p_pooled=p_pooled, std_null=std_null,\n            std_alt=std_alt, nobs1=nobs1, nobs2=ratio * nobs1, nobs_ratio=\n            ratio, alpha=alpha)\n        return res\n    else:\n        return pow_"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_fstat2effectsize", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/oneway.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\n\n\ndef _fstat2effectsize(f_stat, df): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.special import ncfdtrinc\nfrom statsmodels.stats.power import ncf_cdf\nfrom statsmodels.stats.power import ncf_ppf\nfrom statsmodels.stats.robust_compare import TrimmedMean\nfrom statsmodels.stats.robust_compare import scale_transform\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.stats.base import HolderTuple\ndef _fstat2effectsize(f_stat, df):\n    \"\"\"Compute anova effect size from F-statistic\n\n    This might be combined with convert_effectsize_fsqu\n\n    Parameters\n    ----------\n    f_stat : array_like\n        Test statistic of an F-test\n    df : tuple\n        degrees of freedom ``df = (df1, df2)`` where\n         - df1 : numerator degrees of freedom, number of constraints\n         - df2 : denominator degrees of freedom, df_resid\n\n    Returns\n    -------\n    res : Holder instance\n        This instance contains effect size measures f2, eta2, omega2 and eps2\n        as attributes.\n\n    Notes\n    -----\n    This uses the following definitions:\n\n    - f2 = f_stat * df1 / df2\n    - eta2 = f2 / (f2 + 1)\n    - omega2 = (f2 - df1 / df2) / (f2 + 2)\n    - eps2 = (f2 - df1 / df2) / (f2 + 1)\n\n    This differs from effect size measures in other function which define\n    ``f2 = f_stat * df1 / nobs``\n    or an equivalent expression for power computation. The noncentrality\n    index for the hypothesis test is in those cases given by\n    ``nc = f_stat * df1``.\n\n    Currently omega2 and eps2 are computed in two different ways. Those\n    values agree for regular cases but can show different behavior in corner\n    cases (e.g. zero division).\n\n    \"\"\"\n    df1, df2 = df\n    f2 = f_stat * df1 / df2\n    eta2 = f2 / (f2 + 1)\n    omega2_ = (f_stat - 1) / (f_stat + (df2 + 1) / df1)\n    omega2 = (f2 - df1 / df2) / (f2 + 1 + 1 / df2)\n    eps2_ = (f_stat - 1) / (f_stat + df2 / df1)\n    eps2 = (f2 - df1 / df2) / (f2 + 1)\n    return Holder(f2=f2, eta2=eta2, omega2=omega2, eps2=eps2, eps2_=eps2_,\n        omega2_=omega2_)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_right_squeeze", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/validation/validation.py", "method_code_mask": "from typing import Any\nfrom typing import Optional\nfrom collections.abc import Mapping\nimport numpy as np\nimport pandas as pd\n\n\ndef _right_squeeze(arr, stop_dim=0): [MASK]\n", "response": "from typing import Any\nfrom typing import Optional\nfrom collections.abc import Mapping\nimport numpy as np\nimport pandas as pd\ndef _right_squeeze(arr, stop_dim=0):\n    \"\"\"\n    Remove trailing singleton dimensions\n\n    Parameters\n    ----------\n    arr : ndarray\n        Input array\n    stop_dim : int\n        Dimension where checking should stop so that shape[i] is not checked\n        for i < stop_dim\n\n    Returns\n    -------\n    squeezed : ndarray\n        Array with all trailing singleton dimensions (0 or 1) removed.\n        Singleton dimensions for dimension < stop_dim are retained.\n    \"\"\"\n    last = arr.ndim\n    for s in reversed(arr.shape):\n        if s > 1:\n            break\n        last -= 1\n    last = max(last, stop_dim)\n    return arr.reshape(arr.shape[:last])"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_noncentrality_chisquare", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/effect_size.py", "method_code_mask": "import numpy as np\nfrom scipy import special\nfrom statsmodels.stats.base import Holder\n\n\ndef _noncentrality_chisquare(chi2_stat, df, alpha=0.05): [MASK]\n", "response": "import numpy as np\nfrom scipy import special\nfrom statsmodels.stats.base import Holder\ndef _noncentrality_chisquare(chi2_stat, df, alpha=0.05):\n    \"\"\"noncentrality parameter for chi-square statistic\n\n    `nc` is zero-truncated umvue\n\n    Parameters\n    ----------\n    chi2_stat : float\n        Chisquare-statistic, for example from a hypothesis test\n    df : int or float\n        Degrees of freedom\n    alpha : float in (0, 1)\n        Significance level for the confidence interval, covarage is 1 - alpha.\n\n    Returns\n    -------\n    HolderTuple\n        The main attributes are\n\n        - ``nc`` : estimate of noncentrality parameter\n        - ``confint`` : lower and upper bound of confidence interval for `nc``\n\n        Other attributes are estimates for nc by different methods.\n\n    References\n    ----------\n    .. [1] Kubokawa, T., C.P. Robert, and A.K.Md.E. Saleh. 1993. \u201cEstimation of\n        Noncentrality Parameters.\u201d\n        Canadian Journal of Statistics 21 (1): 45\u201357.\n        https://doi.org/10.2307/3315657.\n\n    .. [2] Li, Qizhai, Junjian Zhang, and Shuai Dai. 2009. \u201cOn Estimating the\n        Non-Centrality Parameter of a Chi-Squared Distribution.\u201d\n        Statistics & Probability Letters 79 (1): 98\u2013104.\n        https://doi.org/10.1016/j.spl.2008.07.025.\n\n    \"\"\"\n    alpha_half = alpha / 2\n    nc_umvue = chi2_stat - df\n    nc = np.maximum(nc_umvue, 0)\n    nc_lzd = np.maximum(nc_umvue, chi2_stat / (df + 1))\n    nc_krs = np.maximum(nc_umvue, chi2_stat * 2 / (df + 2))\n    nc_median = special.chndtrinc(chi2_stat, df, 0.5)\n    ci = special.chndtrinc(chi2_stat, df, [1 - alpha_half, alpha_half])\n    res = Holder(nc=nc, confint=ci, nc_umvue=nc_umvue, nc_lzd=nc_lzd,\n        nc_krs=nc_krs, nc_median=nc_median, name=\n        'Noncentrality for chisquare-distributed random variable')\n    return res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "samplesize_proportions_2indep_onetail", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/proportion.py", "method_code_mask": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\n\n\ndef samplesize_proportions_2indep_onetail(diff, prop2, power, ratio=1,\n    alpha=0.05, value=0, alternative='two-sided'): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nfrom typing import Callable\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import stats\nfrom statsmodels.stats.base import AllPairsResults\nfrom statsmodels.stats.base import HolderTuple\nfrom statsmodels.stats.weightstats import _zstat_generic2\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.testing import Holder\nfrom statsmodels.tools.validation import array_like\nimport warnings\nfrom statsmodels.stats.power import normal_power_het\nfrom statsmodels.stats.power import normal_sample_size_one_tail\ndef samplesize_proportions_2indep_onetail(diff, prop2, power, ratio=1,\n    alpha=0.05, value=0, alternative='two-sided'):\n    \"\"\"\n    Required sample size assuming normal distribution based on one tail\n\n    This uses an explicit computation for the sample size that is required\n    to achieve a given power corresponding to the appropriate tails of the\n    normal distribution. This ignores the far tail in a two-sided test\n    which is negligible in the common case when alternative and null are\n    far apart.\n\n    Parameters\n    ----------\n    diff : float\n        Difference between proportion 1 and 2 under the alternative\n    prop2 : float\n        proportion for the reference case, prop2, proportions for the\n        first case will be computing using p2 and diff\n        p1 = p2 + diff\n    power : float\n        Power for which sample size is computed.\n    ratio : float\n        Sample size ratio, nobs2 = ratio * nobs1\n    alpha : float in interval (0,1)\n        Significance level, e.g. 0.05, is the probability of a type I\n        error, that is wrong rejections if the Null Hypothesis is true.\n    value : float\n        Currently only `value=0`, i.e. equality testing, is supported\n    alternative : string, 'two-sided' (default), 'larger', 'smaller'\n        Alternative hypothesis whether the power is calculated for a\n        two-sided (default) or one sided test. In the case of a one-sided\n        alternative, it is assumed that the test is in the appropriate tail.\n\n    Returns\n    -------\n    nobs1 : float\n        Number of observations in sample 1.\n    \"\"\"\n    from statsmodels.stats.power import normal_sample_size_one_tail\n    if alternative in ['two-sided', '2s']:\n        alpha = alpha / 2\n    _, std_null, std_alt = _std_2prop_power(diff, prop2, ratio=ratio, alpha\n        =alpha, value=value)\n    nobs = normal_sample_size_one_tail(diff, power, alpha, std_null=\n        std_null, std_alternative=std_alt)\n    return nobs"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "medianbias", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef medianbias(x1, x2, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef medianbias(x1, x2, axis=0):\n    \"\"\"median bias, median error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    medianbias : ndarray or float\n       median bias, or median difference along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.median(x1 - x2, axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "mse", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef mse(x1, x2, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef mse(x1, x2, axis=0):\n    \"\"\"mean squared error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    mse : ndarray or float\n       mean squared error along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass, for example\n    numpy matrices will silently produce an incorrect result.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.mean((x1 - x2) ** 2, axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_noncentrality_t", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/effect_size.py", "method_code_mask": "import numpy as np\nfrom scipy import special\nfrom statsmodels.stats.base import Holder\n\n\ndef _noncentrality_t(t_stat, df, alpha=0.05): [MASK]\n", "response": "import numpy as np\nfrom scipy import special\nfrom statsmodels.stats.base import Holder\ndef _noncentrality_t(t_stat, df, alpha=0.05):\n    \"\"\"noncentrality parameter for t statistic\n\n    Parameters\n    ----------\n    fstat : float\n        f-statistic, for example from a hypothesis test\n        df : int or float\n        Degrees of freedom\n    alpha : float in (0, 1)\n        Significance level for the confidence interval, covarage is 1 - alpha.\n\n    Returns\n    -------\n    HolderTuple\n        The main attributes are\n\n        - ``nc`` : estimate of noncentrality parameter\n        - ``confint`` : lower and upper bound of confidence interval for `nc``\n\n        Other attributes are estimates for nc by different methods.\n\n    References\n    ----------\n    .. [1] Hedges, Larry V. 2016. \u201cDistribution Theory for Glass\u2019s Estimator of\n       Effect Size and Related Estimators:\u201d\n       Journal of Educational Statistics, November.\n       https://doi.org/10.3102/10769986006002107.\n\n    \"\"\"\n    alpha_half = alpha / 2\n    gfac = np.exp(special.gammaln(df / 2.0 - 0.5) - special.gammaln(df / 2.0))\n    c11 = np.sqrt(df / 2.0) * gfac\n    nc = t_stat / c11\n    nc_median = special.nctdtrinc(df, 0.5, t_stat)\n    ci = special.nctdtrinc(df, [1 - alpha_half, alpha_half], t_stat)\n    res = Holder(nc=nc, confint=ci, nc_median=nc_median, name=\n        'Noncentrality for t-distributed random variable')\n    return res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "rmse", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef rmse(x1, x2, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef rmse(x1, x2, axis=0):\n    \"\"\"root mean squared error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    rmse : ndarray or float\n       root mean squared error along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass, for example\n    numpy matrices will silently produce an incorrect result.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.sqrt(mse(x1, x2, axis=axis))"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "bias", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef bias(x1, x2, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef bias(x1, x2, axis=0):\n    \"\"\"bias, mean error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    bias : ndarray or float\n       bias, or mean difference along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.mean(x1 - x2, axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "maxabs", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef maxabs(x1, x2, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef maxabs(x1, x2, axis=0):\n    \"\"\"maximum absolute error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    maxabs : ndarray or float\n       maximum absolute difference along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.max(np.abs(x1 - x2), axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "vare", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef vare(x1, x2, ddof=0, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef vare(x1, x2, ddof=0, axis=0):\n    \"\"\"variance of error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    vare : ndarray or float\n       variance of difference along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.var(x1 - x2, ddof=ddof, axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "get_duncan_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/tests/test_diagnostic.py", "method_code_mask": "import json\nimport os\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_array_equal\nfrom numpy.testing import assert_equal\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\nimport pytest\nfrom statsmodels.datasets import macrodata\nfrom statsmodels.datasets import sunspots\nfrom statsmodels.regression.linear_model import OLS\nimport statsmodels.stats.diagnostic as smsdia\nimport statsmodels.stats.outliers_influence as oi\nimport statsmodels.stats.sandwich_covariance as sw\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pandas import DataFrame\n\n\ndef get_duncan_data(): [MASK]\n", "response": "import json\nimport os\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_array_equal\nfrom numpy.testing import assert_equal\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\nimport pytest\nfrom statsmodels.datasets import macrodata\nfrom statsmodels.datasets import sunspots\nfrom statsmodels.regression.linear_model import OLS\nimport statsmodels.stats.diagnostic as smsdia\nimport statsmodels.stats.outliers_influence as oi\nimport statsmodels.stats.sandwich_covariance as sw\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pandas import DataFrame\ndef get_duncan_data():\n    labels = ['accountant', 'pilot', 'architect', 'author', 'chemist',\n        'minister', 'professor', 'dentist', 'reporter', 'engineer',\n        'undertaker', 'lawyer', 'physician', 'welfare.worker', 'teacher',\n        'conductor', 'contractor', 'factory.owner', 'store.manager',\n        'banker', 'bookkeeper', 'mail.carrier', 'insurance.agent',\n        'store.clerk', 'carpenter', 'electrician', 'RR.engineer',\n        'machinist', 'auto.repairman', 'plumber', 'gas.stn.attendant',\n        'coal.miner', 'streetcar.motorman', 'taxi.driver', 'truck.driver',\n        'machine.operator', 'barber', 'bartender', 'shoe.shiner', 'cook',\n        'soda.clerk', 'watchman', 'janitor', 'policeman', 'waiter']\n    exog = [[1.0, 62.0, 86.0], [1.0, 72.0, 76.0], [1.0, 75.0, 92.0], [1.0, \n        55.0, 90.0], [1.0, 64.0, 86.0], [1.0, 21.0, 84.0], [1.0, 64.0, 93.0\n        ], [1.0, 80.0, 100.0], [1.0, 67.0, 87.0], [1.0, 72.0, 86.0], [1.0, \n        42.0, 74.0], [1.0, 76.0, 98.0], [1.0, 76.0, 97.0], [1.0, 41.0, 84.0\n        ], [1.0, 48.0, 91.0], [1.0, 76.0, 34.0], [1.0, 53.0, 45.0], [1.0, \n        60.0, 56.0], [1.0, 42.0, 44.0], [1.0, 78.0, 82.0], [1.0, 29.0, 72.0\n        ], [1.0, 48.0, 55.0], [1.0, 55.0, 71.0], [1.0, 29.0, 50.0], [1.0, \n        21.0, 23.0], [1.0, 47.0, 39.0], [1.0, 81.0, 28.0], [1.0, 36.0, 32.0\n        ], [1.0, 22.0, 22.0], [1.0, 44.0, 25.0], [1.0, 15.0, 29.0], [1.0, \n        7.0, 7.0], [1.0, 42.0, 26.0], [1.0, 9.0, 19.0], [1.0, 21.0, 15.0],\n        [1.0, 21.0, 20.0], [1.0, 16.0, 26.0], [1.0, 16.0, 28.0], [1.0, 9.0,\n        17.0], [1.0, 14.0, 22.0], [1.0, 12.0, 30.0], [1.0, 17.0, 25.0], [\n        1.0, 7.0, 20.0], [1.0, 34.0, 47.0], [1.0, 8.0, 32.0]]\n    endog = [82.0, 83.0, 90.0, 76.0, 90.0, 87.0, 93.0, 90.0, 52.0, 88.0, \n        57.0, 89.0, 97.0, 59.0, 73.0, 38.0, 76.0, 81.0, 45.0, 92.0, 39.0, \n        34.0, 41.0, 16.0, 33.0, 53.0, 67.0, 57.0, 26.0, 29.0, 10.0, 15.0, \n        19.0, 10.0, 13.0, 24.0, 20.0, 7.0, 3.0, 16.0, 6.0, 11.0, 8.0, 41.0,\n        10.0]\n    return endog, exog, labels"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "rmspe", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef rmspe(y, y_hat, axis=0, zeros=np.nan): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef rmspe(y, y_hat, axis=0, zeros=np.nan):\n    \"\"\"\n    Root Mean Squared Percentage Error\n\n    Parameters\n    ----------\n    y : array_like\n      The actual value.\n    y_hat : array_like\n       The predicted value.\n    axis : int\n       Axis along which the summary statistic is calculated\n    zeros : float\n       Value to assign to error where y is zero\n\n    Returns\n    -------\n    rmspe : ndarray or float\n       Root Mean Squared Percentage Error along given axis.\n    \"\"\"\n    y_hat = np.asarray(y_hat)\n    y = np.asarray(y)\n    error = y - y_hat\n    loc = y != 0\n    loc = loc.ravel()\n    percentage_error = np.full_like(error, zeros)\n    percentage_error.flat[loc] = error.flat[loc] / y.flat[loc]\n    mspe = np.nanmean(percentage_error ** 2, axis=axis) * 100\n    return np.sqrt(mspe)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "meanabs", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef meanabs(x1, x2, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef meanabs(x1, x2, axis=0):\n    \"\"\"mean absolute error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    meanabs : ndarray or float\n       mean absolute difference along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.mean(np.abs(x1 - x2), axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "string_like", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/validation/validation.py", "method_code_mask": "from typing import Any\nfrom typing import Optional\nfrom collections.abc import Mapping\nimport numpy as np\nimport pandas as pd\n\n\ndef string_like(value, name, optional=False, options=None, lower=True): [MASK]\n", "response": "from typing import Any\nfrom typing import Optional\nfrom collections.abc import Mapping\nimport numpy as np\nimport pandas as pd\ndef string_like(value, name, optional=False, options=None, lower=True):\n    \"\"\"\n    Check if object is string-like and raise if not\n\n    Parameters\n    ----------\n    value : object\n        Value to verify.\n    name : str\n        Variable name for exceptions.\n    optional : bool\n        Flag indicating whether None is allowed.\n    options : tuple[str]\n        Allowed values for input parameter `value`.\n    lower : bool\n        Convert all case-based characters in `value` into lowercase.\n\n    Returns\n    -------\n    str\n        The validated input\n\n    Raises\n    ------\n    TypeError\n        If the value is not a string or None when optional is True.\n    ValueError\n        If the input is not in ``options`` when ``options`` is set.\n    \"\"\"\n    if value is None:\n        return None\n    if not isinstance(value, str):\n        extra_text = ' or None' if optional else ''\n        raise TypeError(f'{name} must be a string{extra_text}')\n    if lower:\n        value = value.lower()\n    if options is not None and value not in options:\n        extra_text = 'If not None, ' if optional else ''\n        options_text = \"'\" + \"', '\".join(options) + \"'\"\n        msg = '{}{} must be one of: {}'.format(extra_text, name, options_text)\n        raise ValueError(msg)\n    return value"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "medianabs", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/eval_measures.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import array_like\n\n\ndef medianabs(x1, x2, axis=0): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import array_like\ndef medianabs(x1, x2, axis=0):\n    \"\"\"median absolute error\n\n    Parameters\n    ----------\n    x1, x2 : array_like\n       The performance measure depends on the difference between these two\n       arrays.\n    axis : int\n       axis along which the summary statistic is calculated\n\n    Returns\n    -------\n    medianabs : ndarray or float\n       median absolute difference along given axis.\n\n    Notes\n    -----\n    If ``x1`` and ``x2`` have different shapes, then they need to broadcast.\n    This uses ``numpy.asanyarray`` to convert the input. Whether this is the\n    desired result or not depends on the array subclass.\n    \"\"\"\n    x1 = np.asanyarray(x1)\n    x2 = np.asanyarray(x2)\n    return np.median(np.abs(x1 - x2), axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "combine_indices", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/grouputils.py", "method_code_mask": "from statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport numpy as np\nimport pandas as pd\nimport statsmodels.tools.data as data_util\nfrom pandas import Index\nfrom pandas import MultiIndex\nfrom scipy import sparse\n\n\ndef combine_indices(groups, prefix='', sep='.', return_labels=False): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport numpy as np\nimport pandas as pd\nimport statsmodels.tools.data as data_util\nfrom pandas import Index\nfrom pandas import MultiIndex\nfrom scipy import sparse\ndef combine_indices(groups, prefix='', sep='.', return_labels=False):\n    \"\"\"use np.unique to get integer group indices for product, intersection\n    \"\"\"\n    if isinstance(groups, tuple):\n        groups = np.column_stack(groups)\n    else:\n        groups = np.asarray(groups)\n    dt = groups.dtype\n    is2d = groups.ndim == 2\n    if is2d:\n        ncols = groups.shape[1]\n        if not groups.flags.c_contiguous:\n            groups = np.array(groups, order='C')\n        groups_ = groups.view([('', groups.dtype)] * groups.shape[1])\n    else:\n        groups_ = groups\n    uni, uni_idx, uni_inv = np.unique(groups_, return_index=True,\n        return_inverse=True)\n    if is2d:\n        uni = uni.view(dt).reshape(-1, ncols)\n    if return_labels:\n        label = [((prefix + sep.join(['%s'] * len(uni[0]))) % tuple(ii)) for\n            ii in uni]\n        return uni_inv, uni_idx, uni, label\n    else:\n        return uni_inv, uni_idx, uni"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_approx_fprime_cs_scalar", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/numdiff.py", "method_code_mask": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\n\n\ndef _approx_fprime_cs_scalar(x, f, epsilon=None, args=(), kwargs={}): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\ndef _approx_fprime_cs_scalar(x, f, epsilon=None, args=(), kwargs={}):\n    \"\"\"\n    Calculate gradient for scalar parameter with complex step derivatives.\n\n    This assumes that the function ``f`` is vectorized for a scalar parameter.\n    The function value ``f(x)`` has then the same shape as the input ``x``.\n    The derivative returned by this function also has the same shape as ``x``.\n\n    Parameters\n    ----------\n    x : ndarray\n        Parameters at which the derivative is evaluated.\n    f : function\n        `f(*((x,)+args), **kwargs)` returning either one value or 1d array.\n    epsilon : float, optional\n        Stepsize, if None, optimal stepsize is used. Optimal step-size is\n        EPS*x. See note.\n    args : tuple\n        Tuple of additional arguments for function `f`.\n    kwargs : dict\n        Dictionary of additional keyword arguments for function `f`.\n\n    Returns\n    -------\n    partials : ndarray\n       Array of derivatives, gradient evaluated for parameters ``x``.\n\n    Notes\n    -----\n    The complex-step derivative has truncation error O(epsilon**2), so\n    truncation error can be eliminated by choosing epsilon to be very small.\n    The complex-step derivative avoids the problem of round-off error with\n    small epsilon because there is no subtraction.\n    \"\"\"\n    x = np.asarray(x)\n    n = x.shape[-1]\n    epsilon = _get_epsilon(x, 1, epsilon, n)\n    eps = 1.0j * epsilon\n    partials = f(x + eps, *args, **kwargs).imag / epsilon\n    return np.array(partials)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "parallel_func", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/parallel.py", "method_code_mask": "from statsmodels.tools.sm_exceptions import ModuleUnavailableWarning\nfrom statsmodels.tools.sm_exceptions import module_unavailable_doc\nfrom joblib import Parallel\nfrom joblib import delayed\nimport multiprocessing\nimport warnings\n\n\ndef parallel_func(func, n_jobs, verbose=5): [MASK]\n", "response": "from statsmodels.tools.sm_exceptions import ModuleUnavailableWarning\nfrom statsmodels.tools.sm_exceptions import module_unavailable_doc\nfrom joblib import Parallel\nfrom joblib import delayed\nimport multiprocessing\nimport warnings\ndef parallel_func(func, n_jobs, verbose=5):\n    \"\"\"Return parallel instance with delayed function\n\n    Util function to use joblib only if available\n\n    Parameters\n    ----------\n    func : callable\n        A function\n    n_jobs : int\n        Number of jobs to run in parallel\n    verbose : int\n        Verbosity level\n\n    Returns\n    -------\n    parallel : instance of joblib.Parallel or list\n        The parallel object\n    my_func : callable\n        func if not parallel or delayed(func)\n    n_jobs : int\n        Number of jobs >= 0\n\n    Examples\n    --------\n    >>> from math import sqrt\n    >>> from statsmodels.tools.parallel import parallel_func\n    >>> parallel, p_func, n_jobs = parallel_func(sqrt, n_jobs=-1, verbose=0)\n    >>> print(n_jobs)\n    >>> parallel(p_func(i**2) for i in range(10))\n    \"\"\"\n    try:\n        try:\n            from joblib import Parallel, delayed\n        except ImportError:\n            from sklearn.externals.joblib import Parallel, delayed\n        parallel = Parallel(n_jobs, verbose=verbose)\n        my_func = delayed(func)\n        if n_jobs == -1:\n            try:\n                import multiprocessing\n                n_jobs = multiprocessing.cpu_count()\n            except (ImportError, NotImplementedError):\n                import warnings\n                warnings.warn(module_unavailable_doc.format(\n                    'multiprocessing'), ModuleUnavailableWarning)\n                n_jobs = 1\n    except ImportError:\n        import warnings\n        warnings.warn(module_unavailable_doc.format('joblib'),\n            ModuleUnavailableWarning)\n        n_jobs = 1\n        my_func = func\n        parallel = list\n    return parallel, my_func, n_jobs"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "dummy_sparse", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/grouputils.py", "method_code_mask": "from statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport numpy as np\nimport pandas as pd\nimport statsmodels.tools.data as data_util\nfrom pandas import Index\nfrom pandas import MultiIndex\nfrom scipy import sparse\n\n\ndef dummy_sparse(groups): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport numpy as np\nimport pandas as pd\nimport statsmodels.tools.data as data_util\nfrom pandas import Index\nfrom pandas import MultiIndex\nfrom scipy import sparse\ndef dummy_sparse(groups):\n    \"\"\"create a sparse indicator from a group array with integer labels\n\n    Parameters\n    ----------\n    groups : ndarray, int, 1d (nobs,)\n        an array of group indicators for each observation. Group levels are\n        assumed to be defined as consecutive integers, i.e. range(n_groups)\n        where n_groups is the number of group levels. A group level with no\n        observations for it will still produce a column of zeros.\n\n    Returns\n    -------\n    indi : ndarray, int8, 2d (nobs, n_groups)\n        an indicator array with one row per observation, that has 1 in the\n        column of the group level for that observation\n\n    Examples\n    --------\n\n    >>> g = np.array([0, 0, 2, 1, 1, 2, 0])\n    >>> indi = dummy_sparse(g)\n    >>> indi\n    <7x3 sparse matrix of type '<type 'numpy.int8'>'\n        with 7 stored elements in Compressed Sparse Row format>\n    >>> indi.todense()\n    matrix([[1, 0, 0],\n            [1, 0, 0],\n            [0, 0, 1],\n            [0, 1, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1, 0, 0]], dtype=int8)\n\n\n    current behavior with missing groups\n    >>> g = np.array([0, 0, 2, 0, 2, 0])\n    >>> indi = dummy_sparse(g)\n    >>> indi.todense()\n    matrix([[1, 0, 0],\n            [1, 0, 0],\n            [0, 0, 1],\n            [1, 0, 0],\n            [0, 0, 1],\n            [1, 0, 0]], dtype=int8)\n    \"\"\"\n    from scipy import sparse\n    indptr = np.arange(len(groups) + 1)\n    data = np.ones(len(groups), dtype=np.int8)\n    indi = sparse.csr_matrix((data, groups, indptr))\n    return indi"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "approx_fprime", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/numdiff.py", "method_code_mask": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\n\n\ndef approx_fprime(x, f, epsilon=None, args=(), kwargs={}, centered=False): [M\n    ASK]\n", "response": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\ndef approx_fprime(x, f, epsilon=None, args=(), kwargs={}, centered=False):\n    \"\"\"\n    Gradient of function, or Jacobian if function f returns 1d array\n\n    Parameters\n    ----------\n    x : ndarray\n        parameters at which the derivative is evaluated\n    f : function\n        `f(*((x,)+args), **kwargs)` returning either one value or 1d array\n    epsilon : float, optional\n        Stepsize, if None, optimal stepsize is used. This is EPS**(1/2)*x for\n        `centered` == False and EPS**(1/3)*x for `centered` == True.\n    args : tuple\n        Tuple of additional arguments for function `f`.\n    kwargs : dict\n        Dictionary of additional keyword arguments for function `f`.\n    centered : bool\n        Whether central difference should be returned. If not, does forward\n        differencing.\n\n    Returns\n    -------\n    grad : ndarray\n        gradient or Jacobian\n\n    Notes\n    -----\n    If f returns a 1d array, it returns a Jacobian. If a 2d array is returned\n    by f (e.g., with a value for each observation), it returns a 3d array\n    with the Jacobian of each observation with shape xk x nobs x xk. I.e.,\n    the Jacobian of the first observation would be [:, 0, :]\n    \"\"\"\n    n = len(x)\n    f0 = f(*((x,) + args), **kwargs)\n    dim = np.atleast_1d(f0).shape\n    grad = np.zeros((n,) + dim, np.promote_types(float, x.dtype))\n    ei = np.zeros((n,), float)\n    if not centered:\n        epsilon = _get_epsilon(x, 2, epsilon, n)\n        for k in range(n):\n            ei[k] = epsilon[k]\n            grad[k, :] = (f(*((x + ei,) + args), **kwargs) - f0) / epsilon[k]\n            ei[k] = 0.0\n    else:\n        epsilon = _get_epsilon(x, 3, epsilon, n) / 2.0\n        for k in range(n):\n            ei[k] = epsilon[k]\n            grad[k, :] = (f(*((x + ei,) + args), **kwargs) - f(*((x - ei,) +\n                args), **kwargs)) / (2 * epsilon[k])\n            ei[k] = 0.0\n    if n == 1:\n        return grad.T\n    else:\n        return grad.squeeze().T"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "recipr0", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/tools.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\n\n\ndef recipr0(x): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\ndef recipr0(x):\n    \"\"\"\n    Reciprocal of an array with entries less than 0 set to 0.\n\n    Parameters\n    ----------\n    x : array_like\n        The input array.\n\n    Returns\n    -------\n    ndarray\n        The array with 0-filled reciprocals.\n    \"\"\"\n    x = np.asarray(x)\n    out = np.zeros_like(x, dtype=np.float64)\n    nans = np.isnan(x.flat)\n    non_zero = ~nans\n    non_zero[non_zero] = non_zero[non_zero] & (x.flat[non_zero] != 0)\n    out.flat[non_zero] = 1.0 / x.flat[non_zero]\n    out.flat[nans] = np.nan\n    return out"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "recipr", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/tools.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\n\n\ndef recipr(x): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\ndef recipr(x):\n    \"\"\"\n    Reciprocal of an array with entries less than or equal to 0 set to 0.\n\n    Parameters\n    ----------\n    x : array_like\n        The input array.\n\n    Returns\n    -------\n    ndarray\n        The array with 0-filled reciprocals.\n    \"\"\"\n    x = np.asarray(x)\n    out = np.zeros_like(x, dtype=np.float64)\n    nans = np.isnan(x.flat)\n    pos = ~nans\n    pos[pos] = pos[pos] & (x.flat[pos] > 0)\n    out.flat[pos] = 1.0 / x.flat[pos]\n    out.flat[nans] = np.nan\n    return out"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/validation/tests/test_validation.py", "method_code_mask": "from collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import PandasWrapper\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.validation.validation import _right_squeeze\n\n\ndef gen_data(dim, use_pandas): [MASK]\n", "response": "from collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import PandasWrapper\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.validation.validation import _right_squeeze\ndef gen_data(dim, use_pandas):\n    if dim == 1:\n        out = np.empty(10)\n        if use_pandas:\n            out = pd.Series(out)\n    elif dim == 2:\n        out = np.empty((20, 10))\n        if use_pandas:\n            out = pd.DataFrame(out)\n    else:\n        out = np.empty(np.arange(5, 5 + dim))\n    return out"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_approx_fprime_scalar", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/numdiff.py", "method_code_mask": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\n\n\ndef _approx_fprime_scalar(x, f, epsilon=None, args=(), kwargs={}, centered=\n    False): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\ndef _approx_fprime_scalar(x, f, epsilon=None, args=(), kwargs={}, centered=\n    False):\n    \"\"\"\n    Gradient of function vectorized for scalar parameter.\n\n    This assumes that the function ``f`` is vectorized for a scalar parameter.\n    The function value ``f(x)`` has then the same shape as the input ``x``.\n    The derivative returned by this function also has the same shape as ``x``.\n\n    Parameters\n    ----------\n    x : ndarray\n        Parameters at which the derivative is evaluated.\n    f : function\n        `f(*((x,)+args), **kwargs)` returning either one value or 1d array\n    epsilon : float, optional\n        Stepsize, if None, optimal stepsize is used. This is EPS**(1/2)*x for\n        `centered` == False and EPS**(1/3)*x for `centered` == True.\n    args : tuple\n        Tuple of additional arguments for function `f`.\n    kwargs : dict\n        Dictionary of additional keyword arguments for function `f`.\n    centered : bool\n        Whether central difference should be returned. If not, does forward\n        differencing.\n\n    Returns\n    -------\n    grad : ndarray\n        Array of derivatives, gradient evaluated at parameters ``x``.\n    \"\"\"\n    x = np.asarray(x)\n    n = 1\n    f0 = f(*((x,) + args), **kwargs)\n    if not centered:\n        eps = _get_epsilon(x, 2, epsilon, n)\n        grad = (f(*((x + eps,) + args), **kwargs) - f0) / eps\n    else:\n        eps = _get_epsilon(x, 3, epsilon, n) / 2.0\n        grad = (f(*((x + eps,) + args), **kwargs) - f(*((x - eps,) + args),\n            **kwargs)) / (2 * eps)\n    return grad"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_generate_url", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/web.py", "method_code_mask": "import webbrowser\nfrom urllib.parse import urlencode\nfrom statsmodels import __version__\n\n\ndef _generate_url(func, stable): [MASK]\n", "response": "import webbrowser\nfrom urllib.parse import urlencode\nfrom statsmodels import __version__\ndef _generate_url(func, stable):\n    \"\"\"\n    Parse inputs and return a correctly formatted URL or raises ValueError\n    if the input is not understandable\n    \"\"\"\n    url = BASE_URL\n    if stable:\n        url += 'stable/'\n    else:\n        url += 'devel/'\n    if func is None:\n        return url\n    elif isinstance(func, str):\n        url += 'search.html?'\n        url += urlencode({'q': func})\n        url += '&check_keywords=yes&area=default'\n    else:\n        try:\n            func = func\n            func_name = func.__name__\n            func_module = func.__module__\n            if not func_module.startswith('statsmodels.'):\n                raise ValueError('Function must be from statsmodels')\n            url += 'generated/'\n            url += func_module + '.' + func_name + '.html'\n        except AttributeError:\n            raise ValueError('Input not understood')\n    return url"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "fullrank", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/tools.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\n\n\ndef fullrank(x, r=None): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\ndef fullrank(x, r=None):\n    \"\"\"\n    Return an array whose column span is the same as x.\n\n    Parameters\n    ----------\n    x : ndarray\n        The array to adjust, 2d.\n    r : int, optional\n        The rank of x. If not provided, determined by `np.linalg.matrix_rank`.\n\n    Returns\n    -------\n    ndarray\n        The array adjusted to have full rank.\n\n    Notes\n    -----\n    If the rank of x is known it can be specified as r -- no check\n    is made to ensure that this really is the rank of x.\n    \"\"\"\n    if r is None:\n        r = np.linalg.matrix_rank(x)\n    v, d, u = np.linalg.svd(x, full_matrices=False)\n    order = np.argsort(d)\n    order = order[::-1]\n    value = []\n    for i in range(r):\n        value.append(v[:, order[i]])\n    return np.asarray(np.transpose(value)).astype(np.float64)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "add_indep", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/catadd.py", "method_code_mask": "import numpy as np\n\n\ndef add_indep(x, varnames, dtype=None): [MASK]\n", "response": "import numpy as np\ndef add_indep(x, varnames, dtype=None):\n    \"\"\"\n    construct array with independent columns\n\n    x is either iterable (list, tuple) or instance of ndarray or a subclass\n    of it.  If x is an ndarray, then each column is assumed to represent a\n    variable with observations in rows.\n    \"\"\"\n    if isinstance(x, np.ndarray) and x.ndim == 2:\n        x = x.T\n    nvars_orig = len(x)\n    nobs = len(x[0])\n    if not dtype:\n        dtype = np.asarray(x[0]).dtype\n    xout = np.zeros((nobs, nvars_orig), dtype=dtype)\n    count = 0\n    rank_old = 0\n    varnames_new = []\n    varnames_dropped = []\n    keepindx = []\n    for xi, ni in zip(x, varnames):\n        xout[:, count] = xi\n        rank_new = np.linalg.matrix_rank(xout)\n        if rank_new > rank_old:\n            varnames_new.append(ni)\n            rank_old = rank_new\n            count += 1\n        else:\n            varnames_dropped.append(ni)\n    return xout[:, :count], varnames_new"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "van_der_corput", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/sequences.py", "method_code_mask": "import numpy as np\n\n\ndef van_der_corput(n_sample, base=2, start_index=0): [MASK]\n", "response": "import numpy as np\ndef van_der_corput(n_sample, base=2, start_index=0):\n    \"\"\"Van der Corput sequence.\n\n    Pseudo-random number generator based on a b-adic expansion.\n\n    Parameters\n    ----------\n    n_sample : int\n        Number of element of the sequence.\n    base : int\n        Base of the sequence.\n    start_index : int\n        Index to start the sequence from.\n\n    Returns\n    -------\n    sequence : list (n_samples,)\n        Sequence of Van der Corput.\n    \"\"\"\n    sequence = []\n    for i in range(start_index, start_index + n_sample):\n        n_th_number, denom = 0.0, 1.0\n        quotient = i\n        while quotient > 0:\n            quotient, remainder = divmod(quotient, base)\n            denom *= base\n            n_th_number += remainder / denom\n        sequence.append(n_th_number)\n    return sequence"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "stationary_solve", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/linalg.py", "method_code_mask": "import numpy as np\nfrom scipy import linalg\nimport warnings\n\n\ndef stationary_solve(r, b): [MASK]\n", "response": "import numpy as np\nfrom scipy import linalg\nimport warnings\ndef stationary_solve(r, b):\n    \"\"\"\n    Solve a linear system for a Toeplitz correlation matrix.\n\n    A Toeplitz correlation matrix represents the covariance of a\n    stationary series with unit variance.\n\n    Parameters\n    ----------\n    r : array_like\n        A vector describing the coefficient matrix.  r[0] is the first\n        band next to the diagonal, r[1] is the second band, etc.\n    b : array_like\n        The right-hand side for which we are solving, i.e. we solve\n        Tx = b and return b, where T is the Toeplitz coefficient matrix.\n\n    Returns\n    -------\n    The solution to the linear system.\n    \"\"\"\n    db = r[0:1]\n    dim = b.ndim\n    if b.ndim == 1:\n        b = b[:, None]\n    x = b[0:1, :]\n    for j in range(1, len(b)):\n        rf = r[0:j][::-1]\n        a = (b[j, :] - np.dot(rf, x)) / (1 - np.dot(rf, db[::-1]))\n        z = x - np.outer(db[::-1], a)\n        x = np.concatenate((z, a[None, :]), axis=0)\n        if j == len(b) - 1:\n            break\n        rn = r[j]\n        a = (rn - np.dot(rf, db)) / (1 - np.dot(rf, db[::-1]))\n        z = db - a * db[::-1]\n        db = np.concatenate((z, np.r_[a]))\n    if dim == 1:\n        x = x[:, 0]\n    return x"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "discrepancy", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/sequences.py", "method_code_mask": "import numpy as np\n\n\ndef discrepancy(sample, bounds=None): [MASK]\n", "response": "import numpy as np\ndef discrepancy(sample, bounds=None):\n    \"\"\"Discrepancy.\n\n    Compute the centered discrepancy on a given sample.\n    It is a measure of the uniformity of the points in the parameter space.\n    The lower the value is, the better the coverage of the parameter space is.\n\n    Parameters\n    ----------\n    sample : array_like (n_samples, k_vars)\n        The sample to compute the discrepancy from.\n    bounds : tuple or array_like ([min, k_vars], [max, k_vars])\n        Desired range of transformed data. The transformation apply the bounds\n        on the sample and not the theoretical space, unit cube. Thus min and\n        max values of the sample will coincide with the bounds.\n\n    Returns\n    -------\n    discrepancy : float\n        Centered discrepancy.\n\n    References\n    ----------\n    [1] Fang et al. \"Design and modeling for computer experiments\",\n      Computer Science and Data Analysis Series Science and Data Analysis\n      Series, 2006.\n    \"\"\"\n    sample = np.asarray(sample)\n    n_sample, dim = sample.shape\n    if bounds is not None:\n        min_ = bounds.min(axis=0)\n        max_ = bounds.max(axis=0)\n        sample = (sample - min_) / (max_ - min_)\n    abs_ = abs(sample - 0.5)\n    disc1 = np.sum(np.prod(1 + 0.5 * abs_ - 0.5 * abs_ ** 2, axis=1))\n    prod_arr = 1\n    for i in range(dim):\n        s0 = sample[:, i]\n        prod_arr *= 1 + 0.5 * abs(s0[:, None] - 0.5) + 0.5 * abs(s0 - 0.5\n            ) - 0.5 * abs(s0[:, None] - s0)\n    disc2 = prod_arr.sum()\n    c2 = (13.0 / 12.0\n        ) ** dim - 2.0 / n_sample * disc1 + 1.0 / n_sample ** 2 * disc2\n    return c2"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "primes_from_2_to", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/sequences.py", "method_code_mask": "import numpy as np\n\n\ndef primes_from_2_to(n): [MASK]\n", "response": "import numpy as np\ndef primes_from_2_to(n):\n    \"\"\"Prime numbers from 2 to *n*.\n\n    Parameters\n    ----------\n    n : int\n        Sup bound with ``n >= 6``.\n\n    Returns\n    -------\n    primes : list(int)\n        Primes in ``2 <= p < n``.\n\n    References\n    ----------\n    [1] `StackOverflow <https://stackoverflow.com/questions/2068372>`_.\n    \"\"\"\n    sieve = np.ones(n // 3 + (n % 6 == 2), dtype=bool)\n    for i in range(1, int(n ** 0.5) // 3 + 1):\n        if sieve[i]:\n            k = 3 * i + 1 | 1\n            sieve[k * k // 3::2 * k] = False\n            sieve[k * (k - 2 * (i & 1) + 4) // 3::2 * k] = False\n    return np.r_[2, 3, 3 * np.nonzero(sieve)[0][1:] + 1 | 1]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "pinv_extended", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/tools.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\n\n\ndef pinv_extended(x, rcond=1e-15): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nimport scipy.linalg\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.tsatools import add_trend\ndef pinv_extended(x, rcond=1e-15):\n    \"\"\"\n    Return the pinv of an array X as well as the singular values\n    used in computation.\n\n    Code adapted from numpy.\n    \"\"\"\n    x = np.asarray(x)\n    x = x.conjugate()\n    u, s, vt = np.linalg.svd(x, False)\n    s_orig = np.copy(s)\n    m = u.shape[0]\n    n = vt.shape[1]\n    cutoff = rcond * np.maximum.reduce(s)\n    for i in range(min(n, m)):\n        if s[i] > cutoff:\n            s[i] = 1.0 / s[i]\n        else:\n            s[i] = 0.0\n    res = np.dot(np.transpose(vt), np.multiply(s[:, np.newaxis], np.\n        transpose(u)))\n    return res, s_orig"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_next_regular", "method_path": "../srcdata/Computation/statsmodels/statsmodels/compat/scipy.py", "method_code_mask": "from packaging.version import Version\nfrom packaging.version import parse\nimport numpy as np\nimport scipy\nfrom scipy.stats import multivariate_t\n\n\ndef _next_regular(target): [MASK]\n", "response": "from packaging.version import Version\nfrom packaging.version import parse\nimport numpy as np\nimport scipy\nfrom scipy.stats import multivariate_t\ndef _next_regular(target):\n    \"\"\"\n    Find the next regular number greater than or equal to target.\n    Regular numbers are composites of the prime factors 2, 3, and 5.\n    Also known as 5-smooth numbers or Hamming numbers, these are the optimal\n    size for inputs to FFTPACK.\n\n    Target must be a positive integer.\n    \"\"\"\n    if target <= 6:\n        return target\n    if not target & target - 1:\n        return target\n    match = float('inf')\n    p5 = 1\n    while p5 < target:\n        p35 = p5\n        while p35 < target:\n            quotient = -(-target // p35)\n            p2 = 2 ** (quotient - 1).bit_length()\n            N = p2 * p35\n            if N == target:\n                return N\n            elif N < match:\n                match = N\n            p35 *= 3\n            if p35 == target:\n                return p35\n        if p35 < match:\n            match = p35\n        p5 *= 5\n        if p5 == target:\n            return p5\n    if p5 < match:\n        match = p5\n    return match"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "approx_hess_cs", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/numdiff.py", "method_code_mask": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\n\n\ndef approx_hess_cs(x, f, epsilon=None, args=(), kwargs={}): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\ndef approx_hess_cs(x, f, epsilon=None, args=(), kwargs={}):\n    \"\"\"Calculate Hessian with complex-step derivative approximation\n\n    Parameters\n    ----------\n    x : array_like\n       value at which function derivative is evaluated\n    f : function\n       function of one array f(x)\n    epsilon : float\n       stepsize, if None, then stepsize is automatically chosen\n\n    Returns\n    -------\n    hess : ndarray\n       array of partial second derivatives, Hessian\n\n    Notes\n    -----\n    based on equation 10 in\n    M. S. RIDOUT: Statistical Applications of the Complex-step Method\n    of Numerical Differentiation, University of Kent, Canterbury, Kent, U.K.\n\n    The stepsize is the same for the complex and the finite difference part.\n    \"\"\"\n    n = len(x)\n    h = _get_epsilon(x, 3, epsilon, n)\n    ee = np.diag(h)\n    hess = np.outer(h, h)\n    n = len(x)\n    for i in range(n):\n        for j in range(i, n):\n            hess[i, j] = np.squeeze((f(*((x + 1.0j * ee[i, :] + ee[j, :],) +\n                args), **kwargs) - f(*((x + 1.0j * ee[i, :] - ee[j, :],) +\n                args), **kwargs)).imag / 2.0 / hess[i, j])\n            hess[j, i] = hess[i, j]\n    return hess"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "make_hypotheses_matrices", "method_path": "../srcdata/Computation/statsmodels/statsmodels/formula/formulatools.py", "method_code_mask": "import statsmodels.tools.data as data_util\nfrom patsy import dmatrices\nfrom patsy import NAAction\nimport numpy as np\nfrom patsy.desc import INTERCEPT\nfrom numpy import array\nfrom patsy.constraint import linear_constraint\n\n\ndef make_hypotheses_matrices(model_results, test_formula): [MASK]\n", "response": "import statsmodels.tools.data as data_util\nfrom patsy import dmatrices\nfrom patsy import NAAction\nimport numpy as np\nfrom patsy.desc import INTERCEPT\nfrom numpy import array\nfrom patsy.constraint import linear_constraint\ndef make_hypotheses_matrices(model_results, test_formula):\n    \"\"\"\n    \"\"\"\n    from patsy.constraint import linear_constraint\n    exog_names = model_results.model.exog_names\n    LC = linear_constraint(test_formula, exog_names)\n    return LC"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_endog", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_glm_weights.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_raises\nimport pandas as pd\nimport pytest\nimport statsmodels.api as sm\nfrom statsmodels.datasets.cpunish import load\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.tools.sm_exceptions import SpecificationWarning\nfrom statsmodels.tools.tools import add_constant\nfrom copy import copy\nimport statsmodels.formula.api as smf\nfrom scipy.stats.distributions import nbinom\nfrom scipy.stats.distributions import invgauss\nfrom statsmodels.datasets.star98 import load\n\n\ndef gen_endog(lin_pred, family_class, link, binom_version=0): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_raises\nimport pandas as pd\nimport pytest\nimport statsmodels.api as sm\nfrom statsmodels.datasets.cpunish import load\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.tools.sm_exceptions import SpecificationWarning\nfrom statsmodels.tools.tools import add_constant\nfrom copy import copy\nimport statsmodels.formula.api as smf\nfrom scipy.stats.distributions import nbinom\nfrom scipy.stats.distributions import invgauss\nfrom statsmodels.datasets.star98 import load\ndef gen_endog(lin_pred, family_class, link, binom_version=0):\n    np.random.seed(872)\n    fam = sm.families\n    mu = link().inverse(lin_pred)\n    if family_class == fam.Binomial:\n        if binom_version == 0:\n            endog = 1 * (np.random.uniform(size=len(lin_pred)) < mu)\n        else:\n            endog = np.empty((len(lin_pred), 2))\n            n = 10\n            endog[:, 0] = (np.random.uniform(size=(len(lin_pred), n)) < mu[\n                :, None]).sum(1)\n            endog[:, 1] = n - endog[:, 0]\n    elif family_class == fam.Poisson:\n        endog = np.random.poisson(mu)\n    elif family_class == fam.Gamma:\n        endog = np.random.gamma(2, mu)\n    elif family_class == fam.Gaussian:\n        endog = mu + np.random.normal(size=len(lin_pred))\n    elif family_class == fam.NegativeBinomial:\n        from scipy.stats.distributions import nbinom\n        endog = nbinom.rvs(mu, 0.5)\n    elif family_class == fam.InverseGaussian:\n        from scipy.stats.distributions import invgauss\n        endog = invgauss.rvs(mu)\n    elif family_class == fam.Tweedie:\n        rate = 1\n        shape = 1.0\n        scale = mu / (rate * shape)\n        endog = np.random.poisson(rate, size=scale.shape[0]) * np.random.gamma(\n            shape * scale)\n    else:\n        raise ValueError\n    return endog"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_calc_grad", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/distributed_estimation.py", "method_code_mask": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\n\n\ndef _calc_grad(mod, params, alpha, L1_wt, score_kwds): [MASK]\n", "response": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\ndef _calc_grad(mod, params, alpha, L1_wt, score_kwds):\n    \"\"\"calculates the log-likelihood gradient for the debiasing\n\n    Parameters\n    ----------\n    mod : statsmodels model class instance\n        The model for the current partition.\n    params : array_like\n        The estimated coefficients for the current partition.\n    alpha : scalar or array_like\n        The penalty weight.  If a scalar, the same penalty weight\n        applies to all variables in the model.  If a vector, it\n        must have the same length as `params`, and contains a\n        penalty weight for each coefficient.\n    L1_wt : scalar\n        The fraction of the penalty given to the L1 penalty term.\n        Must be between 0 and 1 (inclusive).  If 0, the fit is\n        a ridge fit, if 1 it is a lasso fit.\n    score_kwds : dict-like or None\n        Keyword arguments for the score function.\n\n    Returns\n    -------\n    An array-like object of the same dimension as params\n\n    Notes\n    -----\n    In general:\n\n    gradient l_k(params)\n\n    where k corresponds to the index of the partition\n\n    For OLS:\n\n    X^T(y - X^T params)\n    \"\"\"\n    grad = -mod.score(np.asarray(params), **score_kwds)\n    grad += alpha * (1 - L1_wt)\n    return grad"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_isdummy", "method_path": "../srcdata/Computation/statsmodels/statsmodels/discrete/discrete_margins.py", "method_code_mask": "from statsmodels.compat.python import lzip\nimport numpy as np\nfrom scipy.stats import norm\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom pandas import DataFrame\nfrom pandas import MultiIndex\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib.summary import summary_params\nfrom statsmodels.iolib.summary import table_extend\n\n\ndef _isdummy(X): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nimport numpy as np\nfrom scipy.stats import norm\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom pandas import DataFrame\nfrom pandas import MultiIndex\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib.summary import summary_params\nfrom statsmodels.iolib.summary import table_extend\ndef _isdummy(X):\n    \"\"\"\n    Given an array X, returns the column indices for the dummy variables.\n\n    Parameters\n    ----------\n    X : array_like\n        A 1d or 2d array of numbers\n\n    Examples\n    --------\n    >>> X = np.random.randint(0, 2, size=(15,5)).astype(float)\n    >>> X[:,1:3] = np.random.randn(15,2)\n    >>> ind = _isdummy(X)\n    >>> ind\n    array([0, 3, 4])\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim > 1:\n        ind = np.zeros(X.shape[1]).astype(bool)\n    max = np.max(X, axis=0) == 1\n    min = np.min(X, axis=0) == 0\n    remainder = np.all(X % 1.0 == 0, axis=0)\n    ind = min & max & remainder\n    if X.ndim == 1:\n        ind = np.asarray([ind])\n    return np.where(ind)[0]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_iscount", "method_path": "../srcdata/Computation/statsmodels/statsmodels/discrete/discrete_margins.py", "method_code_mask": "from statsmodels.compat.python import lzip\nimport numpy as np\nfrom scipy.stats import norm\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom pandas import DataFrame\nfrom pandas import MultiIndex\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib.summary import summary_params\nfrom statsmodels.iolib.summary import table_extend\n\n\ndef _iscount(X): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nimport numpy as np\nfrom scipy.stats import norm\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom pandas import DataFrame\nfrom pandas import MultiIndex\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib.summary import summary_params\nfrom statsmodels.iolib.summary import table_extend\ndef _iscount(X):\n    \"\"\"\n    Given an array X, returns the column indices for count variables.\n\n    Parameters\n    ----------\n    X : array_like\n        A 1d or 2d array of numbers\n\n    Examples\n    --------\n    >>> X = np.random.randint(0, 10, size=(15,5)).astype(float)\n    >>> X[:,1:3] = np.random.randn(15,2)\n    >>> ind = _iscount(X)\n    >>> ind\n    array([0, 3, 4])\n    \"\"\"\n    X = np.asarray(X)\n    remainder = np.logical_and(np.logical_and(np.all(X % 1.0 == 0, axis=0),\n        X.var(0) != 0), np.all(X >= 0, axis=0))\n    dummy = _isdummy(X)\n    remainder = np.where(remainder)[0].tolist()\n    for idx in dummy:\n        remainder.remove(idx)\n    return np.array(remainder)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_calc_wdesign_mat", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/distributed_estimation.py", "method_code_mask": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\n\n\ndef _calc_wdesign_mat(mod, params, hess_kwds): [MASK]\n", "response": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\ndef _calc_wdesign_mat(mod, params, hess_kwds):\n    \"\"\"calculates the weighted design matrix necessary to generate\n    the approximate inverse covariance matrix\n\n    Parameters\n    ----------\n    mod : statsmodels model class instance\n        The model for the current partition.\n    params : array_like\n        The estimated coefficients for the current partition.\n    hess_kwds : dict-like or None\n        Keyword arguments for the hessian function.\n\n    Returns\n    -------\n    An array-like object, updated design matrix, same dimension\n    as mod.exog\n    \"\"\"\n    rhess = np.sqrt(mod.hessian_factor(np.asarray(params), **hess_kwds))\n    return rhess[:, None] * mod.exog"}
{"result": "timed out", "model_name": "std", "repository": "statsmodels", "full_method_name": "_spg_optim", "method_path": "../srcdata/Computation/statsmodels/statsmodels/stats/correlation_tools.py", "method_code_mask": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\n\n\ndef _spg_optim(func, grad, start, project, maxiter=10000.0, M=10, ctol=\n    0.001, maxiter_nmls=200, lam_min=1e-30, lam_max=1e+30, sig1=0.1, sig2=\n    0.9, gam=0.0001): [MASK]\n", "response": "import numpy as np\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import svds\nfrom scipy.optimize import fminbound\nimport warnings\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import IterationLimitWarning\nfrom statsmodels.tools.sm_exceptions import iteration_limit_doc\nfrom statsmodels.stats.moment_helpers import cov2corr\nfrom statsmodels.stats.moment_helpers import corr2cov\ndef _spg_optim(func, grad, start, project, maxiter=10000.0, M=10, ctol=\n    0.001, maxiter_nmls=200, lam_min=1e-30, lam_max=1e+30, sig1=0.1, sig2=\n    0.9, gam=0.0001):\n    \"\"\"\n    Implements the spectral projected gradient method for minimizing a\n    differentiable function on a convex domain.\n\n    Parameters\n    ----------\n    func : real valued function\n        The objective function to be minimized.\n    grad : real array-valued function\n        The gradient of the objective function\n    start : array_like\n        The starting point\n    project : function\n        In-place projection of the argument to the domain\n        of func.\n    ... See notes regarding additional arguments\n\n    Returns\n    -------\n    rslt : Bunch\n        rslt.params is the final iterate, other fields describe\n        convergence status.\n\n    Notes\n    -----\n    This can be an effective heuristic algorithm for problems where no\n    guaranteed algorithm for computing a global minimizer is known.\n\n    There are a number of tuning parameters, but these generally\n    should not be changed except for `maxiter` (positive integer) and\n    `ctol` (small positive real).  See the Birgin et al reference for\n    more information about the tuning parameters.\n\n    Reference\n    ---------\n    E. Birgin, J.M. Martinez, and M. Raydan. Spectral projected\n    gradient methods: Review and perspectives. Journal of Statistical\n    Software (preprint).  Available at:\n    http://www.ime.usp.br/~egbirgin/publications/bmr5.pdf\n    \"\"\"\n    lam = min(10 * lam_min, lam_max)\n    params = start.copy()\n    gval = grad(params)\n    obj_hist = [func(params)]\n    for itr in range(int(maxiter)):\n        df = params - gval\n        project(df)\n        df -= params\n        if np.max(np.abs(df)) < ctol:\n            return Bunch(**{'Converged': True, 'params': params,\n                'objective_values': obj_hist, 'Message':\n                'Converged successfully'})\n        d = params - lam * gval\n        project(d)\n        d -= params\n        alpha, params1, fval, gval1 = _nmono_linesearch(func, grad, params,\n            d, obj_hist, M=M, sig1=sig1, sig2=sig2, gam=gam, maxiter=\n            maxiter_nmls)\n        if alpha is None:\n            return Bunch(**{'Converged': False, 'params': params,\n                'objective_values': obj_hist, 'Message':\n                'Failed in nmono_linesearch'})\n        obj_hist.append(fval)\n        s = params1 - params\n        y = gval1 - gval\n        sy = (s * y).sum()\n        if sy <= 0:\n            lam = lam_max\n        else:\n            ss = (s * s).sum()\n            lam = max(lam_min, min(ss / sy, lam_max))\n        params = params1\n        gval = gval1\n    return Bunch(**{'Converged': False, 'params': params,\n        'objective_values': obj_hist, 'Message': 'spg_optim did not converge'})"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_endog", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_glm.py", "method_code_mask": "import os\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_array_less\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_raises\nimport pandas as pd\nfrom pandas.testing import assert_series_equal\nimport pytest\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.compat.scipy import SP_LT_17\nfrom statsmodels.datasets import cpunish\nfrom statsmodels.datasets import longley\nfrom statsmodels.discrete import discrete_model as discrete\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import SET_USE_BIC_LLF\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_hess\nfrom statsmodels.tools.numdiff import approx_hess_cs\nfrom statsmodels.tools.sm_exceptions import DomainWarning\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import add_constant\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport copy\nfrom statsmodels.datasets.longley import load\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.datasets.star98 import load\nfrom statsmodels.datasets.scotland import load\nfrom statsmodels.datasets.committee import load\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.regressionplots import add_lowess\nfrom scipy.stats.distributions import nbinom\nfrom scipy.stats.distributions import invgauss\nfrom statsmodels.datasets.fair import load_pandas\nimport patsy\n\n\ndef gen_endog(lin_pred, family_class, link, binom_version=0): [MASK]\n", "response": "import os\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_array_less\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_raises\nimport pandas as pd\nfrom pandas.testing import assert_series_equal\nimport pytest\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.compat.scipy import SP_LT_17\nfrom statsmodels.datasets import cpunish\nfrom statsmodels.datasets import longley\nfrom statsmodels.discrete import discrete_model as discrete\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import SET_USE_BIC_LLF\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_hess\nfrom statsmodels.tools.numdiff import approx_hess_cs\nfrom statsmodels.tools.sm_exceptions import DomainWarning\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import add_constant\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport copy\nfrom statsmodels.datasets.longley import load\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.datasets.star98 import load\nfrom statsmodels.datasets.scotland import load\nfrom statsmodels.datasets.committee import load\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.regressionplots import add_lowess\nfrom scipy.stats.distributions import nbinom\nfrom scipy.stats.distributions import invgauss\nfrom statsmodels.datasets.fair import load_pandas\nimport patsy\ndef gen_endog(lin_pred, family_class, link, binom_version=0):\n    np.random.seed(872)\n    fam = sm.families\n    mu = link().inverse(lin_pred)\n    if family_class == fam.Binomial:\n        if binom_version == 0:\n            endog = 1 * (np.random.uniform(size=len(lin_pred)) < mu)\n        else:\n            endog = np.empty((len(lin_pred), 2))\n            n = 10\n            endog[:, 0] = (np.random.uniform(size=(len(lin_pred), n)) < mu[\n                :, None]).sum(1)\n            endog[:, 1] = n - endog[:, 0]\n    elif family_class == fam.Poisson:\n        endog = np.random.poisson(mu)\n    elif family_class == fam.Gamma:\n        endog = np.random.gamma(2, mu)\n    elif family_class == fam.Gaussian:\n        endog = mu + 2 * np.random.normal(size=len(lin_pred))\n    elif family_class == fam.NegativeBinomial:\n        from scipy.stats.distributions import nbinom\n        endog = nbinom.rvs(mu, 0.5)\n    elif family_class == fam.InverseGaussian:\n        from scipy.stats.distributions import invgauss\n        endog = invgauss.rvs(mu, scale=20)\n    else:\n        raise ValueError\n    return endog"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_crossed_logit", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_bayes_mixed_glm.py", "method_code_mask": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\n\n\ndef gen_crossed_logit(nc, cs, s1, s2): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\ndef gen_crossed_logit(nc, cs, s1, s2):\n    np.random.seed(3799)\n    a = np.kron(np.eye(nc), np.ones((cs, 1)))\n    b = np.kron(np.ones((cs, 1)), np.eye(nc))\n    exog_vc = np.concatenate((a, b), axis=1)\n    exog_fe = np.random.normal(size=(nc * cs, 1))\n    vc = s1 * np.random.normal(size=2 * nc)\n    vc[nc:] *= s2 / s1\n    lp = np.dot(exog_fe, np.r_[-0.5]) + np.dot(exog_vc, vc)\n    pr = 1 / (1 + np.exp(-lp))\n    y = 1 * (np.random.uniform(size=nc * cs) < pr)\n    ident = np.zeros(2 * nc, dtype=int)\n    ident[nc:] = 1\n    return y, exog_fe, exog_vc, ident"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "approx_fprime_cs", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/numdiff.py", "method_code_mask": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\n\n\ndef approx_fprime_cs(x, f, epsilon=None, args=(), kwargs={}): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.compat.pandas import Substitution\ndef approx_fprime_cs(x, f, epsilon=None, args=(), kwargs={}):\n    \"\"\"\n    Calculate gradient or Jacobian with complex step derivative approximation\n\n    Parameters\n    ----------\n    x : ndarray\n        parameters at which the derivative is evaluated\n    f : function\n        `f(*((x,)+args), **kwargs)` returning either one value or 1d array\n    epsilon : float, optional\n        Stepsize, if None, optimal stepsize is used. Optimal step-size is\n        EPS*x. See note.\n    args : tuple\n        Tuple of additional arguments for function `f`.\n    kwargs : dict\n        Dictionary of additional keyword arguments for function `f`.\n\n    Returns\n    -------\n    partials : ndarray\n       array of partial derivatives, Gradient or Jacobian\n\n    Notes\n    -----\n    The complex-step derivative has truncation error O(epsilon**2), so\n    truncation error can be eliminated by choosing epsilon to be very small.\n    The complex-step derivative avoids the problem of round-off error with\n    small epsilon because there is no subtraction.\n    \"\"\"\n    n = len(x)\n    epsilon = _get_epsilon(x, 1, epsilon, n)\n    increments = np.identity(n) * 1.0j * epsilon\n    partials = [(f(x + ih, *args, **kwargs).imag / epsilon[i]) for i, ih in\n        enumerate(increments)]\n    return np.array(partials).T"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_simple_poisson", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_bayes_mixed_glm.py", "method_code_mask": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\n\n\ndef gen_simple_poisson(nc, cs, s): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\ndef gen_simple_poisson(nc, cs, s):\n    np.random.seed(3799)\n    exog_vc = np.kron(np.eye(nc), np.ones((cs, 1)))\n    exog_fe = np.random.normal(size=(nc * cs, 2))\n    vc = s * np.random.normal(size=nc)\n    lp = np.dot(exog_fe, np.r_[0.1, -0.1]) + np.dot(exog_vc, vc)\n    r = np.exp(lp)\n    y = np.random.poisson(r)\n    ident = np.zeros(nc, dtype=int)\n    return y, exog_fe, exog_vc, ident"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_crossed_logit_pandas", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_bayes_mixed_glm.py", "method_code_mask": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\n\n\ndef gen_crossed_logit_pandas(nc, cs, s1, s2): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\ndef gen_crossed_logit_pandas(nc, cs, s1, s2):\n    np.random.seed(3799)\n    a = np.kron(np.arange(nc), np.ones(cs))\n    b = np.kron(np.ones(cs), np.arange(nc))\n    fe = np.ones(nc * cs)\n    vc = np.zeros(nc * cs)\n    for i in np.unique(a):\n        ii = np.flatnonzero(a == i)\n        vc[ii] += s1 * np.random.normal()\n    for i in np.unique(b):\n        ii = np.flatnonzero(b == i)\n        vc[ii] += s2 * np.random.normal()\n    lp = -0.5 * fe + vc\n    pr = 1 / (1 + np.exp(-lp))\n    y = 1 * (np.random.uniform(size=nc * cs) < pr)\n    ident = np.zeros(2 * nc, dtype=int)\n    ident[nc:] = 1\n    df = pd.DataFrame({'fe': fe, 'a': a, 'b': b, 'y': y})\n    return df"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_join_naive", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/distributed_estimation.py", "method_code_mask": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\n\n\ndef _join_naive(params_l, threshold=0): [MASK]\n", "response": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\ndef _join_naive(params_l, threshold=0):\n    \"\"\"joins the results from each run of _est_<type>_naive\n    and returns the mean estimate of the coefficients\n\n    Parameters\n    ----------\n    params_l : list\n        A list of arrays of coefficients.\n    threshold : scalar\n        The threshold at which the coefficients will be cut.\n    \"\"\"\n    p = len(params_l[0])\n    partitions = len(params_l)\n    params_mn = np.zeros(p)\n    for params in params_l:\n        params_mn += params\n    params_mn /= partitions\n    params_mn[np.abs(params_mn) < threshold] = 0\n    return params_mn"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_est_unregularized_naive", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/distributed_estimation.py", "method_code_mask": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\n\n\ndef _est_unregularized_naive(mod, pnum, partitions, fit_kwds=None): [MASK]\n", "response": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\ndef _est_unregularized_naive(mod, pnum, partitions, fit_kwds=None):\n    \"\"\"estimates the unregularized fitted parameters.\n\n    Parameters\n    ----------\n    mod : statsmodels model class instance\n        The model for the current partition.\n    pnum : scalar\n        Index of current partition\n    partitions : scalar\n        Total number of partitions\n    fit_kwds : dict-like or None\n        Keyword arguments to be given to fit\n\n    Returns\n    -------\n    An array of the parameters for the fit\n    \"\"\"\n    if fit_kwds is None:\n        raise ValueError('_est_unregularized_naive currently ' +\n            'requires that fit_kwds not be None.')\n    return mod.fit(**fit_kwds).params"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_join_debiased", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/distributed_estimation.py", "method_code_mask": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\n\n\ndef _join_debiased(results_l, threshold=0): [MASK]\n", "response": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\ndef _join_debiased(results_l, threshold=0):\n    \"\"\"joins the results from each run of _est_regularized_debiased\n    and returns the debiased estimate of the coefficients\n\n    Parameters\n    ----------\n    results_l : list\n        A list of tuples each one containing the params, grad,\n        nodewise_row and nodewise_weight values for each partition.\n    threshold : scalar\n        The threshold at which the coefficients will be cut.\n    \"\"\"\n    p = len(results_l[0][0])\n    partitions = len(results_l)\n    params_mn = np.zeros(p)\n    grad_mn = np.zeros(p)\n    nodewise_row_l = []\n    nodewise_weight_l = []\n    for r in results_l:\n        params_mn += r[0]\n        grad_mn += r[1]\n        nodewise_row_l.extend(r[2])\n        nodewise_weight_l.extend(r[3])\n    nodewise_row_l = np.array(nodewise_row_l)\n    nodewise_weight_l = np.array(nodewise_weight_l)\n    params_mn /= partitions\n    grad_mn *= -1.0 / partitions\n    approx_inv_cov = _calc_approx_inv_cov(nodewise_row_l, nodewise_weight_l)\n    debiased_params = params_mn + approx_inv_cov.dot(grad_mn)\n    debiased_params[np.abs(debiased_params) < threshold] = 0\n    return debiased_params"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "handle_formula_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/formula/formulatools.py", "method_code_mask": "import statsmodels.tools.data as data_util\nfrom patsy import dmatrices\nfrom patsy import NAAction\nimport numpy as np\nfrom patsy.desc import INTERCEPT\nfrom numpy import array\nfrom patsy.constraint import linear_constraint\n\n\ndef handle_formula_data(Y, X, formula, depth=0, missing='drop'): [MASK]\n", "response": "import statsmodels.tools.data as data_util\nfrom patsy import dmatrices\nfrom patsy import NAAction\nimport numpy as np\nfrom patsy.desc import INTERCEPT\nfrom numpy import array\nfrom patsy.constraint import linear_constraint\ndef handle_formula_data(Y, X, formula, depth=0, missing='drop'):\n    \"\"\"\n    Returns endog, exog, and the model specification from arrays and formula.\n\n    Parameters\n    ----------\n    Y : array_like\n        Either endog (the LHS) of a model specification or all of the data.\n        Y must define __getitem__ for now.\n    X : array_like\n        Either exog or None. If all the data for the formula is provided in\n        Y then you must explicitly set X to None.\n    formula : str or patsy.model_desc\n        You can pass a handler by import formula_handler and adding a\n        key-value pair where the key is the formula object class and\n        the value is a function that returns endog, exog, formula object.\n\n    Returns\n    -------\n    endog : array_like\n        Should preserve the input type of Y,X.\n    exog : array_like\n        Should preserve the input type of Y,X. Could be None.\n    \"\"\"\n    if isinstance(formula, tuple(formula_handler.keys())):\n        return formula_handler[type(formula)]\n    na_action = NAAction(on_NA=missing)\n    if X is not None:\n        if data_util._is_using_pandas(Y, X):\n            result = dmatrices(formula, (Y, X), depth, return_type=\n                'dataframe', NA_action=na_action)\n        else:\n            result = dmatrices(formula, (Y, X), depth, return_type=\n                'dataframe', NA_action=na_action)\n    elif data_util._is_using_pandas(Y, None):\n        result = dmatrices(formula, Y, depth, return_type='dataframe',\n            NA_action=na_action)\n    else:\n        result = dmatrices(formula, Y, depth, return_type='dataframe',\n            NA_action=na_action)\n    missing_mask = getattr(na_action, 'missing_mask', None)\n    if not np.any(missing_mask):\n        missing_mask = None\n    if len(result) > 1:\n        design_info = result[1].design_info\n    else:\n        design_info = None\n    return result, missing_mask, design_info"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_est_regularized_debiased", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/distributed_estimation.py", "method_code_mask": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\n\n\ndef _est_regularized_debiased(mod, mnum, partitions, fit_kwds=None,\n    score_kwds=None, hess_kwds=None): [MASK]\n", "response": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\ndef _est_regularized_debiased(mod, mnum, partitions, fit_kwds=None,\n    score_kwds=None, hess_kwds=None):\n    \"\"\"estimates the regularized fitted parameters, is the default\n    estimation_method for class DistributedModel.\n\n    Parameters\n    ----------\n    mod : statsmodels model class instance\n        The model for the current partition.\n    mnum : scalar\n        Index of current partition.\n    partitions : scalar\n        Total number of partitions.\n    fit_kwds : dict-like or None\n        Keyword arguments to be given to fit_regularized\n    score_kwds : dict-like or None\n        Keyword arguments for the score function.\n    hess_kwds : dict-like or None\n        Keyword arguments for the Hessian function.\n\n    Returns\n    -------\n    A tuple of parameters for regularized fit\n        An array-like object of the fitted parameters, params\n        An array-like object for the gradient\n        A list of array like objects for nodewise_row\n        A list of array like objects for nodewise_weight\n    \"\"\"\n    score_kwds = {} if score_kwds is None else score_kwds\n    hess_kwds = {} if hess_kwds is None else hess_kwds\n    if fit_kwds is None:\n        raise ValueError('_est_regularized_debiased currently ' +\n            'requires that fit_kwds not be None.')\n    else:\n        alpha = fit_kwds['alpha']\n    if 'L1_wt' in fit_kwds:\n        L1_wt = fit_kwds['L1_wt']\n    else:\n        L1_wt = 1\n    nobs, p = mod.exog.shape\n    p_part = int(np.ceil(1.0 * p / partitions))\n    params = mod.fit_regularized(**fit_kwds).params\n    grad = _calc_grad(mod, params, alpha, L1_wt, score_kwds) / nobs\n    wexog = _calc_wdesign_mat(mod, params, hess_kwds)\n    nodewise_row_l = []\n    nodewise_weight_l = []\n    for idx in range(mnum * p_part, min((mnum + 1) * p_part, p)):\n        nodewise_row = _calc_nodewise_row(wexog, idx, alpha)\n        nodewise_row_l.append(nodewise_row)\n        nodewise_weight = _calc_nodewise_weight(wexog, nodewise_row, idx, alpha\n            )\n        nodewise_weight_l.append(nodewise_weight)\n    return params, grad, nodewise_row_l, nodewise_weight_l"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_est_regularized_naive", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/distributed_estimation.py", "method_code_mask": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\n\n\ndef _est_regularized_naive(mod, pnum, partitions, fit_kwds=None): [MASK]\n", "response": "from statsmodels.base.elastic_net import RegularizedResults\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_row\nfrom statsmodels.stats.regularized_covariance import _calc_nodewise_weight\nfrom statsmodels.stats.regularized_covariance import _calc_approx_inv_cov\nfrom statsmodels.base.model import LikelihoodModelResults\nfrom statsmodels.regression.linear_model import OLS\nimport numpy as np\nfrom statsmodels.tools.parallel import parallel_func\ndef _est_regularized_naive(mod, pnum, partitions, fit_kwds=None):\n    \"\"\"estimates the regularized fitted parameters.\n\n    Parameters\n    ----------\n    mod : statsmodels model class instance\n        The model for the current partition.\n    pnum : scalar\n        Index of current partition\n    partitions : scalar\n        Total number of partitions\n    fit_kwds : dict-like or None\n        Keyword arguments to be given to fit_regularized\n\n    Returns\n    -------\n    An array of the parameters for the regularized fit\n    \"\"\"\n    if fit_kwds is None:\n        raise ValueError('_est_regularized_naive currently ' +\n            'requires that fit_kwds not be None.')\n    return mod.fit_regularized(**fit_kwds).params"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "mahalanobis", "method_path": "../srcdata/Computation/statsmodels/statsmodels/robust/covariance.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy import linalg\nfrom scipy.linalg.lapack import dtrtri\nimport statsmodels.robust.norms as rnorms\nimport statsmodels.robust.scale as rscale\nfrom statsmodels.tools.testing import Holder\nimport warnings\n\n\ndef mahalanobis(data, cov=None, cov_inv=None, sqrt=False): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy import linalg\nfrom scipy.linalg.lapack import dtrtri\nimport statsmodels.robust.norms as rnorms\nimport statsmodels.robust.scale as rscale\nfrom statsmodels.tools.testing import Holder\nimport warnings\ndef mahalanobis(data, cov=None, cov_inv=None, sqrt=False):\n    \"\"\"Mahalanobis distance squared\n\n    Note: this is without taking the square root.\n    assumes data is already centered.\n\n    Parameters\n    ----------\n    data : array-like\n        Multivariate data with observation in rows.\n    cov : None or ndarray\n        Covariance matrix used in computing distance.\n        This is only used if cov_inv is None.\n    cov_inv : None or ndarray\n        Inverse ovariance matrix used in computing distance.\n        One of cov and cov_inv needs to be provided.\n    sqrt : bool\n        If False, then the squared distance is returned.\n        If True, then the square root is returmend.\n\n    Return\n    ------\n    ndarray : Mahalanobis distances or squared distance.\n    \"\"\"\n    x = np.asarray(data)\n    if cov_inv is not None:\n        d = (x * cov_inv.dot(x.T).T).sum(1)\n    elif cov is not None:\n        d = (x * np.linalg.solve(cov, x.T).T).sum(1)\n    else:\n        raise ValueError('either cov or cov_inv needs to be given')\n    if sqrt:\n        d = np.sqrt(d)\n    return d"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "qn_scale", "method_path": "../srcdata/Computation/statsmodels/statsmodels/robust/scale.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm as Gaussian\nfrom statsmodels.tools import tools\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import float_like\n\n\ndef qn_scale(a, c=1 / (np.sqrt(2) * Gaussian.ppf(5 / 8)), axis=0): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm as Gaussian\nfrom statsmodels.tools import tools\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import float_like\ndef qn_scale(a, c=1 / (np.sqrt(2) * Gaussian.ppf(5 / 8)), axis=0):\n    \"\"\"\n    Computes the Qn robust estimator of scale\n\n    The Qn scale estimator is a more efficient alternative to the MAD.\n    The Qn scale estimator of an array a of length n is defined as\n    c * {abs(a[i] - a[j]): i<j}_(k), for k equal to [n/2] + 1 choose 2. Thus,\n    the Qn estimator is the k-th order statistic of the absolute differences\n    of the array. The optional constant is used to normalize the estimate\n    as explained below. The implementation follows the algorithm described\n    in Croux and Rousseeuw (1992).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    c : float, optional\n        The normalization constant. The default value is used to get consistent\n        estimates of the standard deviation at the normal distribution.\n    axis : int, optional\n        The default is 0.\n\n    Returns\n    -------\n    {float, ndarray}\n        The Qn robust estimator of scale\n    \"\"\"\n    a = array_like(a, 'a', ndim=None, dtype=np.float64, contiguous=True,\n        order='C')\n    c = float_like(c, 'c')\n    if a.ndim == 0:\n        raise ValueError('a should have at least one dimension')\n    elif a.size == 0:\n        return np.nan\n    else:\n        out = np.apply_along_axis(_qn, axis=axis, arr=a, c=c)\n        if out.ndim == 0:\n            return float(out)\n        return out"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_outlier_gy", "method_path": "../srcdata/Computation/statsmodels/statsmodels/robust/covariance.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy import linalg\nfrom scipy.linalg.lapack import dtrtri\nimport statsmodels.robust.norms as rnorms\nimport statsmodels.robust.scale as rscale\nfrom statsmodels.tools.testing import Holder\nimport warnings\n\n\ndef _outlier_gy(d, distr=None, k_endog=1, trim_prob=0.975): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy import linalg\nfrom scipy.linalg.lapack import dtrtri\nimport statsmodels.robust.norms as rnorms\nimport statsmodels.robust.scale as rscale\nfrom statsmodels.tools.testing import Holder\nimport warnings\ndef _outlier_gy(d, distr=None, k_endog=1, trim_prob=0.975):\n    \"\"\"determine outlier fraction given reference distribution\n\n    This implements the outlier cutoff of Gervini and Yohai 2002\n    for use in efficient reweighting.\n\n    Parameters\n    ----------\n    d : array_like, 1-D\n        array of squared standardized residuals or Mahalanobis distance\n    distr : None or distribution instance\n        reference distribution of d, needs cdf and ppf methods.\n        If None, then chisquare with k_endog degrees of freedom is\n        used. Otherwise, it should be a callable that provides the\n        cdf function\n    k_endog : int or float\n        used only if cdf is None. In that case, it provides the degrees\n        of freedom for the chisquare distribution.\n    trim_prob : float in (0.5, 1)\n        threshold for the tail probability at which the search for\n        trimming or outlier fraction starts.\n\n    Returns\n    -------\n    frac : float\n        fraction of outliers\n    cutoff : float\n        cutoff value, values with `d > cutoff` are considered outliers\n    ntail : int\n        number of outliers\n    ntail0 : int\n        initial number of outliers based on trim tail probability.\n    cutoff0 : float\n        initial cutoff value based on trim tail probability.\n\n    Notes\n    -----\n    This does not fully correct for multiple testing and does not\n    maintain a familywise error rate or false discovery rate.\n    The error rate goes to zero asymptotically under the null model,\n    i.e. if there are no outliers.\n\n    This might not handle threshold points correctly with discrete\n    distribution.\n    TODO: check weak versus strict inequalities (e.g. in isf)\n\n    This only checks the upper tail of the distribution and of `d`.\n\n    \"\"\"\n    d = np.asarray(d)\n    nobs = d.shape[0]\n    if distr is None:\n        distr = stats.chi2(k_endog)\n    threshold = distr.isf(1 - trim_prob)\n    dtail = np.sort(d[d >= threshold])\n    ntail0 = len(dtail)\n    if ntail0 == 0:\n        return 0, threshold, 0, 0, threshold\n    ranks = np.arange(nobs - ntail0, nobs) / nobs\n    frac = np.maximum(0, distr.cdf(dtail) - ranks).max()\n    ntail = int(nobs * frac)\n    if ntail > 0:\n        cutoff = dtail[-ntail - 1]\n    else:\n        cutoff = dtail[-1] + 1e-15\n    if (dtail > cutoff).sum() < ntail:\n        import warnings\n        warnings.warn(\n            'ties at cutoff, cutoff rule produces feweroutliers than `ntail`')\n    return frac, cutoff, ntail, ntail0, threshold"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_var_normal_jump", "method_path": "../srcdata/Computation/statsmodels/statsmodels/robust/tools.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy import integrate\nfrom scipy import optimize\nfrom statsmodels.tools.testing import Holder\n\n\ndef _var_normal_jump(norm): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy import integrate\nfrom scipy import optimize\nfrom statsmodels.tools.testing import Holder\ndef _var_normal_jump(norm):\n    \"\"\"Variance factor for asymptotic relative efficiency of mean M-estimator.\n\n    The reference distribution is the standard normal distribution.\n    This allows for the case when the psi function is not continuous, i.e.\n    has jumps as in TrimmedMean norm.\n\n    Relative efficiency is 1 / var_normal\n\n    Parameters\n    ----------\n    norm : instance of a RobustNorm subclass.\n        Norm for which variance for relative efficiency is computed.\n\n    Returns\n    -------\n    Variance factor.\n\n    Notes\n    -----\n    This function does not verify that the assumption on the psi function and\n    it's derivative hold.\n\n    Examples\n    --------\n\n    >>> import statsmodels.robust import norms\n    >>> v = _var_normal_jump(norms.HuberT())\n    >>> eff = 1 / v\n    >>> v, eff\n    (1.0526312908510451, 0.950000260007003)\n\n    Reference\n    ---------\n    Menenez et al., but it's also in all text books for robust statistics.\n\n\n    \"\"\"\n    num = stats.norm.expect(lambda x: norm.psi(x) ** 2)\n\n    def func(x):\n        return norm.psi(x) * (-x * np.exp(-x ** 2 / 2) / np.sqrt(2 * np.pi))\n    denom = integrate.quad(func, -np.inf, np.inf)[0]\n    return num / denom ** 2"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_toy", "method_path": "../srcdata/Computation/statsmodels/statsmodels/multivariate/tests/test_ml_factor.py", "method_code_mask": "import numpy as np\nfrom statsmodels.multivariate.factor import Factor\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\nimport warnings\n\n\ndef _toy(): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.multivariate.factor import Factor\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\nimport warnings\ndef _toy():\n    uniq = np.r_[4, 9, 16]\n    load = np.asarray([[3, 1, 2], [2, 5, 8]]).T\n    par = np.r_[2, 3, 4, 3, 1, 2, 2, 5, 8]\n    corr = np.asarray([[1, 0.5, 0.25], [0.5, 1, 0.5], [0.25, 0.5, 1]])\n    return uniq, load, corr, par"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_crossed_poisson", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_bayes_mixed_glm.py", "method_code_mask": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\n\n\ndef gen_crossed_poisson(nc, cs, s1, s2): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\ndef gen_crossed_poisson(nc, cs, s1, s2):\n    np.random.seed(3799)\n    a = np.kron(np.eye(nc), np.ones((cs, 1)))\n    b = np.kron(np.ones((cs, 1)), np.eye(nc))\n    exog_vc = np.concatenate((a, b), axis=1)\n    exog_fe = np.random.normal(size=(nc * cs, 1))\n    vc = s1 * np.random.normal(size=2 * nc)\n    vc[nc:] *= s2 / s1\n    lp = np.dot(exog_fe, np.r_[-0.5]) + np.dot(exog_vc, vc)\n    r = np.exp(lp)\n    y = np.random.poisson(r)\n    ident = np.zeros(2 * nc, dtype=int)\n    ident[nc:] = 1\n    return y, exog_fe, exog_vc, ident"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "make_augmented_matrix", "method_path": "../srcdata/Computation/statsmodels/statsmodels/gam/generalized_additive_model.py", "method_code_mask": "from collections.abc import Iterable\nimport copy\nimport numpy as np\nfrom scipy import optimize\nimport pandas as pd\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.discrete.discrete_model import Logit\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import GLMResults\nfrom statsmodels.genmod.generalized_linear_model import GLMResultsWrapper\nfrom statsmodels.genmod.generalized_linear_model import _check_convergence\nimport statsmodels.regression.linear_model as lm\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.linalg import matrix_sqrt\nfrom statsmodels.base._penalized import PenalizedMixin\nfrom statsmodels.gam.gam_penalties import MultivariateGamPenalty\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCVPath\nfrom statsmodels.gam.gam_cross_validation.cross_validators import KFold\nfrom patsy import dmatrix\nimport warnings\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\n\n\ndef make_augmented_matrix(endog, exog, penalty_matrix, weights): [MASK]\n", "response": "from collections.abc import Iterable\nimport copy\nimport numpy as np\nfrom scipy import optimize\nimport pandas as pd\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.discrete.discrete_model import Logit\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import GLMResults\nfrom statsmodels.genmod.generalized_linear_model import GLMResultsWrapper\nfrom statsmodels.genmod.generalized_linear_model import _check_convergence\nimport statsmodels.regression.linear_model as lm\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.linalg import matrix_sqrt\nfrom statsmodels.base._penalized import PenalizedMixin\nfrom statsmodels.gam.gam_penalties import MultivariateGamPenalty\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCVPath\nfrom statsmodels.gam.gam_cross_validation.cross_validators import KFold\nfrom patsy import dmatrix\nimport warnings\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\ndef make_augmented_matrix(endog, exog, penalty_matrix, weights):\n    \"\"\"augment endog, exog and weights with stochastic restriction matrix\n\n    Parameters\n    ----------\n    endog : ndarray\n        response or endogenous variable\n    exog : ndarray\n        design matrix, matrix of exogenous or explanatory variables\n    penalty_matrix : ndarray, 2-Dim square\n        penality matrix for quadratic penalization\n    weights : ndarray\n        weights for WLS\n\n    Returns\n    -------\n    endog_aug : ndarray\n        augmented response variable\n    exog_aug : ndarray\n        augmented design matrix\n    weights_aug : ndarray\n        augmented weights for WLS\n    \"\"\"\n    y, x, s = endog, exog, penalty_matrix\n    nobs = x.shape[0]\n    rs = matrix_sqrt(s)\n    x1 = np.vstack([x, rs])\n    n_samp1es_x1 = x1.shape[0]\n    y1 = np.array([0.0] * n_samp1es_x1)\n    y1[:nobs] = y\n    id1 = np.array([1.0] * rs.shape[0])\n    w1 = np.concatenate([weights, id1])\n    return y1, x1, w1"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "penalized_wls", "method_path": "../srcdata/Computation/statsmodels/statsmodels/gam/generalized_additive_model.py", "method_code_mask": "from collections.abc import Iterable\nimport copy\nimport numpy as np\nfrom scipy import optimize\nimport pandas as pd\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.discrete.discrete_model import Logit\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import GLMResults\nfrom statsmodels.genmod.generalized_linear_model import GLMResultsWrapper\nfrom statsmodels.genmod.generalized_linear_model import _check_convergence\nimport statsmodels.regression.linear_model as lm\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.linalg import matrix_sqrt\nfrom statsmodels.base._penalized import PenalizedMixin\nfrom statsmodels.gam.gam_penalties import MultivariateGamPenalty\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCVPath\nfrom statsmodels.gam.gam_cross_validation.cross_validators import KFold\nfrom patsy import dmatrix\nimport warnings\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\n\n\ndef penalized_wls(endog, exog, penalty_matrix, weights): [MASK]\n", "response": "from collections.abc import Iterable\nimport copy\nimport numpy as np\nfrom scipy import optimize\nimport pandas as pd\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.discrete.discrete_model import Logit\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import GLMResults\nfrom statsmodels.genmod.generalized_linear_model import GLMResultsWrapper\nfrom statsmodels.genmod.generalized_linear_model import _check_convergence\nimport statsmodels.regression.linear_model as lm\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.linalg import matrix_sqrt\nfrom statsmodels.base._penalized import PenalizedMixin\nfrom statsmodels.gam.gam_penalties import MultivariateGamPenalty\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCVPath\nfrom statsmodels.gam.gam_cross_validation.cross_validators import KFold\nfrom patsy import dmatrix\nimport warnings\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\ndef penalized_wls(endog, exog, penalty_matrix, weights):\n    \"\"\"weighted least squares with quadratic penalty\n\n    Parameters\n    ----------\n    endog : ndarray\n        response or endogenous variable\n    exog : ndarray\n        design matrix, matrix of exogenous or explanatory variables\n    penalty_matrix : ndarray, 2-Dim square\n        penality matrix for quadratic penalization. Note, the penalty_matrix\n        is multiplied by two to match non-pirls fitting methods.\n    weights : ndarray\n        weights for WLS\n\n    Returns\n    -------\n    results : Results instance of WLS\n    \"\"\"\n    y, x, s = endog, exog, penalty_matrix\n    aug_y, aug_x, aug_weights = make_augmented_matrix(y, x, 2 * s, weights)\n    wls_results = lm.WLS(aug_y, aug_x, aug_weights).fit()\n    wls_results.params = wls_results.params.ravel()\n    return wls_results"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_tweedie", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_glm.py", "method_code_mask": "import os\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_array_less\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_raises\nimport pandas as pd\nfrom pandas.testing import assert_series_equal\nimport pytest\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.compat.scipy import SP_LT_17\nfrom statsmodels.datasets import cpunish\nfrom statsmodels.datasets import longley\nfrom statsmodels.discrete import discrete_model as discrete\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import SET_USE_BIC_LLF\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_hess\nfrom statsmodels.tools.numdiff import approx_hess_cs\nfrom statsmodels.tools.sm_exceptions import DomainWarning\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import add_constant\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport copy\nfrom statsmodels.datasets.longley import load\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.datasets.star98 import load\nfrom statsmodels.datasets.scotland import load\nfrom statsmodels.datasets.committee import load\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.regressionplots import add_lowess\nfrom scipy.stats.distributions import nbinom\nfrom scipy.stats.distributions import invgauss\nfrom statsmodels.datasets.fair import load_pandas\nimport patsy\n\n\ndef gen_tweedie(p): [MASK]\n", "response": "import os\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_array_less\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_raises\nimport pandas as pd\nfrom pandas.testing import assert_series_equal\nimport pytest\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.compat.scipy import SP_LT_17\nfrom statsmodels.datasets import cpunish\nfrom statsmodels.datasets import longley\nfrom statsmodels.discrete import discrete_model as discrete\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.generalized_linear_model import SET_USE_BIC_LLF\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom statsmodels.tools.numdiff import approx_hess\nfrom statsmodels.tools.numdiff import approx_hess_cs\nfrom statsmodels.tools.sm_exceptions import DomainWarning\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import add_constant\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport copy\nfrom statsmodels.datasets.longley import load\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.datasets.star98 import load\nfrom statsmodels.datasets.scotland import load\nfrom statsmodels.datasets.committee import load\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.regressionplots import add_lowess\nfrom scipy.stats.distributions import nbinom\nfrom scipy.stats.distributions import invgauss\nfrom statsmodels.datasets.fair import load_pandas\nimport patsy\ndef gen_tweedie(p):\n    np.random.seed(3242)\n    n = 500\n    x = np.random.normal(size=(n, 4))\n    lpr = np.dot(x, np.r_[1, -1, 0, 0.5])\n    mu = np.exp(lpr)\n    lam = 10 * mu ** (2 - p) / (2 - p)\n    alp = (2 - p) / (p - 1)\n    bet = 10 * mu ** (1 - p) / (p - 1)\n    y = np.empty(n)\n    N = np.random.poisson(lam)\n    for i in range(n):\n        y[i] = np.random.gamma(alp, 1 / bet[i], N[i]).sum()\n    return y, x"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_ros_group_rank", "method_path": "../srcdata/Computation/statsmodels/statsmodels/imputation/ros.py", "method_code_mask": "import warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n\ndef _ros_group_rank(df, dl_idx, censorship): [MASK]\n", "response": "import warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef _ros_group_rank(df, dl_idx, censorship):\n    \"\"\"\n    Ranks each observation within the data groups.\n\n    In this case, the groups are defined by the record's detection\n    limit index and censorship status.\n\n    Parameters\n    ----------\n    df : DataFrame\n\n    dl_idx : str\n        Name of the column in the dataframe the index of the\n        observations' corresponding detection limit in the `cohn`\n        dataframe.\n\n    censorship : str\n        Name of the column in the dataframe that indicates that a\n        observation is left-censored. (i.e., True -> censored,\n        False -> uncensored)\n\n    Returns\n    -------\n    ranks : ndarray\n        Array of ranks for the dataset.\n    \"\"\"\n    ranks = df.copy()\n    ranks.loc[:, 'rank'] = 1\n    ranks = ranks.groupby(by=[dl_idx, censorship])['rank'].transform(lambda\n        g: g.cumsum())\n    return ranks"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "load_advanced_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/imputation/tests/test_ros.py", "method_code_mask": "from statsmodels.compat.pandas import assert_series_equal\nfrom statsmodels.compat.pandas import assert_frame_equal\nfrom io import StringIO\nfrom textwrap import dedent\nimport numpy as np\nimport numpy.testing as npt\nimport numpy\nfrom numpy.testing import assert_equal\nimport pandas\nimport pytest\nfrom statsmodels.imputation import ros\n\n\ndef load_advanced_data(): [MASK]\n", "response": "from statsmodels.compat.pandas import assert_series_equal\nfrom statsmodels.compat.pandas import assert_frame_equal\nfrom io import StringIO\nfrom textwrap import dedent\nimport numpy as np\nimport numpy.testing as npt\nimport numpy\nfrom numpy.testing import assert_equal\nimport pandas\nimport pytest\nfrom statsmodels.imputation import ros\ndef load_advanced_data():\n    df = pandas.DataFrame([{'Zprelim': -1.4456202174142005, 'censored': \n        True, 'conc': 5.0, 'det_limit_index': 1, 'plot_pos': \n        0.07414187643020594, 'rank': 1}, {'Zprelim': -1.2201035333697587,\n        'censored': True, 'conc': 5.0, 'det_limit_index': 1, 'plot_pos': \n        0.11121281464530891, 'rank': 2}, {'Zprelim': -1.043822530159519,\n        'censored': True, 'conc': 5.5, 'det_limit_index': 2, 'plot_pos': \n        0.14828375286041187, 'rank': 1}, {'Zprelim': -1.0438225301595188,\n        'censored': True, 'conc': 5.75, 'det_limit_index': 3, 'plot_pos': \n        0.1482837528604119, 'rank': 1}, {'Zprelim': -0.8109553641377003,\n        'censored': True, 'conc': 9.5, 'det_limit_index': 4, 'plot_pos': \n        0.20869565217391303, 'rank': 1}, {'Zprelim': -0.4046779045300476,\n        'censored': True, 'conc': 9.5, 'det_limit_index': 4, 'plot_pos': \n        0.34285714285714286, 'rank': 2}, {'Zprelim': -0.20857169501420522,\n        'censored': True, 'conc': 11.0, 'det_limit_index': 5, 'plot_pos': \n        0.41739130434782606, 'rank': 1}, {'Zprelim': -1.5927654676048002,\n        'censored': False, 'conc': 2.0, 'det_limit_index': 0, 'plot_pos': \n        0.055606407322654455, 'rank': 1}, {'Zprelim': -1.2201035333697587,\n        'censored': False, 'conc': 4.2, 'det_limit_index': 0, 'plot_pos': \n        0.11121281464530891, 'rank': 2}, {'Zprelim': -0.9668111610681008,\n        'censored': False, 'conc': 4.62, 'det_limit_index': 0, 'plot_pos': \n        0.16681922196796337, 'rank': 3}, {'Zprelim': -0.6835186393930371,\n        'censored': False, 'conc': 5.57, 'det_limit_index': 2, 'plot_pos': \n        0.24713958810068648, 'rank': 1}, {'Zprelim': -0.6072167256926887,\n        'censored': False, 'conc': 5.66, 'det_limit_index': 2, 'plot_pos': \n        0.27185354691075514, 'rank': 2}, {'Zprelim': -0.44953240276543616,\n        'censored': False, 'conc': 5.86, 'det_limit_index': 3, 'plot_pos': \n        0.3265238194299979, 'rank': 1}, {'Zprelim': -0.36788328223414807,\n        'censored': False, 'conc': 6.65, 'det_limit_index': 3, 'plot_pos': \n        0.35648013313917204, 'rank': 2}, {'Zprelim': -0.28861907892223937,\n        'censored': False, 'conc': 6.78, 'det_limit_index': 3, 'plot_pos': \n        0.38643644684834616, 'rank': 3}, {'Zprelim': -0.21113039741112186,\n        'censored': False, 'conc': 6.79, 'det_limit_index': 3, 'plot_pos': \n        0.4163927605575203, 'rank': 4}, {'Zprelim': -0.1348908823006299,\n        'censored': False, 'conc': 7.5, 'det_limit_index': 3, 'plot_pos': \n        0.4463490742666944, 'rank': 5}, {'Zprelim': -0.05942854708257491,\n        'censored': False, 'conc': 7.5, 'det_limit_index': 3, 'plot_pos': \n        0.4763053879758685, 'rank': 6}, {'Zprelim': 0.015696403006170083,\n        'censored': False, 'conc': 7.5, 'det_limit_index': 3, 'plot_pos': \n        0.5062617016850427, 'rank': 7}, {'Zprelim': 0.09091016994359362,\n        'censored': False, 'conc': 8.63, 'det_limit_index': 3, 'plot_pos': \n        0.5362180153942168, 'rank': 8}, {'Zprelim': 0.16664251178856201,\n        'censored': False, 'conc': 8.71, 'det_limit_index': 3, 'plot_pos': \n        0.5661743291033909, 'rank': 9}, {'Zprelim': 0.24334426739770573,\n        'censored': False, 'conc': 8.99, 'det_limit_index': 3, 'plot_pos': \n        0.596130642812565, 'rank': 10}, {'Zprelim': 0.3744432988606558,\n        'censored': False, 'conc': 9.85, 'det_limit_index': 4, 'plot_pos': \n        0.6459627329192545, 'rank': 1}, {'Zprelim': 0.4284507519609981,\n        'censored': False, 'conc': 10.82, 'det_limit_index': 4, 'plot_pos':\n        0.6658385093167701, 'rank': 2}, {'Zprelim': 0.5589578655042562,\n        'censored': False, 'conc': 11.25, 'det_limit_index': 5, 'plot_pos':\n        0.7119047619047619, 'rank': 1}, {'Zprelim': 0.6374841609623771,\n        'censored': False, 'conc': 11.25, 'det_limit_index': 5, 'plot_pos':\n        0.7380952380952381, 'rank': 2}, {'Zprelim': 0.7201566171385521,\n        'censored': False, 'conc': 12.2, 'det_limit_index': 5, 'plot_pos': \n        0.7642857142857142, 'rank': 3}, {'Zprelim': 0.8080746339118065,\n        'censored': False, 'conc': 14.92, 'det_limit_index': 5, 'plot_pos':\n        0.7904761904761904, 'rank': 4}, {'Zprelim': 0.9027347916438648,\n        'censored': False, 'conc': 16.77, 'det_limit_index': 5, 'plot_pos':\n        0.8166666666666667, 'rank': 5}, {'Zprelim': 1.0062699858608395,\n        'censored': False, 'conc': 17.81, 'det_limit_index': 5, 'plot_pos':\n        0.8428571428571429, 'rank': 6}, {'Zprelim': 1.1219004674623523,\n        'censored': False, 'conc': 19.16, 'det_limit_index': 5, 'plot_pos':\n        0.8690476190476191, 'rank': 7}, {'Zprelim': 1.2548759122271174,\n        'censored': False, 'conc': 19.19, 'det_limit_index': 5, 'plot_pos':\n        0.8952380952380953, 'rank': 8}, {'Zprelim': 1.414746425534976,\n        'censored': False, 'conc': 19.64, 'det_limit_index': 5, 'plot_pos':\n        0.9214285714285714, 'rank': 9}, {'Zprelim': 1.622193585315426,\n        'censored': False, 'conc': 20.18, 'det_limit_index': 5, 'plot_pos':\n        0.9476190476190476, 'rank': 10}, {'Zprelim': 1.9399896117517081,\n        'censored': False, 'conc': 22.97, 'det_limit_index': 5, 'plot_pos':\n        0.9738095238095239, 'rank': 11}])\n    return df"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_norm_plot_pos", "method_path": "../srcdata/Computation/statsmodels/statsmodels/imputation/ros.py", "method_code_mask": "import warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n\ndef _norm_plot_pos(observations): [MASK]\n", "response": "import warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef _norm_plot_pos(observations):\n    \"\"\"\n    Computes standard normal (Gaussian) plotting positions using scipy.\n\n    Parameters\n    ----------\n    observations : array_like\n        Sequence of observed quantities.\n\n    Returns\n    -------\n    plotting_position : array of floats\n    \"\"\"\n    ppos, sorted_res = stats.probplot(observations, fit=False)\n    return stats.norm.cdf(ppos)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_simple_logit", "method_path": "../srcdata/Computation/statsmodels/statsmodels/genmod/tests/test_bayes_mixed_glm.py", "method_code_mask": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\n\n\ndef gen_simple_logit(nc, cs, s): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\nfrom statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM\nimport pandas as pd\nfrom scipy import sparse\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nfrom scipy.optimize import approx_fprime\ndef gen_simple_logit(nc, cs, s):\n    np.random.seed(3799)\n    exog_vc = np.kron(np.eye(nc), np.ones((cs, 1)))\n    exog_fe = np.random.normal(size=(nc * cs, 2))\n    vc = s * np.random.normal(size=nc)\n    lp = np.dot(exog_fe, np.r_[1, -1]) + np.dot(exog_vc, vc)\n    pr = 1 / (1 + np.exp(-lp))\n    y = 1 * (np.random.uniform(size=nc * cs) < pr)\n    ident = np.zeros(nc, dtype=int)\n    return y, exog_fe, exog_vc, ident"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "load_basic_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/imputation/tests/test_ros.py", "method_code_mask": "from statsmodels.compat.pandas import assert_series_equal\nfrom statsmodels.compat.pandas import assert_frame_equal\nfrom io import StringIO\nfrom textwrap import dedent\nimport numpy as np\nimport numpy.testing as npt\nimport numpy\nfrom numpy.testing import assert_equal\nimport pandas\nimport pytest\nfrom statsmodels.imputation import ros\n\n\ndef load_basic_data(): [MASK]\n", "response": "from statsmodels.compat.pandas import assert_series_equal\nfrom statsmodels.compat.pandas import assert_frame_equal\nfrom io import StringIO\nfrom textwrap import dedent\nimport numpy as np\nimport numpy.testing as npt\nimport numpy\nfrom numpy.testing import assert_equal\nimport pandas\nimport pytest\nfrom statsmodels.imputation import ros\ndef load_basic_data():\n    raw_csv = StringIO(\n        \"\"\"res,qual\n2.00,=\n4.20,=\n4.62,=\n5.00,ND\n5.00,ND\n5.50,ND\n5.57,=\n5.66,=\n5.75,ND\n5.86,=\n6.65,=\n6.78,=\n6.79,=\n7.50,=\n7.50,=\n7.50,=\n8.63,=\n8.71,=\n8.99,=\n9.50,ND\n9.50,ND\n9.85,=\n10.82,=\n11.00,ND\n11.25,=\n11.25,=\n12.20,=\n14.92,=\n16.77,=\n17.81,=\n19.16,=\n19.19,=\n19.64,=\n20.18,=\n22.97,=\n\"\"\"\n        )\n    df = pandas.read_csv(raw_csv)\n    df.loc[:, 'conc'] = df['res']\n    df.loc[:, 'censored'] = df['qual'] == 'ND'\n    return df"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "tukey_pvalues", "method_path": "../srcdata/Computation/statsmodels/statsmodels/sandbox/stats/multicomp.py", "method_code_mask": "from collections import namedtuple\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.python import lrange\nimport copy\nimport math\nimport numpy as np\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_equal\nfrom scipy import stats\nfrom scipy import interpolate\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.stats.multitest import _ecdf as ecdf\nfrom statsmodels.stats.multitest import fdrcorrection as fdrcorrection0\nfrom statsmodels.stats.multitest import fdrcorrection_twostage\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom scipy.stats import studentized_range\nfrom statsmodels.stats.libqsturng import qsturng\nfrom statsmodels.stats.libqsturng import psturng\nimport warnings\nfrom statsmodels.sandbox.distributions.multivariate import mvstdtprob\n\n\ndef tukey_pvalues(std_range, nm, df): [MASK]\n", "response": "from collections import namedtuple\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.python import lrange\nimport copy\nimport math\nimport numpy as np\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_equal\nfrom scipy import stats\nfrom scipy import interpolate\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.stats.multitest import _ecdf as ecdf\nfrom statsmodels.stats.multitest import fdrcorrection as fdrcorrection0\nfrom statsmodels.stats.multitest import fdrcorrection_twostage\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom scipy.stats import studentized_range\nfrom statsmodels.stats.libqsturng import qsturng\nfrom statsmodels.stats.libqsturng import psturng\nimport warnings\nfrom statsmodels.sandbox.distributions.multivariate import mvstdtprob\ndef tukey_pvalues(std_range, nm, df):\n    contr = contrast_allpairs(nm)\n    corr = np.dot(contr, contr.T) / 2.0\n    tstat = std_range / np.sqrt(2) * np.ones(corr.shape[0])\n    return multicontrast_pvalues(tstat, corr, df=df)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "matrix_sqrt", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tools/linalg.py", "method_code_mask": "import numpy as np\nfrom scipy import linalg\nimport warnings\n\n\ndef matrix_sqrt(mat, inverse=False, full=False, nullspace=False, threshold=\n    1e-15): [MASK]\n", "response": "import numpy as np\nfrom scipy import linalg\nimport warnings\ndef matrix_sqrt(mat, inverse=False, full=False, nullspace=False, threshold=\n    1e-15):\n    \"\"\"matrix square root for symmetric matrices\n\n    Usage is for decomposing a covariance function S into a square root R\n    such that\n\n        R' R = S if inverse is False, or\n        R' R = pinv(S) if inverse is True\n\n    Parameters\n    ----------\n    mat : array_like, 2-d square\n        symmetric square matrix for which square root or inverse square\n        root is computed.\n        There is no checking for whether the matrix is symmetric.\n        A warning is issued if some singular values are negative, i.e.\n        below the negative of the threshold.\n    inverse : bool\n        If False (default), then the matrix square root is returned.\n        If inverse is True, then the matrix square root of the inverse\n        matrix is returned.\n    full : bool\n        If full is False (default, then the square root has reduce number\n        of rows if the matrix is singular, i.e. has singular values below\n        the threshold.\n    nullspace : bool\n        If nullspace is true, then the matrix square root of the null space\n        of the matrix is returned.\n    threshold : float\n        Singular values below the threshold are dropped.\n\n    Returns\n    -------\n    msqrt : ndarray\n        matrix square root or square root of inverse matrix.\n    \"\"\"\n    u, s, v = np.linalg.svd(mat)\n    if np.any(s < -threshold):\n        import warnings\n        warnings.warn('some singular values are negative')\n    if not nullspace:\n        mask = s > threshold\n        s[s < threshold] = 0\n    else:\n        mask = s < threshold\n        s[s > threshold] = 0\n    sqrt_s = np.sqrt(s[mask])\n    if inverse:\n        sqrt_s = 1 / np.sqrt(s[mask])\n    if full:\n        b = np.dot(u[:, mask], np.dot(np.diag(sqrt_s), v[mask]))\n    else:\n        b = np.dot(np.diag(sqrt_s), v[mask])\n    return b"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "mad", "method_path": "../srcdata/Computation/statsmodels/statsmodels/robust/scale.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm as Gaussian\nfrom statsmodels.tools import tools\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import float_like\n\n\ndef mad(a, c=Gaussian.ppf(3 / 4.0), axis=0, center=np.median): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm as Gaussian\nfrom statsmodels.tools import tools\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import float_like\ndef mad(a, c=Gaussian.ppf(3 / 4.0), axis=0, center=np.median):\n    \"\"\"\n    The Median Absolute Deviation along given axis of an array\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    c : float, optional\n        The normalization constant.  Defined as scipy.stats.norm.ppf(3/4.),\n        which is approximately 0.6745.\n    axis : int, optional\n        The default is 0. Can also be None.\n    center : callable or float\n        If a callable is provided, such as the default `np.median` then it\n        is expected to be called center(a). The axis argument will be applied\n        via np.apply_over_axes. Otherwise, provide a float.\n\n    Returns\n    -------\n    mad : float\n        `mad` = median(abs(`a` - center))/`c`\n    \"\"\"\n    a = array_like(a, 'a', ndim=None)\n    c = float_like(c, 'c')\n    if not a.size:\n        center_val = 0.0\n    elif callable(center):\n        if axis is not None:\n            center_val = np.apply_over_axes(center, a, axis)\n        else:\n            center_val = center(a.ravel())\n    else:\n        center_val = float_like(center, 'center')\n    err = np.abs(a - center_val) / c\n    if not err.size:\n        if axis is None or err.ndim == 1:\n            return np.nan\n        else:\n            shape = list(err.shape)\n            shape.pop(axis)\n            return np.empty(shape)\n    return np.median(err, axis=axis)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "bootstrap", "method_path": "../srcdata/Computation/statsmodels/statsmodels/sandbox/distributions/gof_new.py", "method_code_mask": "from statsmodels.compat.python import lmap\nimport numpy as np\nfrom scipy.stats import distributions\nfrom statsmodels.tools.decorators import cache_readonly\nfrom scipy.special import kolmogorov as ksprob\nfrom collections import defaultdict\nfrom scipy import stats\n\n\ndef bootstrap(distr, args=(), nobs=200, nrep=100, value=None, batch_size=None\n    ): [MASK]\n", "response": "from statsmodels.compat.python import lmap\nimport numpy as np\nfrom scipy.stats import distributions\nfrom statsmodels.tools.decorators import cache_readonly\nfrom scipy.special import kolmogorov as ksprob\nfrom collections import defaultdict\nfrom scipy import stats\ndef bootstrap(distr, args=(), nobs=200, nrep=100, value=None, batch_size=None):\n    \"\"\"Monte Carlo (or parametric bootstrap) p-values for gof\n\n    currently hardcoded for A^2 only\n\n    assumes vectorized fit_vec method,\n    builds and analyses (nobs, nrep) sample in one step\n\n    rename function to less generic\n\n    this works also with nrep=1\n\n    \"\"\"\n    if batch_size is not None:\n        if value is None:\n            raise ValueError('using batching requires a value')\n        n_batch = int(np.ceil(nrep / float(batch_size)))\n        count = 0\n        for irep in range(n_batch):\n            rvs = distr.rvs(args, **{'size': (batch_size, nobs)})\n            params = distr.fit_vec(rvs, axis=1)\n            params = lmap(lambda x: np.expand_dims(x, 1), params)\n            cdfvals = np.sort(distr.cdf(rvs, params), axis=1)\n            stat = asquare(cdfvals, axis=1)\n            count += (stat >= value).sum()\n        return count / float(n_batch * batch_size)\n    else:\n        rvs = distr.rvs(args, **{'size': (nrep, nobs)})\n        params = distr.fit_vec(rvs, axis=1)\n        params = lmap(lambda x: np.expand_dims(x, 1), params)\n        cdfvals = np.sort(distr.cdf(rvs, params), axis=1)\n        stat = asquare(cdfvals, axis=1)\n        if value is None:\n            stat_sorted = np.sort(stat)\n            return stat_sorted\n        else:\n            return (stat >= value).mean()"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_recode", "method_path": "../srcdata/Computation/statsmodels/statsmodels/graphics/factorplots.py", "method_code_mask": "from statsmodels.compat.python import lrange\nimport numpy as np\nfrom statsmodels.graphics.plottools import rainbow\nimport statsmodels.graphics.utils as utils\nfrom pandas import DataFrame\nfrom pandas import Series\n\n\ndef _recode(x, levels): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nimport numpy as np\nfrom statsmodels.graphics.plottools import rainbow\nimport statsmodels.graphics.utils as utils\nfrom pandas import DataFrame\nfrom pandas import Series\ndef _recode(x, levels):\n    \"\"\" Recode categorial data to int factor.\n\n    Parameters\n    ----------\n    x : array_like\n        array like object supporting with numpy array methods of categorially\n        coded data.\n    levels : dict\n        mapping of labels to integer-codings\n\n    Returns\n    -------\n    out : instance numpy.ndarray\n    \"\"\"\n    from pandas import Series\n    name = None\n    index = None\n    if isinstance(x, Series):\n        name = x.name\n        index = x.index\n        x = x.values\n    if x.dtype.type not in [np.str_, np.object_]:\n        raise ValueError(\n            'This is not a categorial factor. Array of str type required.')\n    elif not isinstance(levels, dict):\n        raise ValueError('This is not a valid value for levels. Dict required.'\n            )\n    elif not (np.unique(x) == np.unique(list(levels.keys()))).all():\n        raise ValueError('The levels do not match the array values.')\n    else:\n        out = np.empty(x.shape[0], dtype=int)\n        for level, coding in levels.items():\n            out[x == level] = coding\n        if name:\n            out = Series(out, name=name, index=index)\n        return out"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_debyem1_expansion", "method_path": "../srcdata/Computation/statsmodels/statsmodels/distributions/copula/archimedean.py", "method_code_mask": "import sys\nimport numpy as np\nfrom scipy import stats\nfrom scipy import integrate\nfrom scipy import optimize\nfrom statsmodels.tools.rng_qrng import check_random_state\nimport warnings\n\n\ndef _debyem1_expansion(x): [MASK]\n", "response": "import sys\nimport numpy as np\nfrom scipy import stats\nfrom scipy import integrate\nfrom scipy import optimize\nfrom statsmodels.tools.rng_qrng import check_random_state\nimport warnings\ndef _debyem1_expansion(x):\n    \"\"\"Debye function minus 1, Taylor series approximation around zero\n\n    function is not used\n    \"\"\"\n    x = np.asarray(x)\n    dm1 = (-x / 4 + x ** 2 / 36 - x ** 4 / 3600 + x ** 6 / 211680 - x ** 8 /\n        10886400 + x ** 10 / 526901760 - x ** 12 * 691 / 16999766784000)\n    return dm1"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "monotone_fn_inverter", "method_path": "../srcdata/Computation/statsmodels/statsmodels/distributions/empirical_distribution.py", "method_code_mask": "import numpy as np\nfrom scipy.interpolate import interp1d\n\n\ndef monotone_fn_inverter(fn, x, vectorized=True, **keywords): [MASK]\n", "response": "import numpy as np\nfrom scipy.interpolate import interp1d\ndef monotone_fn_inverter(fn, x, vectorized=True, **keywords):\n    \"\"\"\n    Given a monotone function fn (no checking is done to verify monotonicity)\n    and a set of x values, return an linearly interpolated approximation\n    to its inverse from its values on x.\n    \"\"\"\n    x = np.asarray(x)\n    if vectorized:\n        y = fn(x, **keywords)\n    else:\n        y = []\n        for _x in x:\n            y.append(fn(_x, **keywords))\n        y = np.array(y)\n    a = np.argsort(y)\n    return interp1d(y[a], x[a])"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "prob2cdf_grid", "method_path": "../srcdata/Computation/statsmodels/statsmodels/distributions/tools.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n\n\ndef prob2cdf_grid(probs): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\ndef prob2cdf_grid(probs):\n    \"\"\"Cumulative probabilities from cell provabilites on a grid\n\n    Parameters\n    ----------\n    probs : array_like\n        Rectangular grid of cell probabilities.\n\n    Returns\n    -------\n    cdf : ndarray\n        Grid of cumulative probabilities with same shape as probs.\n    \"\"\"\n    cdf = np.asarray(probs).copy()\n    k = cdf.ndim\n    for i in range(k):\n        cdf = cdf.cumsum(axis=i)\n    return cdf"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "multivariate_sample_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/gam/tests/test_gam.py", "method_code_mask": "import os\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport pandas as pd\nfrom scipy.linalg import block_diag\nimport pytest\nfrom statsmodels.tools.linalg import matrix_sqrt\nfrom statsmodels.gam.smooth_basis import UnivariatePolynomialSmoother\nfrom statsmodels.gam.smooth_basis import PolynomialSmoother\nfrom statsmodels.gam.smooth_basis import BSplines\nfrom statsmodels.gam.smooth_basis import GenericSmoothers\nfrom statsmodels.gam.smooth_basis import UnivariateCubicSplines\nfrom statsmodels.gam.smooth_basis import CyclicCubicSplines\nfrom statsmodels.gam.generalized_additive_model import GLMGam\nfrom statsmodels.gam.generalized_additive_model import LogitGam\nfrom statsmodels.gam.generalized_additive_model import make_augmented_matrix\nfrom statsmodels.gam.generalized_additive_model import penalized_wls\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCV\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCVPath\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import _split_train_test_smoothers\nfrom statsmodels.gam.gam_penalties import UnivariateGamPenalty\nfrom statsmodels.gam.gam_penalties import MultivariateGamPenalty\nfrom statsmodels.gam.gam_cross_validation.cross_validators import KFold\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.families.family import Gaussian\nfrom statsmodels.genmod.generalized_linear_model import lm\nfrom statsmodels.gam.smooth_basis import CubicSplines\n\n\ndef multivariate_sample_data(seed=1): [MASK]\n", "response": "import os\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport pandas as pd\nfrom scipy.linalg import block_diag\nimport pytest\nfrom statsmodels.tools.linalg import matrix_sqrt\nfrom statsmodels.gam.smooth_basis import UnivariatePolynomialSmoother\nfrom statsmodels.gam.smooth_basis import PolynomialSmoother\nfrom statsmodels.gam.smooth_basis import BSplines\nfrom statsmodels.gam.smooth_basis import GenericSmoothers\nfrom statsmodels.gam.smooth_basis import UnivariateCubicSplines\nfrom statsmodels.gam.smooth_basis import CyclicCubicSplines\nfrom statsmodels.gam.generalized_additive_model import GLMGam\nfrom statsmodels.gam.generalized_additive_model import LogitGam\nfrom statsmodels.gam.generalized_additive_model import make_augmented_matrix\nfrom statsmodels.gam.generalized_additive_model import penalized_wls\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCV\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import MultivariateGAMCVPath\nfrom statsmodels.gam.gam_cross_validation.gam_cross_validation import _split_train_test_smoothers\nfrom statsmodels.gam.gam_penalties import UnivariateGamPenalty\nfrom statsmodels.gam.gam_penalties import MultivariateGamPenalty\nfrom statsmodels.gam.gam_cross_validation.cross_validators import KFold\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.families.family import Gaussian\nfrom statsmodels.genmod.generalized_linear_model import lm\nfrom statsmodels.gam.smooth_basis import CubicSplines\ndef multivariate_sample_data(seed=1):\n    n = 1000\n    x1 = np.linspace(-1, 1, n)\n    x2 = np.linspace(-10, 10, n)\n    x = np.vstack([x1, x2]).T\n    np.random.seed(seed)\n    y = x1 * x1 * x1 + x2 + np.random.normal(0, 0.01, n)\n    degree1 = 4\n    degree2 = 3\n    degrees = [degree1, degree2]\n    pol = PolynomialSmoother(x, degrees)\n    return x, y, pol"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "seasonal_plot", "method_path": "../srcdata/Computation/statsmodels/statsmodels/graphics/tsaplots.py", "method_code_mask": "from statsmodels.compat.pandas import deprecate_kwarg\nimport calendar\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.stattools import ccf\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.data import _check_period_index\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\nfrom statsmodels.tsa.base.prediction import PredictionResults\n\n\ndef seasonal_plot(grouped_x, xticklabels, ylabel=None, ax=None): [MASK]\n", "response": "from statsmodels.compat.pandas import deprecate_kwarg\nimport calendar\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.stattools import ccf\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.data import _check_period_index\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\nfrom statsmodels.tsa.base.prediction import PredictionResults\ndef seasonal_plot(grouped_x, xticklabels, ylabel=None, ax=None):\n    \"\"\"\n    Consider using one of month_plot or quarter_plot unless you need\n    irregular plotting.\n\n    Parameters\n    ----------\n    grouped_x : iterable of DataFrames\n        Should be a GroupBy object (or similar pair of group_names and groups\n        as DataFrames) with a DatetimeIndex or PeriodIndex\n    xticklabels : list of str\n        List of season labels, one for each group.\n    ylabel : str\n        Lable for y axis\n    ax : AxesSubplot, optional\n        If given, this subplot is used to plot in instead of a new figure being\n        created.\n    \"\"\"\n    fig, ax = utils.create_mpl_ax(ax)\n    start = 0\n    ticks = []\n    for season, df in grouped_x:\n        df = df.copy()\n        df.sort_index()\n        nobs = len(df)\n        x_plot = np.arange(start, start + nobs)\n        ticks.append(x_plot.mean())\n        ax.plot(x_plot, df.values, 'k')\n        ax.hlines(df.values.mean(), x_plot[0], x_plot[-1], colors='r',\n            linewidth=3)\n        start += nobs\n    ax.set_xticks(ticks)\n    ax.set_xticklabels(xticklabels)\n    ax.set_ylabel(ylabel)\n    ax.margins(0.1, 0.05)\n    return fig"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "cdf2prob_grid", "method_path": "../srcdata/Computation/statsmodels/statsmodels/distributions/tools.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n\n\ndef cdf2prob_grid(cdf, prepend=0): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\ndef cdf2prob_grid(cdf, prepend=0):\n    \"\"\"Cell probabilities from cumulative probabilities on a grid.\n\n    Parameters\n    ----------\n    cdf : array_like\n        Grid of cumulative probabilities with same shape as probs.\n\n    Returns\n    -------\n    probs : ndarray\n        Rectangular grid of cell probabilities.\n\n    \"\"\"\n    if prepend is None:\n        prepend = np._NoValue\n    prob = np.asarray(cdf).copy()\n    k = prob.ndim\n    for i in range(k):\n        prob = np.diff(prob, prepend=prepend, axis=i)\n    return prob"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "plot_pacf", "method_path": "../srcdata/Computation/statsmodels/statsmodels/graphics/tsaplots.py", "method_code_mask": "from statsmodels.compat.pandas import deprecate_kwarg\nimport calendar\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.stattools import ccf\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.data import _check_period_index\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\nfrom statsmodels.tsa.base.prediction import PredictionResults\n\n\ndef plot_pacf(x, ax=None, lags=None, alpha=0.05, method='ywm', use_vlines=\n    True, title='Partial Autocorrelation', zero=True, vlines_kwargs=None,\n    **kwargs): [MASK]\n", "response": "from statsmodels.compat.pandas import deprecate_kwarg\nimport calendar\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.stattools import ccf\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.data import _check_period_index\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\nfrom statsmodels.tsa.base.prediction import PredictionResults\ndef plot_pacf(x, ax=None, lags=None, alpha=0.05, method='ywm', use_vlines=\n    True, title='Partial Autocorrelation', zero=True, vlines_kwargs=None,\n    **kwargs):\n    \"\"\"\n    Plot the partial autocorrelation function\n\n    Parameters\n    ----------\n    x : array_like\n        Array of time-series values\n    ax : AxesSubplot, optional\n        If given, this subplot is used to plot in instead of a new figure being\n        created.\n    lags : {int, array_like}, optional\n        An int or array of lag values, used on horizontal axis. Uses\n        np.arange(lags) when lags is an int.  If not provided,\n        ``lags=np.arange(len(corr))`` is used.\n    alpha : float, optional\n        If a number is given, the confidence intervals for the given level are\n        returned. For instance if alpha=.05, 95 % confidence intervals are\n        returned where the standard deviation is computed according to\n        1/sqrt(len(x))\n    method : str\n        Specifies which method for the calculations to use:\n\n        - \"ywm\" or \"ywmle\" : Yule-Walker without adjustment. Default.\n        - \"yw\" or \"ywadjusted\" : Yule-Walker with sample-size adjustment in\n          denominator for acovf. Default.\n        - \"ols\" : regression of time series on lags of it and on constant.\n        - \"ols-inefficient\" : regression of time series on lags using a single\n          common sample to estimate all pacf coefficients.\n        - \"ols-adjusted\" : regression of time series on lags with a bias\n          adjustment.\n        - \"ld\" or \"ldadjusted\" : Levinson-Durbin recursion with bias\n          correction.\n        - \"ldb\" or \"ldbiased\" : Levinson-Durbin recursion without bias\n          correction.\n\n    use_vlines : bool, optional\n        If True, vertical lines and markers are plotted.\n        If False, only markers are plotted.  The default marker is 'o'; it can\n        be overridden with a ``marker`` kwarg.\n    title : str, optional\n        Title to place on plot.  Default is 'Partial Autocorrelation'\n    zero : bool, optional\n        Flag indicating whether to include the 0-lag autocorrelation.\n        Default is True.\n    vlines_kwargs : dict, optional\n        Optional dictionary of keyword arguments that are passed to vlines.\n    **kwargs : kwargs, optional\n        Optional keyword arguments that are directly passed on to the\n        Matplotlib ``plot`` and ``axhline`` functions.\n\n    Returns\n    -------\n    Figure\n        If `ax` is None, the created figure.  Otherwise the figure to which\n        `ax` is connected.\n\n    See Also\n    --------\n    matplotlib.pyplot.xcorr\n    matplotlib.pyplot.acorr\n\n    Notes\n    -----\n    Plots lags on the horizontal and the correlations on vertical axis.\n    Adapted from matplotlib's `xcorr`.\n\n    Data are plotted as ``plot(lags, corr, **kwargs)``\n\n    kwargs is used to pass matplotlib optional arguments to both the line\n    tracing the autocorrelations and for the horizontal line at 0. These\n    options must be valid for a Line2D object.\n\n    vlines_kwargs is used to pass additional optional arguments to the\n    vertical lines connecting each autocorrelation to the axis.  These options\n    must be valid for a LineCollection object.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> import matplotlib.pyplot as plt\n    >>> import statsmodels.api as sm\n\n    >>> dta = sm.datasets.sunspots.load_pandas().data\n    >>> dta.index = pd.Index(sm.tsa.datetools.dates_from_range('1700', '2008'))\n    >>> del dta[\"YEAR\"]\n    >>> sm.graphics.tsa.plot_pacf(dta.values.squeeze(), lags=40, method=\"ywm\")\n    >>> plt.show()\n\n    .. plot:: plots/graphics_tsa_plot_pacf.py\n    \"\"\"\n    fig, ax = utils.create_mpl_ax(ax)\n    vlines_kwargs = {} if vlines_kwargs is None else vlines_kwargs\n    lags, nlags, irregular = _prepare_data_corr_plot(x, lags, zero)\n    confint = None\n    if alpha is None:\n        acf_x = pacf(x, nlags=nlags, alpha=alpha, method=method)\n    else:\n        acf_x, confint = pacf(x, nlags=nlags, alpha=alpha, method=method)\n    _plot_corr(ax, title, acf_x, confint, lags, irregular, use_vlines,\n        vlines_kwargs, **kwargs)\n    return fig"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "average_grid", "method_path": "../srcdata/Computation/statsmodels/statsmodels/distributions/tools.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n\n\ndef average_grid(values, coords=None, _method='slicing'): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\ndef average_grid(values, coords=None, _method='slicing'):\n    \"\"\"Compute average for each cell in grid using endpoints\n\n    Parameters\n    ----------\n    values : array_like\n        Values on a grid that will average over corner points of each cell.\n    coords : None or list of array_like\n        Grid coordinates for each axis use to compute volumne of cell.\n        If None, then averaged values are not rescaled.\n    _method : {\"slicing\", \"convolve\"}\n        Grid averaging is implemented using numpy \"slicing\" or using\n        scipy.signal \"convolve\".\n\n    Returns\n    -------\n    Grid with averaged cell values.\n    \"\"\"\n    k_dim = values.ndim\n    if _method == 'slicing':\n        p = values.copy()\n        for d in range(k_dim):\n            sl1 = [slice(None, None, None)] * k_dim\n            sl2 = [slice(None, None, None)] * k_dim\n            sl1[d] = slice(None, -1, None)\n            sl2[d] = slice(1, None, None)\n            sl1 = tuple(sl1)\n            sl2 = tuple(sl2)\n            p = (p[sl1] + p[sl2]) / 2\n    elif _method == 'convolve':\n        from scipy import signal\n        p = signal.convolve(values, 0.5 ** k_dim * np.ones([2] * k_dim),\n            mode='valid')\n    if coords is not None:\n        dx = np.array(1)\n        for d in range(k_dim):\n            dx = dx[..., None] * np.diff(coords[d])\n        p = p * dx\n    return p"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_make_var_names", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/x13.py", "method_code_mask": "from statsmodels.compat.pandas import deprecate_kwarg\nimport os\nimport subprocess\nimport tempfile\nimport re\nfrom warnings import warn\nimport pandas as pd\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import X13NotFoundError\nfrom statsmodels.tools.sm_exceptions import IOWarning\nfrom statsmodels.tools.sm_exceptions import X13Error\nfrom statsmodels.tools.sm_exceptions import X13Warning\nfrom statsmodels.base.data import _make_exog_names\nfrom io import StringIO\nfrom pandas import read_csv\nfrom pandas.tseries.api import infer_freq\nfrom statsmodels.graphics.utils import _import_mpl\n\n\ndef _make_var_names(exog): [MASK]\n", "response": "from statsmodels.compat.pandas import deprecate_kwarg\nimport os\nimport subprocess\nimport tempfile\nimport re\nfrom warnings import warn\nimport pandas as pd\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.sm_exceptions import X13NotFoundError\nfrom statsmodels.tools.sm_exceptions import IOWarning\nfrom statsmodels.tools.sm_exceptions import X13Error\nfrom statsmodels.tools.sm_exceptions import X13Warning\nfrom statsmodels.base.data import _make_exog_names\nfrom io import StringIO\nfrom pandas import read_csv\nfrom pandas.tseries.api import infer_freq\nfrom statsmodels.graphics.utils import _import_mpl\ndef _make_var_names(exog):\n    if hasattr(exog, 'name'):\n        var_names = [exog.name]\n    elif hasattr(exog, 'columns'):\n        var_names = exog.columns\n    else:\n        raise ValueError('exog is not a Series or DataFrame or is unnamed.')\n    try:\n        var_names = ' '.join(var_names)\n    except TypeError:\n        from statsmodels.base.data import _make_exog_names\n        if exog.ndim == 1:\n            var_names = 'x1'\n        else:\n            var_names = ' '.join(_make_exog_names(exog))\n    return var_names"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_hierarchical_split", "method_path": "../srcdata/Computation/statsmodels/statsmodels/graphics/mosaicplot.py", "method_code_mask": "from statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nfrom itertools import product\nimport numpy as np\nfrom numpy import array\nfrom numpy import cumsum\nfrom numpy import iterable\nfrom numpy import r_\nfrom pandas import DataFrame\nfrom statsmodels.graphics import utils\nfrom matplotlib.colors import hsv_to_rgb\nfrom matplotlib.patches import Rectangle\n\n\ndef _hierarchical_split(count_dict, horizontal=True, gap=0.05): [MASK]\n", "response": "from statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nfrom itertools import product\nimport numpy as np\nfrom numpy import array\nfrom numpy import cumsum\nfrom numpy import iterable\nfrom numpy import r_\nfrom pandas import DataFrame\nfrom statsmodels.graphics import utils\nfrom matplotlib.colors import hsv_to_rgb\nfrom matplotlib.patches import Rectangle\ndef _hierarchical_split(count_dict, horizontal=True, gap=0.05):\n    \"\"\"\n    Split a square in a hierarchical way given a contingency table.\n\n    Hierarchically split the unit square in alternate directions\n    in proportion to the subdivision contained in the contingency table\n    count_dict.  This is the function that actually perform the tiling\n    for the creation of the mosaic plot.  If the gap array has been specified\n    it will insert a corresponding amount of space (proportional to the\n    unit length), while retaining the proportionality of the tiles.\n\n    Parameters\n    ----------\n    count_dict : dict\n        Dictionary containing the contingency table.\n        Each category should contain a non-negative number\n        with a tuple as index.  It expects that all the combination\n        of keys to be represents; if that is not true, will\n        automatically consider the missing values as 0\n    horizontal : bool\n        The starting direction of the split (by default along\n        the horizontal axis)\n    gap : float or array of floats\n        The list of gaps to be applied on each subdivision.\n        If the length of the given array is less of the number\n        of subcategories (or if it's a single number) it will extend\n        it with exponentially decreasing gaps\n\n    Returns\n    -------\n    base_rect : dict\n        A dictionary containing the result of the split.\n        To each key is associated a 4-tuple of coordinates\n        that are required to create the corresponding rectangle:\n\n            0 - x position of the lower left corner\n            1 - y position of the lower left corner\n            2 - width of the rectangle\n            3 - height of the rectangle\n    \"\"\"\n    base_rect = dict([(tuple(), (0, 0, 1, 1))])\n    categories_levels = _categories_level(list(count_dict.keys()))\n    L = len(categories_levels)\n    if not np.iterable(gap):\n        gap = [(gap / 1.5 ** idx) for idx in range(L)]\n    if len(gap) < L:\n        last = gap[-1]\n        gap = list(*gap) + [(last / 1.5 ** idx) for idx in range(L)]\n    gap = gap[:L]\n    count_ordered = {k: count_dict[k] for k in list(product(*\n        categories_levels))}\n    for cat_idx, cat_enum in enumerate(categories_levels):\n        base_keys = list(product(*categories_levels[:cat_idx]))\n        for key in base_keys:\n            part_count = [_reduce_dict(count_ordered, key + (partial,)) for\n                partial in cat_enum]\n            new_gap = gap[cat_idx]\n            base_rect = _key_splitting(base_rect, cat_enum, part_count, key,\n                horizontal, new_gap)\n        horizontal = not horizontal\n    return base_rect"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_eval_bernstein_1d", "method_path": "../srcdata/Computation/statsmodels/statsmodels/distributions/tools.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n\n\ndef _eval_bernstein_1d(x, fvals, method='binom'): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy import stats\nfrom scipy import signal\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\ndef _eval_bernstein_1d(x, fvals, method='binom'):\n    \"\"\"Evaluate 1-dimensional bernstein polynomial given grid of values.\n\n    experimental, comparing methods\n\n    Parameters\n    ----------\n    x : array_like\n        Values at which to evaluate the Bernstein polynomial.\n    fvals : ndarray\n        Grid values of coefficients for Bernstein polynomial basis in the\n        weighted sum.\n    method: \"binom\", \"beta\" or \"bpoly\"\n        Method to construct Bernstein polynomial basis, used for comparison\n        of parameterizations.\n\n        - \"binom\" uses pmf of Binomial distribution\n        - \"beta\" uses pdf of Beta distribution\n        - \"bpoly\" uses one interval in scipy.interpolate.BPoly\n\n    Returns\n    -------\n    Bernstein polynomial at evaluation points, weighted sum of Bernstein\n    polynomial basis.\n    \"\"\"\n    k_terms = fvals.shape[-1]\n    xx = np.asarray(x)\n    k = np.arange(k_terms).astype(float)\n    n = k_terms - 1.0\n    if method.lower() == 'binom':\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', RuntimeWarning)\n            poly_base = stats.binom.pmf(k, n, xx[..., None])\n        bp_values = (fvals * poly_base).sum(-1)\n    elif method.lower() == 'bpoly':\n        bpb = interpolate.BPoly(fvals[:, None], [0.0, 1])\n        bp_values = bpb(x)\n    elif method.lower() == 'beta':\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', RuntimeWarning)\n            poly_base = stats.beta.pdf(xx[..., None], k + 1, n - k + 1) / (n +\n                1)\n        bp_values = (fvals * poly_base).sum(-1)\n    else:\n        raise ValueError('method not recogized')\n    return bp_values"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_compute_multivariate_sample_acovf", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/statespace/tools.py", "method_code_mask": "import numpy as np\nfrom scipy.linalg import solve_sylvester\nimport pandas as pd\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.tools.data import _is_using_pandas\nfrom scipy.linalg.blas import find_best_blas_type\nfrom scipy import linalg\n\n\ndef _compute_multivariate_sample_acovf(endog, maxlag): [MASK]\n", "response": "import numpy as np\nfrom scipy.linalg import solve_sylvester\nimport pandas as pd\nfrom statsmodels.compat.pandas import Appender\nfrom statsmodels.tools.data import _is_using_pandas\nfrom scipy.linalg.blas import find_best_blas_type\nfrom scipy import linalg\ndef _compute_multivariate_sample_acovf(endog, maxlag):\n    \"\"\"\n    Computer multivariate sample autocovariances\n\n    Parameters\n    ----------\n    endog : array_like\n        Sample data on which to compute sample autocovariances. Shaped\n        `nobs` x `k_endog`.\n    maxlag : int\n        Maximum lag to use when computing the sample autocovariances.\n\n    Returns\n    -------\n    sample_autocovariances : list\n        A list of the first `maxlag` sample autocovariance matrices. Each\n        matrix is shaped `k_endog` x `k_endog`.\n\n    Notes\n    -----\n    This function computes the forward sample autocovariances:\n\n    .. math::\n\n        \\\\hat \\\\Gamma(s) = \\\\frac{1}{n} \\\\sum_{t=1}^{n-s}\n        (Z_t - \\\\bar Z) (Z_{t+s} - \\\\bar Z)'\n\n    See page 353 of Wei (1990). This function is primarily implemented for\n    checking the partial autocorrelation functions below, and so is quite slow.\n\n    References\n    ----------\n    .. [*] Wei, William. 1990.\n       Time Series Analysis : Univariate and Multivariate Methods. Boston:\n       Pearson.\n    \"\"\"\n    endog = np.array(endog)\n    if endog.ndim == 1:\n        endog = endog[:, np.newaxis]\n    endog -= np.mean(endog, axis=0)\n    nobs, k_endog = endog.shape\n    sample_autocovariances = []\n    for s in range(maxlag + 1):\n        sample_autocovariances.append(np.zeros((k_endog, k_endog)))\n        for t in range(nobs - s):\n            sample_autocovariances[s] += np.outer(endog[t], endog[t + s])\n        sample_autocovariances[s] /= nobs\n    return sample_autocovariances"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "banddepth", "method_path": "../srcdata/Computation/statsmodels/statsmodels/graphics/functional.py", "method_code_mask": "from statsmodels.compat.numpy import NP_LT_123\nimport numpy as np\nfrom scipy.special import comb\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.multivariate.pca import PCA\nfrom statsmodels.nonparametric.kernel_density import KDEMultivariate\nfrom scipy.optimize import brute\nfrom scipy.optimize import differential_evolution\nfrom scipy.optimize import fmin\nimport itertools\nfrom multiprocessing import Pool\nfrom matplotlib.cm import rainbow_r\n\n\ndef banddepth(data, method='MBD'): [MASK]\n", "response": "from statsmodels.compat.numpy import NP_LT_123\nimport numpy as np\nfrom scipy.special import comb\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.multivariate.pca import PCA\nfrom statsmodels.nonparametric.kernel_density import KDEMultivariate\nfrom scipy.optimize import brute\nfrom scipy.optimize import differential_evolution\nfrom scipy.optimize import fmin\nimport itertools\nfrom multiprocessing import Pool\nfrom matplotlib.cm import rainbow_r\ndef banddepth(data, method='MBD'):\n    \"\"\"\n    Calculate the band depth for a set of functional curves.\n\n    Band depth is an order statistic for functional data (see `fboxplot`), with\n    a higher band depth indicating larger \"centrality\".  In analog to scalar\n    data, the functional curve with highest band depth is called the median\n    curve, and the band made up from the first N/2 of N curves is the 50%\n    central region.\n\n    Parameters\n    ----------\n    data : ndarray\n        The vectors of functions to create a functional boxplot from.\n        The first axis is the function index, the second axis the one along\n        which the function is defined.  So ``data[0, :]`` is the first\n        functional curve.\n    method : {'MBD', 'BD2'}, optional\n        Whether to use the original band depth (with J=2) of [1]_ or the\n        modified band depth.  See Notes for details.\n\n    Returns\n    -------\n    ndarray\n        Depth values for functional curves.\n\n    Notes\n    -----\n    Functional band depth as an order statistic for functional data was\n    proposed in [1]_ and applied to functional boxplots and bagplots in [2]_.\n\n    The method 'BD2' checks for each curve whether it lies completely inside\n    bands constructed from two curves.  All permutations of two curves in the\n    set of curves are used, and the band depth is normalized to one.  Due to\n    the complete curve having to fall within the band, this method yields a lot\n    of ties.\n\n    The method 'MBD' is similar to 'BD2', but checks the fraction of the curve\n    falling within the bands.  It therefore generates very few ties.\n\n    The algorithm uses the efficient implementation proposed in [3]_.\n\n    References\n    ----------\n    .. [1] S. Lopez-Pintado and J. Romo, \"On the Concept of Depth for\n           Functional Data\", Journal of the American Statistical Association,\n           vol.  104, pp. 718-734, 2009.\n    .. [2] Y. Sun and M.G. Genton, \"Functional Boxplots\", Journal of\n           Computational and Graphical Statistics, vol. 20, pp. 1-19, 2011.\n    .. [3] Y. Sun, M. G. Gentonb and D. W. Nychkac, \"Exact fast computation\n           of band depth for large functional datasets: How quickly can one\n           million curves be ranked?\", Journal for the Rapid Dissemination\n           of Statistics Research, vol. 1, pp. 68-74, 2012.\n    \"\"\"\n    n, p = data.shape\n    rv = np.argsort(data, axis=0)\n    rmat = np.argsort(rv, axis=0) + 1\n\n    def _fbd2():\n        down = np.min(rmat, axis=1) - 1\n        up = n - np.max(rmat, axis=1)\n        return (up * down + n - 1) / comb(n, 2)\n\n    def _fmbd():\n        down = rmat - 1\n        up = n - rmat\n        return (np.sum(up * down, axis=1) / p + n - 1) / comb(n, 2)\n    if method == 'BD2':\n        depth = _fbd2()\n    elif method == 'MBD':\n        depth = _fmbd()\n    else:\n        raise ValueError('Unknown input value for parameter `method`.')\n    return depth"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "mv_mixture_rvs", "method_path": "../srcdata/Computation/statsmodels/statsmodels/distributions/mixture_rvs.py", "method_code_mask": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\ndef mv_mixture_rvs(prob, size, dist, nvars, **kwargs): [MASK]\n", "response": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef mv_mixture_rvs(prob, size, dist, nvars, **kwargs):\n    \"\"\"\n    Sample from a mixture of multivariate distributions.\n\n    Parameters\n    ----------\n    prob : array_like\n        Probability of sampling from each distribution in dist\n    size : int\n        The length of the returned sample.\n    dist : array_like\n        An iterable of distributions instances with callable method rvs.\n    nvargs : int\n        dimension of the multivariate distribution, could be inferred instead\n    kwargs : tuple of dicts, optional\n        ignored\n\n    Examples\n    --------\n    Say we want 2000 random variables from mixture of normals with two\n    multivariate normal distributions, and we want to sample from the\n    first with probability .4 and the second with probability .6.\n\n    import statsmodels.sandbox.distributions.mv_normal as mvd\n\n    cov3 = np.array([[ 1.  ,  0.5 ,  0.75],\n                       [ 0.5 ,  1.5 ,  0.6 ],\n                       [ 0.75,  0.6 ,  2.  ]])\n\n    mu = np.array([-1, 0.0, 2.0])\n    mu2 = np.array([4, 2.0, 2.0])\n    mvn3 = mvd.MVNormal(mu, cov3)\n    mvn32 = mvd.MVNormal(mu2, cov3/2., 4)\n    rvs = mix.mv_mixture_rvs([0.4, 0.6], 2000, [mvn3, mvn32], 3)\n    \"\"\"\n    if len(prob) != len(dist):\n        raise ValueError(\n            'You must provide as many probabilities as distributions')\n    if not np.allclose(np.sum(prob), 1):\n        raise ValueError('prob does not sum to 1')\n    if kwargs is None:\n        kwargs = ({},) * len(prob)\n    idx = _make_index(prob, size)\n    sample = np.empty((size, nvars))\n    for i in range(len(prob)):\n        sample_idx = idx[..., i]\n        sample_size = sample_idx.sum()\n        sample[sample_idx] = dist[i].rvs(size=int(sample_size))\n    return sample"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "plot_acf", "method_path": "../srcdata/Computation/statsmodels/statsmodels/graphics/tsaplots.py", "method_code_mask": "from statsmodels.compat.pandas import deprecate_kwarg\nimport calendar\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.stattools import ccf\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.data import _check_period_index\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\nfrom statsmodels.tsa.base.prediction import PredictionResults\n\n\n@deprecate_kwarg('unbiased', 'adjusted')\ndef plot_acf(x, ax=None, lags=None, *, alpha=0.05, use_vlines=True,\n    adjusted=False, fft=False, missing='none', title='Autocorrelation',\n    zero=True, auto_ylims=False, bartlett_confint=True, vlines_kwargs=None,\n    **kwargs): [MASK]\n", "response": "from statsmodels.compat.pandas import deprecate_kwarg\nimport calendar\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.graphics import utils\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.stattools import ccf\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.data import _check_period_index\nfrom statsmodels.graphics.utils import _import_mpl\nfrom statsmodels.graphics.utils import create_mpl_ax\nfrom statsmodels.tsa.base.prediction import PredictionResults\n@deprecate_kwarg('unbiased', 'adjusted')\ndef plot_acf(x, ax=None, lags=None, *, alpha=0.05, use_vlines=True,\n    adjusted=False, fft=False, missing='none', title='Autocorrelation',\n    zero=True, auto_ylims=False, bartlett_confint=True, vlines_kwargs=None,\n    **kwargs):\n    \"\"\"\n    Plot the autocorrelation function\n\n    Plots lags on the horizontal and the correlations on vertical axis.\n\n    Parameters\n    ----------\n    x : array_like\n        Array of time-series values\n    ax : AxesSubplot, optional\n        If given, this subplot is used to plot in instead of a new figure being\n        created.\n    lags : {int, array_like}, optional\n        An int or array of lag values, used on horizontal axis. Uses\n        np.arange(lags) when lags is an int.  If not provided,\n        ``lags=np.arange(len(corr))`` is used.\n    alpha : scalar, optional\n        If a number is given, the confidence intervals for the given level are\n        returned. For instance if alpha=.05, 95 % confidence intervals are\n        returned where the standard deviation is computed according to\n        Bartlett's formula. If None, no confidence intervals are plotted.\n    use_vlines : bool, optional\n        If True, vertical lines and markers are plotted.\n        If False, only markers are plotted.  The default marker is 'o'; it can\n        be overridden with a ``marker`` kwarg.\n    adjusted : bool\n        If True, then denominators for autocovariance are n-k, otherwise n\n    fft : bool, optional\n        If True, computes the ACF via FFT.\n    missing : str, optional\n        A string in ['none', 'raise', 'conservative', 'drop'] specifying how\n        the NaNs are to be treated.\n    title : str, optional\n        Title to place on plot.  Default is 'Autocorrelation'\n    zero : bool, optional\n        Flag indicating whether to include the 0-lag autocorrelation.\n        Default is True.\n    auto_ylims : bool, optional\n        If True, adjusts automatically the y-axis limits to ACF values.\n    bartlett_confint : bool, default True\n        Confidence intervals for ACF values are generally placed at 2\n        standard errors around r_k. The formula used for standard error\n        depends upon the situation. If the autocorrelations are being used\n        to test for randomness of residuals as part of the ARIMA routine,\n        the standard errors are determined assuming the residuals are white\n        noise. The approximate formula for any lag is that standard error\n        of each r_k = 1/sqrt(N). See section 9.4 of [1] for more details on\n        the 1/sqrt(N) result. For more elementary discussion, see section\n        5.3.2 in [2].\n        For the ACF of raw data, the standard error at a lag k is\n        found as if the right model was an MA(k-1). This allows the\n        possible interpretation that if all autocorrelations past a\n        certain lag are within the limits, the model might be an MA of\n        order defined by the last significant autocorrelation. In this\n        case, a moving average model is assumed for the data and the\n        standard errors for the confidence intervals should be\n        generated using Bartlett's formula. For more details on\n        Bartlett formula result, see section 7.2 in [1].\n    vlines_kwargs : dict, optional\n        Optional dictionary of keyword arguments that are passed to vlines.\n    **kwargs : kwargs, optional\n        Optional keyword arguments that are directly passed on to the\n        Matplotlib ``plot`` and ``axhline`` functions.\n\n    Returns\n    -------\n    Figure\n        If `ax` is None, the created figure.  Otherwise the figure to which\n        `ax` is connected.\n\n    See Also\n    --------\n    matplotlib.pyplot.xcorr\n    matplotlib.pyplot.acorr\n\n    Notes\n    -----\n    Adapted from matplotlib's `xcorr`.\n\n    Data are plotted as ``plot(lags, corr, **kwargs)``\n\n    kwargs is used to pass matplotlib optional arguments to both the line\n    tracing the autocorrelations and for the horizontal line at 0. These\n    options must be valid for a Line2D object.\n\n    vlines_kwargs is used to pass additional optional arguments to the\n    vertical lines connecting each autocorrelation to the axis.  These options\n    must be valid for a LineCollection object.\n\n    References\n    ----------\n    [1] Brockwell and Davis, 1987. Time Series Theory and Methods\n    [2] Brockwell and Davis, 2010. Introduction to Time Series and\n    Forecasting, 2nd edition.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> import matplotlib.pyplot as plt\n    >>> import statsmodels.api as sm\n\n    >>> dta = sm.datasets.sunspots.load_pandas().data\n    >>> dta.index = pd.Index(sm.tsa.datetools.dates_from_range('1700', '2008'))\n    >>> del dta[\"YEAR\"]\n    >>> sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40)\n    >>> plt.show()\n\n    .. plot:: plots/graphics_tsa_plot_acf.py\n    \"\"\"\n    fig, ax = utils.create_mpl_ax(ax)\n    lags, nlags, irregular = _prepare_data_corr_plot(x, lags, zero)\n    vlines_kwargs = {} if vlines_kwargs is None else vlines_kwargs\n    confint = None\n    acf_x = acf(x, nlags=nlags, alpha=alpha, fft=fft, bartlett_confint=\n        bartlett_confint, adjusted=adjusted, missing=missing)\n    if alpha is not None:\n        acf_x, confint = acf_x[:2]\n    _plot_corr(ax, title, acf_x, confint, lags, irregular, use_vlines,\n        vlines_kwargs, auto_ylims=auto_ylims, **kwargs)\n    return fig"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "get_sarimax_models", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/statespace/tests/test_concentrated.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.statespace import sarimax\nfrom statsmodels.tsa.statespace import varmax\nfrom numpy.testing import assert_raises\nfrom numpy.testing import assert_allclose\n\n\ndef get_sarimax_models(endog, filter_univariate=False, **kwargs): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.statespace import sarimax\nfrom statsmodels.tsa.statespace import varmax\nfrom numpy.testing import assert_raises\nfrom numpy.testing import assert_allclose\ndef get_sarimax_models(endog, filter_univariate=False, **kwargs):\n    kwargs.setdefault('tolerance', 0)\n    mod_conc = sarimax.SARIMAX(endog, **kwargs)\n    mod_conc.ssm.filter_concentrated = True\n    mod_conc.ssm.filter_univariate = filter_univariate\n    params_conc = mod_conc.start_params\n    params_conc[-1] = 1\n    res_conc = mod_conc.smooth(params_conc)\n    scale = res_conc.scale\n    mod_orig = sarimax.SARIMAX(endog, **kwargs)\n    mod_orig.ssm.filter_univariate = filter_univariate\n    params_orig = params_conc.copy()\n    k_vars = 1 + kwargs.get('measurement_error', False)\n    params_orig[-k_vars:] = scale * params_conc[-k_vars:]\n    res_orig = mod_orig.smooth(params_orig)\n    return Bunch(**{'mod_conc': mod_conc, 'params_conc': params_conc,\n        'mod_orig': mod_orig, 'params_orig': params_orig, 'res_conc':\n        res_conc, 'res_orig': res_orig, 'scale': scale})"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "qqplot_2samples", "method_path": "../srcdata/Computation/statsmodels/statsmodels/graphics/gofplots.py", "method_code_mask": "from statsmodels.compat.python import lzip\nimport numpy as np\nfrom scipy import stats\nfrom statsmodels.distributions import ECDF\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.tools import add_constant\n\n\ndef qqplot_2samples(data1, data2, xlabel=None, ylabel=None, line=None, ax=None\n    ): [MASK]\n", "response": "from statsmodels.compat.python import lzip\nimport numpy as np\nfrom scipy import stats\nfrom statsmodels.distributions import ECDF\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.tools import add_constant\ndef qqplot_2samples(data1, data2, xlabel=None, ylabel=None, line=None, ax=None\n    ):\n    \"\"\"\n    Q-Q Plot of two samples' quantiles.\n\n    Can take either two `ProbPlot` instances or two array-like objects. In the\n    case of the latter, both inputs will be converted to `ProbPlot` instances\n    using only the default values - so use `ProbPlot` instances if\n    finer-grained control of the quantile computations is required.\n\n    Parameters\n    ----------\n    data1 : {array_like, ProbPlot}\n        Data to plot along x axis. If the sample sizes are unequal, the longer\n        series is always plotted along the x-axis.\n    data2 : {array_like, ProbPlot}\n        Data to plot along y axis. Does not need to have the same number of\n        observations as data 1. If the sample sizes are unequal, the longer\n        series is always plotted along the x-axis.\n    xlabel : {None, str}\n        User-provided labels for the x-axis. If None (default),\n        other values are used.\n    ylabel : {None, str}\n        User-provided labels for the y-axis. If None (default),\n        other values are used.\n    line : {None, \"45\", \"s\", \"r\", q\"}\n        Options for the reference line to which the data is compared:\n\n        - \"45\" - 45-degree line\n        - \"s\" - standardized line, the expected order statistics are scaled\n          by the standard deviation of the given sample and have the mean\n          added to them\n        - \"r\" - A regression line is fit\n        - \"q\" - A line is fit through the quartiles.\n        - None - by default no reference line is added to the plot.\n\n    ax : AxesSubplot, optional\n        If given, this subplot is used to plot in instead of a new figure being\n        created.\n\n    Returns\n    -------\n    Figure\n        If `ax` is None, the created figure.  Otherwise the figure to which\n        `ax` is connected.\n\n    See Also\n    --------\n    scipy.stats.probplot\n\n    Notes\n    -----\n    1) Depends on matplotlib.\n    2) If `data1` and `data2` are not `ProbPlot` instances, instances will be\n       created using the default parameters. Therefore, it is recommended to use\n       `ProbPlot` instance if fine-grained control is needed in the computation\n       of the quantiles.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from statsmodels.graphics.gofplots import qqplot_2samples\n    >>> x = np.random.normal(loc=8.5, scale=2.5, size=37)\n    >>> y = np.random.normal(loc=8.0, scale=3.0, size=37)\n    >>> pp_x = sm.ProbPlot(x)\n    >>> pp_y = sm.ProbPlot(y)\n    >>> qqplot_2samples(pp_x, pp_y)\n    >>> plt.show()\n\n    .. plot:: plots/graphics_gofplots_qqplot_2samples.py\n\n    >>> fig = qqplot_2samples(pp_x, pp_y, xlabel=None, ylabel=None,\n    ...                       line=None, ax=None)\n    \"\"\"\n    if not isinstance(data1, ProbPlot):\n        data1 = ProbPlot(data1)\n    if not isinstance(data2, ProbPlot):\n        data2 = ProbPlot(data2)\n    if data2.data.shape[0] > data1.data.shape[0]:\n        fig = data1.qqplot(xlabel=ylabel, ylabel=xlabel, line=line, other=\n            data2, ax=ax)\n    else:\n        fig = data2.qqplot(xlabel=ylabel, ylabel=xlabel, line=line, other=\n            data1, ax=ax, swap=True)\n    return fig"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "model_common_level", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/statespace/tests/test_exact_diffuse_filtering.py", "method_code_mask": "from statsmodels.compat.platform import PLATFORM_WIN\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport os\nfrom statsmodels import datasets\nfrom statsmodels.tsa.statespace.initialization import Initialization\nfrom statsmodels.tsa.statespace.kalman_smoother import KalmanSmoother\nfrom statsmodels.tsa.statespace.varmax import VARMAX\nfrom statsmodels.tsa.statespace.dynamic_factor import DynamicFactor\nfrom statsmodels.tsa.statespace.structural import UnobservedComponents\nfrom statsmodels.tsa.statespace.tests.test_impulse_responses import TVSS\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_allclose\n\n\ndef model_common_level(endog=None, params=None, restricted=False): [MASK]\n", "response": "from statsmodels.compat.platform import PLATFORM_WIN\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport os\nfrom statsmodels import datasets\nfrom statsmodels.tsa.statespace.initialization import Initialization\nfrom statsmodels.tsa.statespace.kalman_smoother import KalmanSmoother\nfrom statsmodels.tsa.statespace.varmax import VARMAX\nfrom statsmodels.tsa.statespace.dynamic_factor import DynamicFactor\nfrom statsmodels.tsa.statespace.structural import UnobservedComponents\nfrom statsmodels.tsa.statespace.tests.test_impulse_responses import TVSS\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_allclose\ndef model_common_level(endog=None, params=None, restricted=False):\n    if endog is None:\n        y11 = 10.2394\n        y21 = 8.2304\n        endog = np.column_stack([np.r_[y11, [1] * 9], np.r_[y21, [1] * 9]])\n    if params is None:\n        params = [0.1111, 3.2324]\n    theta, sigma2_mu = params\n    if not restricted:\n        ssm = KalmanSmoother(k_endog=2, k_states=2, k_posdef=1)\n        ssm.bind(endog.T)\n        init = Initialization(ssm.k_states, initialization_type='diffuse')\n        ssm.initialize(init)\n        ssm['design'] = np.array([[1, 0], [theta, 1]])\n        ssm['obs_cov'] = np.eye(2)\n        ssm['transition'] = np.eye(2)\n        ssm['selection', 0, 0] = 1\n        ssm['state_cov', 0, 0] = sigma2_mu\n    else:\n        ssm = KalmanSmoother(k_endog=2, k_states=1, k_posdef=1)\n        ssm.bind(endog.T)\n        init = Initialization(ssm.k_states, initialization_type='diffuse')\n        ssm.initialize(init)\n        ssm['design'] = np.array([[1, theta]]).T\n        ssm['obs_cov'] = np.eye(2)\n        ssm['transition', :] = 1\n        ssm['selection', :] = 1\n        ssm['state_cov', :] = sigma2_mu\n    return ssm"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "validate_basic", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima/tools.py", "method_code_mask": "import numpy as np\n\n\ndef validate_basic(params, length, allow_infnan=False, title=None): [MASK]\n", "response": "import numpy as np\ndef validate_basic(params, length, allow_infnan=False, title=None):\n    \"\"\"\n    Validate parameter vector for basic correctness.\n\n    Parameters\n    ----------\n    params : array_like\n        Array of parameters to validate.\n    length : int\n        Expected length of the parameter vector.\n    allow_infnan : bool, optional\n            Whether or not to allow `params` to contain -np.inf, np.inf, and\n            np.nan. Default is False.\n    title : str, optional\n        Description of the parameters (e.g. \"autoregressive\") to use in error\n        messages.\n\n    Returns\n    -------\n    params : ndarray\n        Array of validated parameters.\n\n    Notes\n    -----\n    Basic check that the parameters are numeric and that they are the right\n    shape. Optionally checks for NaN / infinite values.\n    \"\"\"\n    title = '' if title is None else ' for %s' % title\n    try:\n        params = np.array(params, dtype=object)\n        is_complex = [isinstance(p, complex) for p in params.ravel()]\n        dtype = complex if any(is_complex) else float\n        params = np.array(params, dtype=dtype)\n    except TypeError:\n        raise ValueError('Parameters vector%s includes invalid values.' % title\n            )\n    if not allow_infnan and (np.any(np.isnan(params)) or np.any(np.isinf(\n        params))):\n        raise ValueError('Parameters vector%s includes NaN or Inf values.' %\n            title)\n    params = np.atleast_1d(np.squeeze(params))\n    if params.shape != (length,):\n        plural = '' if length == 1 else 's'\n        raise ValueError(\n            'Specification%s implies %d parameter%s, but values with shape %s were provided.'\n             % (title, length, plural, params.shape))\n    return params"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "dates_from_range", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/base/datetools.py", "method_code_mask": "from statsmodels.compat.python import asstr\nfrom statsmodels.compat.python import lmap\nfrom statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport datetime\nimport re\nimport numpy as np\nfrom pandas import to_datetime\n\n\ndef dates_from_range(start, end=None, length=None): [MASK]\n", "response": "from statsmodels.compat.python import asstr\nfrom statsmodels.compat.python import lmap\nfrom statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport datetime\nimport re\nimport numpy as np\nfrom pandas import to_datetime\ndef dates_from_range(start, end=None, length=None):\n    \"\"\"\n    Turns a sequence of date strings and returns a list of datetime.\n\n    Parameters\n    ----------\n    start : str\n        The first abbreviated date, for instance, '1965q1' or '1965m1'\n    end : str, optional\n        The last abbreviated date if length is None.\n    length : int, optional\n        The length of the returned array of end is None.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> import pandas as pd\n    >>> nobs = 50\n    >>> dates = pd.date_range('1960m1', length=nobs)\n\n\n    Returns\n    -------\n    date_list : ndarray\n        A list of datetime types.\n    \"\"\"\n    dates = date_range_str(start, end, length)\n    return dates_from_str(dates)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "statespace", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima/estimators/statespace.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.arima.params import SARIMAXParams\n\n\ndef statespace(endog, exog=None, order=(0, 0, 0), seasonal_order=(0, 0, 0, \n    0), include_constant=True, enforce_stationarity=True,\n    enforce_invertibility=True, concentrate_scale=False, start_params=None,\n    fit_kwargs=None): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.arima.params import SARIMAXParams\ndef statespace(endog, exog=None, order=(0, 0, 0), seasonal_order=(0, 0, 0, \n    0), include_constant=True, enforce_stationarity=True,\n    enforce_invertibility=True, concentrate_scale=False, start_params=None,\n    fit_kwargs=None):\n    \"\"\"\n    Estimate SARIMAX parameters using state space methods.\n\n    Parameters\n    ----------\n    endog : array_like\n        Input time series array.\n    order : tuple, optional\n        The (p,d,q) order of the model for the number of AR parameters,\n        differences, and MA parameters. Default is (0, 0, 0).\n    seasonal_order : tuple, optional\n        The (P,D,Q,s) order of the seasonal component of the model for the\n        AR parameters, differences, MA parameters, and periodicity. Default\n        is (0, 0, 0, 0).\n    include_constant : bool, optional\n        Whether to add a constant term in `exog` if it's not already there.\n        The estimate of the constant will then appear as one of the `exog`\n        parameters. If `exog` is None, then the constant will represent the\n        mean of the process.\n    enforce_stationarity : bool, optional\n        Whether or not to transform the AR parameters to enforce stationarity\n        in the autoregressive component of the model. Default is True.\n    enforce_invertibility : bool, optional\n        Whether or not to transform the MA parameters to enforce invertibility\n        in the moving average component of the model. Default is True.\n    concentrate_scale : bool, optional\n        Whether or not to concentrate the scale (variance of the error term)\n        out of the likelihood. This reduces the number of parameters estimated\n        by maximum likelihood by one.\n    start_params : array_like, optional\n        Initial guess of the solution for the loglikelihood maximization. The\n        AR polynomial must be stationary. If `enforce_invertibility=True` the\n        MA poylnomial must be invertible. If not provided, default starting\n        parameters are computed using the Hannan-Rissanen method.\n    fit_kwargs : dict, optional\n        Arguments to pass to the state space model's `fit` method.\n\n    Returns\n    -------\n    parameters : SARIMAXParams object\n    other_results : Bunch\n        Includes two components, `spec`, containing the `SARIMAXSpecification`\n        instance corresponding to the input arguments; and\n        `state_space_results`, corresponding to the results from the underlying\n        state space model and Kalman filter / smoother.\n\n    Notes\n    -----\n    The primary reference is [1]_.\n\n    References\n    ----------\n    .. [1] Durbin, James, and Siem Jan Koopman. 2012.\n       Time Series Analysis by State Space Methods: Second Edition.\n       Oxford University Press.\n    \"\"\"\n    if include_constant:\n        exog = np.ones_like(endog) if exog is None else add_constant(exog)\n    spec = SARIMAXSpecification(endog, exog=exog, order=order,\n        seasonal_order=seasonal_order, enforce_stationarity=\n        enforce_stationarity, enforce_invertibility=enforce_invertibility,\n        concentrate_scale=concentrate_scale)\n    endog = spec.endog\n    exog = spec.exog\n    p = SARIMAXParams(spec=spec)\n    if start_params is not None:\n        sp = SARIMAXParams(spec=spec)\n        sp.params = start_params\n        if spec.enforce_stationarity and not sp.is_stationary:\n            raise ValueError(\n                'Given starting parameters imply a non-stationary AR process with `enforce_stationarity=True`.'\n                )\n        if spec.enforce_invertibility and not sp.is_invertible:\n            raise ValueError(\n                'Given starting parameters imply a non-invertible MA process with `enforce_invertibility=True`.'\n                )\n    mod = SARIMAX(endog, exog=exog, order=spec.order, seasonal_order=spec.\n        seasonal_order, enforce_stationarity=spec.enforce_stationarity,\n        enforce_invertibility=spec.enforce_invertibility, concentrate_scale\n        =spec.concentrate_scale)\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    fit_kwargs.setdefault('disp', 0)\n    res_ss = mod.fit(start_params=start_params, **fit_kwargs)\n    p.params = res_ss.params\n    res = Bunch({'spec': spec, 'statespace_results': res_ss})\n    return p, res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "date_parser", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/base/datetools.py", "method_code_mask": "from statsmodels.compat.python import asstr\nfrom statsmodels.compat.python import lmap\nfrom statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport datetime\nimport re\nimport numpy as np\nfrom pandas import to_datetime\n\n\ndef date_parser(timestr, parserinfo=None, **kwargs): [MASK]\n", "response": "from statsmodels.compat.python import asstr\nfrom statsmodels.compat.python import lmap\nfrom statsmodels.compat.python import lrange\nfrom statsmodels.compat.python import lzip\nimport datetime\nimport re\nimport numpy as np\nfrom pandas import to_datetime\ndef date_parser(timestr, parserinfo=None, **kwargs):\n    \"\"\"\n    Uses dateutil.parser.parse, but also handles monthly dates of the form\n    1999m4, 1999:m4, 1999:mIV, 1999mIV and the same for quarterly data\n    with q instead of m. It is not case sensitive. The default for annual\n    data is the end of the year, which also differs from dateutil.\n    \"\"\"\n    flags = re.IGNORECASE | re.VERBOSE\n    if re.search(_q_pattern, timestr, flags):\n        y, q = timestr.replace(':', '').lower().split('q')\n        month, day = _quarter_to_day[q.upper()]\n        year = int(y)\n    elif re.search(_m_pattern, timestr, flags):\n        y, m = timestr.replace(':', '').lower().split('m')\n        month, day = _month_to_day[m.upper()]\n        year = int(y)\n        if _is_leap(y) and month == 2:\n            day += 1\n    elif re.search(_y_pattern, timestr, flags):\n        month, day = 12, 31\n        year = int(timestr)\n    else:\n        return to_datetime(timestr, **kwargs)\n    return datetime.datetime(year, month, day)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "yule_walker", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima/estimators/yule_walker.py", "method_code_mask": "from statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.regression import linear_model\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.arima.params import SARIMAXParams\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\n\n\n@deprecate_kwarg('unbiased', 'adjusted')\ndef yule_walker(endog, ar_order=0, demean=True, adjusted=False): [MASK]\n", "response": "from statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.regression import linear_model\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.arima.params import SARIMAXParams\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\n@deprecate_kwarg('unbiased', 'adjusted')\ndef yule_walker(endog, ar_order=0, demean=True, adjusted=False):\n    \"\"\"\n    Estimate AR parameters using Yule-Walker equations.\n\n    Parameters\n    ----------\n    endog : array_like or SARIMAXSpecification\n        Input time series array, assumed to be stationary.\n    ar_order : int, optional\n        Autoregressive order. Default is 0.\n    demean : bool, optional\n        Whether to estimate and remove the mean from the process prior to\n        fitting the autoregressive coefficients. Default is True.\n    adjusted : bool, optional\n        Whether to use the adjusted autocovariance estimator, which uses\n        n - h degrees of freedom rather than n. For some processes this option\n        may  result in a non-positive definite autocovariance matrix. Default\n        is False.\n\n    Returns\n    -------\n    parameters : SARIMAXParams object\n        Contains the parameter estimates from the final iteration.\n    other_results : Bunch\n        Includes one component, `spec`, which is the `SARIMAXSpecification`\n        instance corresponding to the input arguments.\n\n    Notes\n    -----\n    The primary reference is [1]_, section 5.1.1.\n\n    This procedure assumes that the series is stationary.\n\n    For a description of the effect of the adjusted estimate of the\n    autocovariance function, see 2.4.2 of [1]_.\n\n    References\n    ----------\n    .. [1] Brockwell, Peter J., and Richard A. Davis. 2016.\n       Introduction to Time Series and Forecasting. Springer.\n    \"\"\"\n    spec = SARIMAXSpecification(endog, ar_order=ar_order)\n    endog = spec.endog\n    p = SARIMAXParams(spec=spec)\n    if not spec.is_ar_consecutive:\n        raise ValueError(\n            'Yule-Walker estimation unavailable for models with seasonal or non-consecutive AR orders.'\n            )\n    method = 'adjusted' if adjusted else 'mle'\n    p.ar_params, sigma = linear_model.yule_walker(endog, order=ar_order,\n        demean=demean, method=method)\n    p.sigma2 = sigma ** 2\n    other_results = Bunch({'spec': spec})\n    return p, other_results"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "get_macrodata", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/vector_ar/tests/test_var.py", "method_code_mask": "from statsmodels.compat.pandas import assert_index_equal\nfrom statsmodels.compat.python import lrange\nfrom io import BytesIO\nfrom io import StringIO\nimport os\nimport sys\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_equal\nimport pandas as pd\nimport pytest\nfrom statsmodels.datasets import macrodata\nimport statsmodels.tools.data as data_util\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tsa.base.datetools import dates_from_str\nimport statsmodels.tsa.vector_ar.util as util\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.tsa.vector_ar.var_model import var_acf\nimport datetime\nfrom pandas import DataFrame\nfrom pandas import DatetimeIndex\n\n\ndef get_macrodata(): [MASK]\n", "response": "from statsmodels.compat.pandas import assert_index_equal\nfrom statsmodels.compat.python import lrange\nfrom io import BytesIO\nfrom io import StringIO\nimport os\nimport sys\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_equal\nimport pandas as pd\nimport pytest\nfrom statsmodels.datasets import macrodata\nimport statsmodels.tools.data as data_util\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tsa.base.datetools import dates_from_str\nimport statsmodels.tsa.vector_ar.util as util\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.tsa.vector_ar.var_model import var_acf\nimport datetime\nfrom pandas import DataFrame\nfrom pandas import DatetimeIndex\ndef get_macrodata():\n    data = macrodata.load_pandas().data[['realgdp', 'realcons', 'realinv']]\n    data = data.to_records(index=False)\n    nd = data.view((float, 3), type=np.ndarray)\n    nd = np.diff(np.log(nd), axis=0)\n    return nd.ravel().view(data.dtype, type=np.ndarray)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "hpfilter", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/filters/hp_filter.py", "method_code_mask": "import numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import spsolve\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import PandasWrapper\n\n\ndef hpfilter(x, lamb=1600): [MASK]\n", "response": "import numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import spsolve\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import PandasWrapper\ndef hpfilter(x, lamb=1600):\n    \"\"\"\n    Hodrick-Prescott filter.\n\n    Parameters\n    ----------\n    x : array_like\n        The time series to filter, 1-d.\n    lamb : float\n        The Hodrick-Prescott smoothing parameter. A value of 1600 is\n        suggested for quarterly data. Ravn and Uhlig suggest using a value\n        of 6.25 (1600/4**4) for annual data and 129600 (1600*3**4) for monthly\n        data.\n\n    Returns\n    -------\n    cycle : ndarray\n        The estimated cycle in the data given lamb.\n    trend : ndarray\n        The estimated trend in the data given lamb.\n\n    See Also\n    --------\n    statsmodels.tsa.filters.bk_filter.bkfilter\n        Baxter-King filter.\n    statsmodels.tsa.filters.cf_filter.cffilter\n        The Christiano Fitzgerald asymmetric, random walk filter.\n    statsmodels.tsa.seasonal.seasonal_decompose\n        Decompose a time series using moving averages.\n    statsmodels.tsa.seasonal.STL\n        Season-Trend decomposition using LOESS.\n\n    Notes\n    -----\n    The HP filter removes a smooth trend, `T`, from the data `x`. by solving\n\n    min sum((x[t] - T[t])**2 + lamb*((T[t+1] - T[t]) - (T[t] - T[t-1]))**2)\n     T   t\n\n    Here we implemented the HP filter as a ridge-regression rule using\n    scipy.sparse. In this sense, the solution can be written as\n\n    T = inv(I + lamb*K'K)x\n\n    where I is a nobs x nobs identity matrix, and K is a (nobs-2) x nobs matrix\n    such that\n\n    K[i,j] = 1 if i == j or i == j + 2\n    K[i,j] = -2 if i == j + 1\n    K[i,j] = 0 otherwise\n\n    See the notebook `Time Series Filters\n    <../examples/notebooks/generated/tsa_filters.html>`__ for an overview.\n\n    References\n    ----------\n    Hodrick, R.J, and E. C. Prescott. 1980. \"Postwar U.S. Business Cycles: An\n        Empirical Investigation.\" `Carnegie Mellon University discussion\n        paper no. 451`.\n    Ravn, M.O and H. Uhlig. 2002. \"Notes On Adjusted the Hodrick-Prescott\n        Filter for the Frequency of Observations.\" `The Review of Economics and\n        Statistics`, 84(2), 371-80.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> import pandas as pd\n    >>> dta = sm.datasets.macrodata.load_pandas().data\n    >>> index = pd.period_range('1959Q1', '2009Q3', freq='Q')\n    >>> dta.set_index(index, inplace=True)\n\n    >>> cycle, trend = sm.tsa.filters.hpfilter(dta.realgdp, 1600)\n    >>> gdp_decomp = dta[['realgdp']]\n    >>> gdp_decomp[\"cycle\"] = cycle\n    >>> gdp_decomp[\"trend\"] = trend\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots()\n    >>> gdp_decomp[[\"realgdp\", \"trend\"]][\"2000-03-31\":].plot(ax=ax,\n    ...                                                      fontsize=16)\n    >>> plt.show()\n\n    .. plot:: plots/hpf_plot.py\n    \"\"\"\n    pw = PandasWrapper(x)\n    x = array_like(x, 'x', ndim=1)\n    nobs = len(x)\n    I = sparse.eye(nobs, nobs)\n    offsets = np.array([0, 1, 2])\n    data = np.repeat([[1.0], [-2.0], [1.0]], nobs, axis=1)\n    K = sparse.dia_matrix((data, offsets), shape=(nobs - 2, nobs))\n    use_umfpack = True\n    trend = spsolve(I + lamb * K.T.dot(K), x, use_umfpack=use_umfpack)\n    cycle = x - trend\n    return pw.wrap(cycle, append='cycle'), pw.wrap(trend, append='trend')"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_mnlogit", "method_path": "../srcdata/Computation/statsmodels/statsmodels/discrete/tests/test_conditional.py", "method_code_mask": "import numpy as np\nfrom statsmodels.discrete.conditional_models import ConditionalLogit\nfrom statsmodels.discrete.conditional_models import ConditionalPoisson\nfrom statsmodels.discrete.conditional_models import ConditionalMNLogit\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom numpy.testing import assert_allclose\nimport pandas as pd\n\n\ndef gen_mnlogit(n): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.discrete.conditional_models import ConditionalLogit\nfrom statsmodels.discrete.conditional_models import ConditionalPoisson\nfrom statsmodels.discrete.conditional_models import ConditionalMNLogit\nfrom statsmodels.tools.numdiff import approx_fprime\nfrom numpy.testing import assert_allclose\nimport pandas as pd\ndef gen_mnlogit(n):\n    np.random.seed(235)\n    g = np.kron(np.ones(5), np.arange(n // 5))\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    xm = np.concatenate((x1[:, None], x2[:, None]), axis=1)\n    pa = np.array([[0, 1, -1], [0, 2, -1]])\n    lpr = np.dot(xm, pa)\n    pr = np.exp(lpr)\n    pr /= pr.sum(1)[:, None]\n    cpr = pr.cumsum(1)\n    y = 2 * np.ones(n)\n    u = np.random.uniform(size=n)\n    y[u < cpr[:, 2]] = 2\n    y[u < cpr[:, 1]] = 1\n    y[u < cpr[:, 0]] = 0\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'g': g})\n    return df"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "cffilter", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/filters/cf_filter.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.validation import PandasWrapper\nfrom statsmodels.tools.validation import array_like\nimport statsmodels as sm\n\n\ndef cffilter(x, low=6, high=32, drift=True): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.validation import PandasWrapper\nfrom statsmodels.tools.validation import array_like\nimport statsmodels as sm\ndef cffilter(x, low=6, high=32, drift=True):\n    \"\"\"\n    Christiano Fitzgerald asymmetric, random walk filter.\n\n    Parameters\n    ----------\n    x : array_like\n        The 1 or 2d array to filter. If 2d, variables are assumed to be in\n        columns.\n    low : float\n        Minimum period of oscillations. Features below low periodicity are\n        filtered out. Default is 6 for quarterly data, giving a 1.5 year\n        periodicity.\n    high : float\n        Maximum period of oscillations. Features above high periodicity are\n        filtered out. Default is 32 for quarterly data, giving an 8 year\n        periodicity.\n    drift : bool\n        Whether or not to remove a trend from the data. The trend is estimated\n        as np.arange(nobs)*(x[-1] - x[0])/(len(x)-1).\n\n    Returns\n    -------\n    cycle : array_like\n        The features of x between the periodicities low and high.\n    trend : array_like\n        The trend in the data with the cycles removed.\n\n    See Also\n    --------\n    statsmodels.tsa.filters.bk_filter.bkfilter\n        Baxter-King filter.\n    statsmodels.tsa.filters.bk_filter.hpfilter\n        Hodrick-Prescott filter.\n    statsmodels.tsa.seasonal.seasonal_decompose\n        Decompose a time series using moving averages.\n    statsmodels.tsa.seasonal.STL\n        Season-Trend decomposition using LOESS.\n\n    Notes\n    -----\n    See the notebook `Time Series Filters\n    <../examples/notebooks/generated/tsa_filters.html>`__ for an overview.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> import pandas as pd\n    >>> dta = sm.datasets.macrodata.load_pandas().data\n    >>> index = pd.DatetimeIndex(start='1959Q1', end='2009Q4', freq='Q')\n    >>> dta.set_index(index, inplace=True)\n\n    >>> cf_cycles, cf_trend = sm.tsa.filters.cffilter(dta[[\"infl\", \"unemp\"]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots()\n    >>> cf_cycles.plot(ax=ax, style=['r--', 'b-'])\n    >>> plt.show()\n\n    .. plot:: plots/cff_plot.py\n    \"\"\"\n    if low < 2:\n        raise ValueError('low must be >= 2')\n    pw = PandasWrapper(x)\n    x = array_like(x, 'x', ndim=2)\n    nobs, nseries = x.shape\n    a = 2 * np.pi / high\n    b = 2 * np.pi / low\n    if drift:\n        x = x - np.arange(nobs)[:, None] * (x[-1] - x[0]) / (nobs - 1)\n    J = np.arange(1, nobs + 1)\n    Bj = (np.sin(b * J) - np.sin(a * J)) / (np.pi * J)\n    B0 = (b - a) / np.pi\n    Bj = np.r_[B0, Bj][:, None]\n    y = np.zeros((nobs, nseries))\n    for i in range(nobs):\n        B = -0.5 * Bj[0] - np.sum(Bj[1:-i - 2])\n        A = -Bj[0] - np.sum(Bj[1:-i - 2]) - np.sum(Bj[1:i]) - B\n        y[i] = Bj[0] * x[i] + np.dot(Bj[1:-i - 2].T, x[i + 1:-1]) + B * x[-1\n            ] + np.dot(Bj[1:i].T, x[1:i][::-1]) + A * x[0]\n    y = y.squeeze()\n    cycle, trend = y.squeeze(), x.squeeze() - y\n    return pw.wrap(cycle, append='cycle'), pw.wrap(trend, append='trend')"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "get_trendorder", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/vector_ar/util.py", "method_code_mask": "from statsmodels.compat.pandas import frequencies\nfrom statsmodels.compat.python import asbytes\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import int_like\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy import linalg\nimport statsmodels.tsa.tsatools as tsa\nfrom collections import deque\nfrom datetime import datetime\nimport re\n\n\ndef get_trendorder(trend='c'): [MASK]\n", "response": "from statsmodels.compat.pandas import frequencies\nfrom statsmodels.compat.python import asbytes\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import int_like\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy import linalg\nimport statsmodels.tsa.tsatools as tsa\nfrom collections import deque\nfrom datetime import datetime\nimport re\ndef get_trendorder(trend='c'):\n    if trend == 'c':\n        trendorder = 1\n    elif trend in ('n', 'nc'):\n        trendorder = 0\n    elif trend == 'ct':\n        trendorder = 2\n    elif trend == 'ctt':\n        trendorder = 3\n    else:\n        raise ValueError(f'Unkown trend: {trend}')\n    return trendorder"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "bkfilter", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/filters/bk_filter.py", "method_code_mask": "import numpy as np\nfrom scipy.signal import fftconvolve\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import PandasWrapper\n\n\ndef bkfilter(x, low=6, high=32, K=12): [MASK]\n", "response": "import numpy as np\nfrom scipy.signal import fftconvolve\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import PandasWrapper\ndef bkfilter(x, low=6, high=32, K=12):\n    \"\"\"\n    Filter a time series using the Baxter-King bandpass filter.\n\n    Parameters\n    ----------\n    x : array_like\n        A 1 or 2d ndarray. If 2d, variables are assumed to be in columns.\n    low : float\n        Minimum period for oscillations, ie., Baxter and King suggest that\n        the Burns-Mitchell U.S. business cycle has 6 for quarterly data and\n        1.5 for annual data.\n    high : float\n        Maximum period for oscillations BK suggest that the U.S.\n        business cycle has 32 for quarterly data and 8 for annual data.\n    K : int\n        Lead-lag length of the filter. Baxter and King propose a truncation\n        length of 12 for quarterly data and 3 for annual data.\n\n    Returns\n    -------\n    ndarray\n        The cyclical component of x.\n\n    See Also\n    --------\n    statsmodels.tsa.filters.cf_filter.cffilter\n        The Christiano Fitzgerald asymmetric, random walk filter.\n    statsmodels.tsa.filters.bk_filter.hpfilter\n        Hodrick-Prescott filter.\n    statsmodels.tsa.seasonal.seasonal_decompose\n        Decompose a time series using moving averages.\n    statsmodels.tsa.seasonal.STL\n        Season-Trend decomposition using LOESS.\n\n    Notes\n    -----\n    Returns a centered weighted moving average of the original series. Where\n    the weights a[j] are computed ::\n\n      a[j] = b[j] + theta, for j = 0, +/-1, +/-2, ... +/- K\n      b[0] = (omega_2 - omega_1)/pi\n      b[j] = 1/(pi*j)(sin(omega_2*j)-sin(omega_1*j), for j = +/-1, +/-2,...\n\n    and theta is a normalizing constant ::\n\n      theta = -sum(b)/(2K+1)\n\n    See the notebook `Time Series Filters\n    <../examples/notebooks/generated/tsa_filters.html>`__ for an overview.\n\n    References\n    ----------\n    Baxter, M. and R. G. King. \"Measuring Business Cycles: Approximate\n        Band-Pass Filters for Economic Time Series.\" *Review of Economics and\n        Statistics*, 1999, 81(4), 575-593.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> import pandas as pd\n    >>> dta = sm.datasets.macrodata.load_pandas().data\n    >>> index = pd.DatetimeIndex(start='1959Q1', end='2009Q4', freq='Q')\n    >>> dta.set_index(index, inplace=True)\n\n    >>> cycles = sm.tsa.filters.bkfilter(dta[['realinv']], 6, 24, 12)\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots()\n    >>> cycles.plot(ax=ax, style=['r--', 'b-'])\n    >>> plt.show()\n\n    .. plot:: plots/bkf_plot.py\n    \"\"\"\n    pw = PandasWrapper(x)\n    x = array_like(x, 'x', maxdim=2)\n    omega_1 = 2.0 * np.pi / high\n    omega_2 = 2.0 * np.pi / low\n    bweights = np.zeros(2 * K + 1)\n    bweights[K] = (omega_2 - omega_1) / np.pi\n    j = np.arange(1, int(K) + 1)\n    weights = 1 / (np.pi * j) * (np.sin(omega_2 * j) - np.sin(omega_1 * j))\n    bweights[K + j] = weights\n    bweights[:K] = weights[::-1]\n    bweights -= bweights.mean()\n    if x.ndim == 2:\n        bweights = bweights[:, None]\n    x = fftconvolve(x, bweights, mode='valid')\n    return pw.wrap(x, append='cycle', trim_start=K, trim_end=K)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "innovations", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima/estimators/innovations.py", "method_code_mask": "import warnings\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.innovations import arma_innovations\nfrom statsmodels.tsa.stattools import acovf\nfrom statsmodels.tsa.stattools import innovations_algo\nfrom statsmodels.tsa.statespace.tools import diff\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.arima.params import SARIMAXParams\nfrom statsmodels.tsa.arima.estimators.hannan_rissanen import hannan_rissanen\n\n\ndef innovations(endog, ma_order=0, demean=True): [MASK]\n", "response": "import warnings\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.innovations import arma_innovations\nfrom statsmodels.tsa.stattools import acovf\nfrom statsmodels.tsa.stattools import innovations_algo\nfrom statsmodels.tsa.statespace.tools import diff\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.arima.params import SARIMAXParams\nfrom statsmodels.tsa.arima.estimators.hannan_rissanen import hannan_rissanen\ndef innovations(endog, ma_order=0, demean=True):\n    \"\"\"\n    Estimate MA parameters using innovations algorithm.\n\n    Parameters\n    ----------\n    endog : array_like or SARIMAXSpecification\n        Input time series array, assumed to be stationary.\n    ma_order : int, optional\n        Maximum moving average order. Default is 0.\n    demean : bool, optional\n        Whether to estimate and remove the mean from the process prior to\n        fitting the moving average coefficients. Default is True.\n\n    Returns\n    -------\n    parameters : list of SARIMAXParams objects\n        List elements correspond to estimates at different `ma_order`. For\n        example, parameters[0] is an `SARIMAXParams` instance corresponding to\n        `ma_order=0`.\n    other_results : Bunch\n        Includes one component, `spec`, containing the `SARIMAXSpecification`\n        instance corresponding to the input arguments.\n\n    Notes\n    -----\n    The primary reference is [1]_, section 5.1.3.\n\n    This procedure assumes that the series is stationary.\n\n    References\n    ----------\n    .. [1] Brockwell, Peter J., and Richard A. Davis. 2016.\n       Introduction to Time Series and Forecasting. Springer.\n    \"\"\"\n    spec = max_spec = SARIMAXSpecification(endog, ma_order=ma_order)\n    endog = max_spec.endog\n    if demean:\n        endog = endog - endog.mean()\n    if not max_spec.is_ma_consecutive:\n        raise ValueError(\n            'Innovations estimation unavailable for models with seasonal or otherwise non-consecutive MA orders.'\n            )\n    sample_acovf = acovf(endog, fft=True)\n    theta, v = innovations_algo(sample_acovf, nobs=max_spec.ma_order + 1)\n    ma_params = [theta[i, :i] for i in range(1, max_spec.ma_order + 1)]\n    sigma2 = v\n    out = []\n    for i in range(max_spec.ma_order + 1):\n        spec = SARIMAXSpecification(ma_order=i)\n        p = SARIMAXParams(spec=spec)\n        if i == 0:\n            p.params = sigma2[i]\n        else:\n            p.params = np.r_[ma_params[i - 1], sigma2[i]]\n        out.append(p)\n    other_results = Bunch({'spec': spec})\n    return out, other_results"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "coint_johansen", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/vector_ar/vecm.py", "method_code_mask": "from collections import defaultdict\nimport numpy as np\nfrom numpy import hstack\nfrom numpy import vstack\nfrom numpy.linalg import inv\nfrom numpy.linalg import svd\nimport scipy\nimport scipy.stats\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.validation import string_like\nimport statsmodels.tsa.base.tsa_model as tsbase\nfrom statsmodels.tsa.coint_tables import c_sja\nfrom statsmodels.tsa.coint_tables import c_sjt\nfrom statsmodels.tsa.tsatools import duplication_matrix\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import vec\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import CausalityTestResults\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import WhitenessTestResults\nimport statsmodels.tsa.vector_ar.irf as irf\nimport statsmodels.tsa.vector_ar.plotting as plot\nfrom statsmodels.tsa.vector_ar.util import get_index\nfrom statsmodels.tsa.vector_ar.util import seasonal_dummies\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.tsa.vector_ar.var_model import LagOrderResults\nfrom statsmodels.tsa.vector_ar.var_model import _compute_acov\nfrom statsmodels.tsa.vector_ar.var_model import forecast\nfrom statsmodels.tsa.vector_ar.var_model import forecast_interval\nfrom statsmodels.tsa.vector_ar.var_model import ma_rep\nfrom statsmodels.tsa.vector_ar.var_model import orth_ma_rep\nfrom statsmodels.tsa.vector_ar.var_model import test_normality\nimport warnings\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.iolib.summary import summary_params\n\n\ndef coint_johansen(endog, det_order, k_ar_diff): [MASK]\n", "response": "from collections import defaultdict\nimport numpy as np\nfrom numpy import hstack\nfrom numpy import vstack\nfrom numpy.linalg import inv\nfrom numpy.linalg import svd\nimport scipy\nimport scipy.stats\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.sm_exceptions import HypothesisTestWarning\nfrom statsmodels.tools.validation import string_like\nimport statsmodels.tsa.base.tsa_model as tsbase\nfrom statsmodels.tsa.coint_tables import c_sja\nfrom statsmodels.tsa.coint_tables import c_sjt\nfrom statsmodels.tsa.tsatools import duplication_matrix\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import vec\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import CausalityTestResults\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import WhitenessTestResults\nimport statsmodels.tsa.vector_ar.irf as irf\nimport statsmodels.tsa.vector_ar.plotting as plot\nfrom statsmodels.tsa.vector_ar.util import get_index\nfrom statsmodels.tsa.vector_ar.util import seasonal_dummies\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.tsa.vector_ar.var_model import LagOrderResults\nfrom statsmodels.tsa.vector_ar.var_model import _compute_acov\nfrom statsmodels.tsa.vector_ar.var_model import forecast\nfrom statsmodels.tsa.vector_ar.var_model import forecast_interval\nfrom statsmodels.tsa.vector_ar.var_model import ma_rep\nfrom statsmodels.tsa.vector_ar.var_model import orth_ma_rep\nfrom statsmodels.tsa.vector_ar.var_model import test_normality\nimport warnings\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.iolib.summary import summary_params\ndef coint_johansen(endog, det_order, k_ar_diff):\n    \"\"\"\n    Johansen cointegration test of the cointegration rank of a VECM\n\n    Parameters\n    ----------\n    endog : array_like (nobs_tot x neqs)\n        Data to test\n    det_order : int\n        * -1 - no deterministic terms\n        * 0 - constant term\n        * 1 - linear trend\n    k_ar_diff : int, nonnegative\n        Number of lagged differences in the model.\n\n    Returns\n    -------\n    result : JohansenTestResult\n        An object containing the test's results. The most important attributes\n        of the result class are:\n\n        * trace_stat and trace_stat_crit_vals\n        * max_eig_stat and max_eig_stat_crit_vals\n\n    Notes\n    -----\n    The implementation might change to make more use of the existing VECM\n    framework.\n\n    See Also\n    --------\n    statsmodels.tsa.vector_ar.vecm.select_coint_rank\n\n    References\n    ----------\n    .. [1] L\u00fctkepohl, H. 2005. New Introduction to Multiple Time Series\n        Analysis. Springer.\n    \"\"\"\n    import warnings\n    if det_order not in [-1, 0, 1]:\n        warnings.warn(\n            'Critical values are only available for a det_order of -1, 0, or 1.'\n            , category=HypothesisTestWarning, stacklevel=2)\n    if endog.shape[1] > 12:\n        warnings.warn(\n            'Critical values are only available for time series with 12 variables at most.'\n            , category=HypothesisTestWarning, stacklevel=2)\n    from statsmodels.regression.linear_model import OLS\n\n    def detrend(y, order):\n        if order == -1:\n            return y\n        return OLS(y, np.vander(np.linspace(-1, 1, len(y)), order + 1)).fit(\n            ).resid\n\n    def resid(y, x):\n        if x.size == 0:\n            return y\n        r = y - np.dot(x, np.dot(np.linalg.pinv(x), y))\n        return r\n    endog = np.asarray(endog)\n    nobs, neqs = endog.shape\n    if det_order > -1:\n        f = 0\n    else:\n        f = det_order\n    endog = detrend(endog, det_order)\n    dx = np.diff(endog, 1, axis=0)\n    z = lagmat(dx, k_ar_diff)\n    z = z[k_ar_diff:]\n    z = detrend(z, f)\n    dx = dx[k_ar_diff:]\n    dx = detrend(dx, f)\n    r0t = resid(dx, z)\n    lx = endog[:endog.shape[0] - k_ar_diff]\n    lx = lx[1:]\n    dx = detrend(lx, f)\n    rkt = resid(dx, z)\n    skk = np.dot(rkt.T, rkt) / rkt.shape[0]\n    sk0 = np.dot(rkt.T, r0t) / rkt.shape[0]\n    s00 = np.dot(r0t.T, r0t) / r0t.shape[0]\n    sig = np.dot(sk0, np.dot(inv(s00), sk0.T))\n    tmp = inv(skk)\n    au, du = np.linalg.eig(np.dot(tmp, sig))\n    temp = inv(np.linalg.cholesky(np.dot(du.T, np.dot(skk, du))))\n    dt = np.dot(du, temp)\n    auind = np.argsort(au)\n    aind = np.flipud(auind)\n    a = au[aind]\n    d = dt[:, aind]\n    non_zero_d = d.flat != 0\n    if np.any(non_zero_d):\n        d *= np.sign(d.flat[non_zero_d][0])\n    lr1 = np.zeros(neqs)\n    lr2 = np.zeros(neqs)\n    cvm = np.zeros((neqs, 3))\n    cvt = np.zeros((neqs, 3))\n    iota = np.ones(neqs)\n    t, junk = rkt.shape\n    for i in range(0, neqs):\n        tmp = np.log(iota - a)[i:]\n        lr1[i] = -t * np.sum(tmp, 0)\n        lr2[i] = -t * np.log(1 - a[i])\n        cvm[i, :] = c_sja(neqs - i, det_order)\n        cvt[i, :] = c_sjt(neqs - i, det_order)\n        aind[i] = i\n    return JohansenTestResult(rkt, r0t, a, d, lr1, lr2, cvt, cvm, aind)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "var_acf", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/vector_ar/var_model.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.python import lrange\nfrom collections import defaultdict\nfrom io import StringIO\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.decorators import deprecated_alias\nfrom statsmodels.tools.linalg import logdet_symm\nfrom statsmodels.tools.sm_exceptions import OutputWarning\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.base.tsa_model import TimeSeriesModel\nfrom statsmodels.tsa.base.tsa_model import TimeSeriesResultsWrapper\nimport statsmodels.tsa.tsatools as tsa\nfrom statsmodels.tsa.tsatools import duplication_matrix\nfrom statsmodels.tsa.tsatools import unvec\nfrom statsmodels.tsa.tsatools import vec\nfrom statsmodels.tsa.vector_ar import output\nfrom statsmodels.tsa.vector_ar import plotting\nfrom statsmodels.tsa.vector_ar import util\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import CausalityTestResults\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import NormalityTestResults\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import WhitenessTestResults\nfrom statsmodels.tsa.vector_ar.irf import IRAnalysis\nfrom statsmodels.tsa.vector_ar.output import VARSummary\nimport warnings\nimport matplotlib.pyplot as plt\n\n\ndef var_acf(coefs, sig_u, nlags=None): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.python import lrange\nfrom collections import defaultdict\nfrom io import StringIO\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.tools.decorators import cache_readonly\nfrom statsmodels.tools.decorators import deprecated_alias\nfrom statsmodels.tools.linalg import logdet_symm\nfrom statsmodels.tools.sm_exceptions import OutputWarning\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.base.tsa_model import TimeSeriesModel\nfrom statsmodels.tsa.base.tsa_model import TimeSeriesResultsWrapper\nimport statsmodels.tsa.tsatools as tsa\nfrom statsmodels.tsa.tsatools import duplication_matrix\nfrom statsmodels.tsa.tsatools import unvec\nfrom statsmodels.tsa.tsatools import vec\nfrom statsmodels.tsa.vector_ar import output\nfrom statsmodels.tsa.vector_ar import plotting\nfrom statsmodels.tsa.vector_ar import util\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import CausalityTestResults\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import NormalityTestResults\nfrom statsmodels.tsa.vector_ar.hypothesis_test_results import WhitenessTestResults\nfrom statsmodels.tsa.vector_ar.irf import IRAnalysis\nfrom statsmodels.tsa.vector_ar.output import VARSummary\nimport warnings\nimport matplotlib.pyplot as plt\ndef var_acf(coefs, sig_u, nlags=None):\n    \"\"\"\n    Compute autocovariance function ACF_y(h) up to nlags of stable VAR(p)\n    process\n\n    Parameters\n    ----------\n    coefs : ndarray (p x k x k)\n        Coefficient matrices A_i\n    sig_u : ndarray (k x k)\n        Covariance of white noise process u_t\n    nlags : int, optional\n        Defaults to order p of system\n\n    Notes\n    -----\n    Ref: L\u00fctkepohl p.28-29\n\n    Returns\n    -------\n    acf : ndarray, (p, k, k)\n    \"\"\"\n    p, k, _ = coefs.shape\n    if nlags is None:\n        nlags = p\n    result = np.zeros((nlags + 1, k, k))\n    result[:p] = _var_acf(coefs, sig_u)\n    for h in range(p, nlags + 1):\n        for j in range(p):\n            result[h] += np.dot(coefs[j], result[h - j - 1])\n    return result"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "get_dummy_mod", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/statespace/tests/test_mlemodel.py", "method_code_mask": "import os\nimport re\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_raises\nimport pandas as pd\nimport pytest\nfrom statsmodels.datasets import nile\nfrom statsmodels.tsa.statespace import kalman_filter\nfrom statsmodels.tsa.statespace import kalman_smoother\nfrom statsmodels.tsa.statespace import sarimax\nfrom statsmodels.tsa.statespace import varmax\nfrom statsmodels.tsa.statespace.mlemodel import MLEModel\nfrom statsmodels.tsa.statespace.mlemodel import MLEResultsWrapper\nfrom statsmodels.tsa.statespace.tests.results import results_sarimax\nfrom statsmodels.tsa.statespace.tests.results import results_var_misc\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\ndef get_dummy_mod(fit=True, pandas=False): [MASK]\n", "response": "import os\nimport re\nimport warnings\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_raises\nimport pandas as pd\nimport pytest\nfrom statsmodels.datasets import nile\nfrom statsmodels.tsa.statespace import kalman_filter\nfrom statsmodels.tsa.statespace import kalman_smoother\nfrom statsmodels.tsa.statespace import sarimax\nfrom statsmodels.tsa.statespace import varmax\nfrom statsmodels.tsa.statespace.mlemodel import MLEModel\nfrom statsmodels.tsa.statespace.mlemodel import MLEResultsWrapper\nfrom statsmodels.tsa.statespace.tests.results import results_sarimax\nfrom statsmodels.tsa.statespace.tests.results import results_var_misc\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\ndef get_dummy_mod(fit=True, pandas=False):\n    endog = np.arange(100) * 1.0\n    exog = 2 * endog\n    if pandas:\n        index = pd.date_range('1960-01-01', periods=100, freq='MS')\n        endog = pd.Series(endog, index=index)\n        exog = pd.Series(exog, index=index)\n    mod = sarimax.SARIMAX(endog, exog=exog, order=(0, 0, 0),\n        time_varying_regression=True, mle_regression=False,\n        use_exact_diffuse=True)\n    if fit:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            res = mod.fit(disp=-1)\n    else:\n        res = None\n    return mod, res"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "pacf_yw", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/stattools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\n\n\ndef pacf_yw(x: ArrayLike1D, nlags: (int | None)=None, method: Literal[\n    'adjusted', 'mle']='adjusted') ->np.ndarray: [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\ndef pacf_yw(x: ArrayLike1D, nlags: (int | None)=None, method: Literal[\n    'adjusted', 'mle']='adjusted') ->np.ndarray:\n    \"\"\"\n    Partial autocorrelation estimated with non-recursive yule_walker.\n\n    Parameters\n    ----------\n    x : array_like\n        The observations of time series for which pacf is calculated.\n    nlags : int, optional\n        Number of lags to return autocorrelation for. If not provided,\n        uses min(10 * np.log10(nobs), nobs - 1).\n    method : {\"adjusted\", \"mle\"}, default \"adjusted\"\n        The method for the autocovariance calculations in yule walker.\n\n    Returns\n    -------\n    ndarray\n        The partial autocorrelations, maxlag+1 elements.\n\n    See Also\n    --------\n    statsmodels.tsa.stattools.pacf\n        Partial autocorrelation estimation.\n    statsmodels.tsa.stattools.pacf_ols\n        Partial autocorrelation estimation using OLS.\n    statsmodels.tsa.stattools.pacf_burg\n        Partial autocorrelation estimation using Burg\"s method.\n\n    Notes\n    -----\n    This solves yule_walker for each desired lag and contains\n    currently duplicate calculations.\n    \"\"\"\n    x = array_like(x, 'x')\n    nlags = int_like(nlags, 'nlags', optional=True)\n    nobs = x.shape[0]\n    if nlags is None:\n        nlags = max(min(int(10 * np.log10(nobs)), nobs - 1), 1)\n    method = string_like(method, 'method', options=('adjusted', 'mle'))\n    pacf = [1.0]\n    with warnings.catch_warnings():\n        warnings.simplefilter('once', ValueWarning)\n        for k in range(1, nlags + 1):\n            pacf.append(yule_walker(x, k, method=method)[0][-1])\n    return np.array(pacf)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "elimination_matrix", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/tsatools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\n\n\ndef elimination_matrix(n): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\ndef elimination_matrix(n):\n    \"\"\"\n    Create the elimination matrix L_n which satisfies vech(M) = L_n vec(M) for\n    any matrix M\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    \"\"\"\n    n = int_like(n, 'n')\n    vech_indices = vec(np.tril(np.ones((n, n))))\n    return np.eye(n * n)[vech_indices != 0]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "dentonm", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/interp/denton.py", "method_code_mask": "import numpy as np\nfrom numpy import dot\nfrom numpy import eye\nfrom numpy import diag_indices\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import diag\nfrom numpy import asarray\nfrom numpy import r_\nfrom numpy.linalg import solve\n\n\ndef dentonm(indicator, benchmark, freq='aq', **kwargs): [MASK]\n", "response": "import numpy as np\nfrom numpy import dot\nfrom numpy import eye\nfrom numpy import diag_indices\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import diag\nfrom numpy import asarray\nfrom numpy import r_\nfrom numpy.linalg import solve\ndef dentonm(indicator, benchmark, freq='aq', **kwargs):\n    \"\"\"\n    Modified Denton's method to convert low-frequency to high-frequency data.\n\n    Uses proportionate first-differences as the penalty function.  See notes.\n\n    Parameters\n    ----------\n    indicator : array_like\n        A low-frequency indicator series.  It is assumed that there are no\n        pre-sample indicators.  Ie., the first indicators line up with\n        the first benchmark.\n    benchmark : array_like\n        The higher frequency benchmark.  A 1d or 2d data series in columns.\n        If 2d, then M series are assumed.\n    freq : str {\"aq\",\"qm\", \"other\"}\n        The frequency to use in the conversion.\n\n        * \"aq\" - Benchmarking an annual series to quarterly.\n        * \"mq\" - Benchmarking a quarterly series to monthly.\n        * \"other\" - Custom stride.  A kwarg, k, must be supplied.\n    **kwargs\n        Additional keyword argument. For example:\n\n        * k, an int, the number of high-frequency observations that sum to make\n          an aggregate low-frequency observation. `k` is used with\n          `freq` == \"other\".\n\n    Returns\n    -------\n    transformed : ndarray\n        The transformed series.\n\n    Examples\n    --------\n    >>> indicator = [50,100,150,100] * 5\n    >>> benchmark = [500,400,300,400,500]\n    >>> benchmarked = dentonm(indicator, benchmark, freq=\"aq\")\n\n    Notes\n    -----\n    Denton's method minimizes the distance given by the penalty function, in\n    a least squares sense, between the unknown benchmarked series and the\n    indicator series subject to the condition that the sum of the benchmarked\n    series is equal to the benchmark. The modification allows that the first\n    value not be pre-determined as is the case with Denton's original method.\n    If the there is no benchmark provided for the last few indicator\n    observations, then extrapolation is performed using the last\n    benchmark-indicator ratio of the previous period.\n\n    Minimizes sum((X[t]/I[t] - X[t-1]/I[t-1])**2)\n\n    s.t.\n\n    sum(X) = A, for each period.  Where X is the benchmarked series, I is\n    the indicator, and A is the benchmark.\n\n    References\n    ----------\n    Bloem, A.M, Dippelsman, R.J. and Maehle, N.O.  2001 Quarterly National\n        Accounts Manual--Concepts, Data Sources, and Compilation. IMF.\n        http://www.imf.org/external/pubs/ft/qna/2000/Textbook/index.htm\n    Cholette, P. 1988. \"Benchmarking systems of socio-economic time series.\"\n        Statistics Canada, Time Series Research and Analysis Division,\n        Working Paper No TSRA-88-017E.\n    Denton, F.T. 1971. \"Adjustment of monthly or quarterly series to annual\n        totals: an approach based on quadratic minimization.\" Journal of the\n        American Statistical Association. 99-102.\n    \"\"\"\n    indicator = asarray(indicator)\n    if indicator.ndim == 1:\n        indicator = indicator[:, None]\n    benchmark = asarray(benchmark)\n    if benchmark.ndim == 1:\n        benchmark = benchmark[:, None]\n    N = len(indicator)\n    m = len(benchmark)\n    if freq == 'aq':\n        k = 4\n    elif freq == 'qm':\n        k = 3\n    elif freq == 'other':\n        k = kwargs.get('k')\n        if not k:\n            raise ValueError('k must be supplied with freq=\"other\"')\n    else:\n        raise ValueError('freq %s not understood' % freq)\n    n = k * m\n    if N > n:\n        q = N - n\n    else:\n        q = 0\n    B = np.kron(np.eye(m), ones((k, 1)))\n    Zinv = diag(1.0 / indicator.squeeze()[:n])\n    HTH = eye(n)\n    diag_idx0, diag_idx1 = diag_indices(n)\n    HTH[diag_idx0[1:-1], diag_idx1[1:-1]] += 1\n    HTH[diag_idx0[:-1] + 1, diag_idx1[:-1]] = -1\n    HTH[diag_idx0[:-1], diag_idx1[:-1] + 1] = -1\n    W = dot(dot(Zinv, HTH), Zinv)\n    I = zeros((n + m, n + m))\n    I[:n, :n] = W\n    I[:n, n:] = B\n    I[n:, :n] = B.T\n    A = zeros((m + n, 1))\n    A[-m:] = benchmark\n    X = solve(I, A)\n    X = X[:-m]\n    if q > 0:\n        bi = X[n - 1] / indicator[n - 1]\n        extrapolated = bi * indicator[n:]\n        X = r_[X, extrapolated]\n    return X.squeeze()"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "unvech", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/tsatools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\n\n\ndef unvech(v): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\ndef unvech(v):\n    rows = 0.5 * (-1 + np.sqrt(1 + 8 * len(v)))\n    rows = int(np.round(rows))\n    result = np.zeros((rows, rows))\n    result[np.triu_indices(rows)] = v\n    result = result + result.T\n    result[np.diag_indices(rows)] /= 2\n    return result"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "commutation_matrix", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/tsatools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\n\n\ndef commutation_matrix(p, q): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\ndef commutation_matrix(p, q):\n    \"\"\"\n    Create the commutation matrix K_{p,q} satisfying vec(A') = K_{p,q} vec(A)\n\n    Parameters\n    ----------\n    p : int\n    q : int\n\n    Returns\n    -------\n    K : ndarray (pq x pq)\n    \"\"\"\n    p = int_like(p, 'p')\n    q = int_like(q, 'q')\n    K = np.eye(p * q)\n    indices = np.arange(p * q).reshape((p, q), order='F')\n    return K.take(indices.ravel(), axis=0)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "durbin_levinson", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima/estimators/durbin_levinson.py", "method_code_mask": "from statsmodels.compat.pandas import deprecate_kwarg\nimport numpy as np\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.arima.params import SARIMAXParams\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.stattools import acovf\n\n\n@deprecate_kwarg('unbiased', 'adjusted')\ndef durbin_levinson(endog, ar_order=0, demean=True, adjusted=False): [MASK]\n", "response": "from statsmodels.compat.pandas import deprecate_kwarg\nimport numpy as np\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.arima.params import SARIMAXParams\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.stattools import acovf\n@deprecate_kwarg('unbiased', 'adjusted')\ndef durbin_levinson(endog, ar_order=0, demean=True, adjusted=False):\n    \"\"\"\n    Estimate AR parameters at multiple orders using Durbin-Levinson recursions.\n\n    Parameters\n    ----------\n    endog : array_like or SARIMAXSpecification\n        Input time series array, assumed to be stationary.\n    ar_order : int, optional\n        Autoregressive order. Default is 0.\n    demean : bool, optional\n        Whether to estimate and remove the mean from the process prior to\n        fitting the autoregressive coefficients. Default is True.\n    adjusted : bool, optional\n        Whether to use the \"adjusted\" autocovariance estimator, which uses\n        n - h degrees of freedom rather than n. This option can result in\n        a non-positive definite autocovariance matrix. Default is False.\n\n    Returns\n    -------\n    parameters : list of SARIMAXParams objects\n        List elements correspond to estimates at different `ar_order`. For\n        example, parameters[0] is an `SARIMAXParams` instance corresponding to\n        `ar_order=0`.\n    other_results : Bunch\n        Includes one component, `spec`, containing the `SARIMAXSpecification`\n        instance corresponding to the input arguments.\n\n    Notes\n    -----\n    The primary reference is [1]_, section 2.5.1.\n\n    This procedure assumes that the series is stationary.\n\n    References\n    ----------\n    .. [1] Brockwell, Peter J., and Richard A. Davis. 2016.\n       Introduction to Time Series and Forecasting. Springer.\n    \"\"\"\n    spec = max_spec = SARIMAXSpecification(endog, ar_order=ar_order)\n    endog = max_spec.endog\n    if not max_spec.is_ar_consecutive:\n        raise ValueError(\n            'Durbin-Levinson estimation unavailable for models with seasonal or otherwise non-consecutive AR orders.'\n            )\n    gamma = acovf(endog, adjusted=adjusted, fft=True, demean=demean, nlag=\n        max_spec.ar_order)\n    if max_spec.ar_order == 0:\n        ar_params = [None]\n        sigma2 = [gamma[0]]\n    else:\n        Phi = np.zeros((max_spec.ar_order, max_spec.ar_order))\n        v = np.zeros(max_spec.ar_order + 1)\n        Phi[0, 0] = gamma[1] / gamma[0]\n        v[0] = gamma[0]\n        v[1] = v[0] * (1 - Phi[0, 0] ** 2)\n        for i in range(1, max_spec.ar_order):\n            tmp = Phi[i - 1, :i]\n            Phi[i, i] = (gamma[i + 1] - np.dot(tmp, gamma[i:0:-1])) / v[i]\n            Phi[i, :i] = tmp - Phi[i, i] * tmp[::-1]\n            v[i + 1] = v[i] * (1 - Phi[i, i] ** 2)\n        ar_params = [None] + [Phi[i, :i + 1] for i in range(max_spec.ar_order)]\n        sigma2 = v\n    out = []\n    for i in range(max_spec.ar_order + 1):\n        spec = SARIMAXSpecification(ar_order=i)\n        p = SARIMAXParams(spec=spec)\n        if i == 0:\n            p.params = sigma2[i]\n        else:\n            p.params = np.r_[ar_params[i], sigma2[i]]\n        out.append(p)\n    other_results = Bunch({'spec': spec})\n    return out, other_results"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_ar_transparams", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/tsatools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\n\n\ndef _ar_transparams(params): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lrange\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nfrom statsmodels.tools.data import _is_recarray\nfrom statsmodels.tools.data import _is_using_pandas\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.typing import NDArray\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tools.sm_exceptions import recarray_exception\ndef _ar_transparams(params):\n    \"\"\"\n    Transforms params to induce stationarity/invertability.\n\n    Parameters\n    ----------\n    params : array_like\n        The AR coefficients\n\n    Reference\n    ---------\n    Jones(1980)\n    \"\"\"\n    newparams = np.tanh(params / 2)\n    tmp = np.tanh(params / 2)\n    for j in range(1, len(params)):\n        a = newparams[j]\n        for kiter in range(j):\n            tmp[kiter] -= a * newparams[j - kiter - 1]\n        newparams[:j] = tmp[:j]\n    return newparams"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "pacf_burg", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/stattools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\n\n\ndef pacf_burg(x: ArrayLike1D, nlags: (int | None)=None, demean: bool=True\n    ) ->tuple[np.ndarray, np.ndarray]: [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\ndef pacf_burg(x: ArrayLike1D, nlags: (int | None)=None, demean: bool=True\n    ) ->tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate Burg\"s partial autocorrelation estimator.\n\n    Parameters\n    ----------\n    x : array_like\n        Observations of time series for which pacf is calculated.\n    nlags : int, optional\n        Number of lags to return autocorrelation for. If not provided,\n        uses min(10 * np.log10(nobs), nobs - 1).\n    demean : bool, optional\n        Flag indicating to demean that data. Set to False if x has been\n        previously demeaned.\n\n    Returns\n    -------\n    pacf : ndarray\n        Partial autocorrelations for lags 0, 1, ..., nlag.\n    sigma2 : ndarray\n        Residual variance estimates where the value in position m is the\n        residual variance in an AR model that includes m lags.\n\n    See Also\n    --------\n    statsmodels.tsa.stattools.pacf\n        Partial autocorrelation estimation.\n    statsmodels.tsa.stattools.pacf_yw\n         Partial autocorrelation estimation using Yule-Walker.\n    statsmodels.tsa.stattools.pacf_ols\n        Partial autocorrelation estimation using OLS.\n\n    References\n    ----------\n    .. [1] Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series\n        and forecasting. Springer.\n    \"\"\"\n    x = array_like(x, 'x')\n    if demean:\n        x = x - x.mean()\n    nobs = x.shape[0]\n    p = nlags if nlags is not None else min(int(10 * np.log10(nobs)), nobs - 1)\n    p = max(p, 1)\n    if p > nobs - 1:\n        raise ValueError('nlags must be smaller than nobs - 1')\n    d = np.zeros(p + 1)\n    d[0] = 2 * x.dot(x)\n    pacf = np.zeros(p + 1)\n    u = x[::-1].copy()\n    v = x[::-1].copy()\n    d[1] = u[:-1].dot(u[:-1]) + v[1:].dot(v[1:])\n    pacf[1] = 2 / d[1] * v[1:].dot(u[:-1])\n    last_u = np.empty_like(u)\n    last_v = np.empty_like(v)\n    for i in range(1, p):\n        last_u[:] = u\n        last_v[:] = v\n        u[1:] = last_u[:-1] - pacf[i] * last_v[1:]\n        v[1:] = last_v[1:] - pacf[i] * last_u[:-1]\n        d[i + 1] = (1 - pacf[i] ** 2) * d[i] - v[i] ** 2 - u[-1] ** 2\n        pacf[i + 1] = 2 / d[i + 1] * v[i + 1:].dot(u[i:-1])\n    sigma2 = (1 - pacf ** 2) * d / (2.0 * (nobs - np.arange(0, p + 1)))\n    pacf[0] = 1\n    return pacf, sigma2"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "lpol_fima", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima_process.py", "method_code_mask": "from statsmodels.compat.pandas import Appender\nimport warnings\nimport numpy as np\nfrom scipy import linalg\nfrom scipy import optimize\nfrom scipy import signal\nfrom statsmodels.tools.docstring import Docstring\nfrom statsmodels.tools.docstring import remove_parameters\nfrom statsmodels.tools.validation import array_like\nfrom scipy.special import gammaln\n\n\ndef lpol_fima(d, n=20): [MASK]\n", "response": "from statsmodels.compat.pandas import Appender\nimport warnings\nimport numpy as np\nfrom scipy import linalg\nfrom scipy import optimize\nfrom scipy import signal\nfrom statsmodels.tools.docstring import Docstring\nfrom statsmodels.tools.docstring import remove_parameters\nfrom statsmodels.tools.validation import array_like\nfrom scipy.special import gammaln\ndef lpol_fima(d, n=20):\n    \"\"\"MA representation of fractional integration\n\n    .. math:: (1-L)^{-d} for |d|<0.5  or |d|<1 (?)\n\n    Parameters\n    ----------\n    d : float\n        fractional power\n    n : int\n        number of terms to calculate, including lag zero\n\n    Returns\n    -------\n    ma : ndarray\n        coefficients of lag polynomial\n    \"\"\"\n    from scipy.special import gammaln\n    j = np.arange(n)\n    return np.exp(gammaln(d + j) - gammaln(j + 1) - gammaln(d))"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "standardize_lag_order", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima/tools.py", "method_code_mask": "import numpy as np\n\n\ndef standardize_lag_order(order, title=None): [MASK]\n", "response": "import numpy as np\ndef standardize_lag_order(order, title=None):\n    \"\"\"\n    Standardize lag order input.\n\n    Parameters\n    ----------\n    order : int or array_like\n        Maximum lag order (if integer) or iterable of specific lag orders.\n    title : str, optional\n        Description of the order (e.g. \"autoregressive\") to use in error\n        messages.\n\n    Returns\n    -------\n    order : int or list of int\n        Maximum lag order if consecutive lag orders were specified, otherwise\n        a list of integer lag orders.\n\n    Notes\n    -----\n    It is ambiguous if order=[1] is meant to be a boolean list or\n    a list of lag orders to include, but this is irrelevant because either\n    interpretation gives the same result.\n\n    Order=[0] would be ambiguous, except that 0 is not a valid lag\n    order to include, so there is no harm in interpreting as a boolean\n    list, in which case it is the same as order=0, which seems like\n    reasonable behavior.\n\n    Examples\n    --------\n    >>> standardize_lag_order(3)\n    3\n    >>> standardize_lag_order(np.arange(1, 4))\n    3\n    >>> standardize_lag_order([1, 3])\n    [1, 3]\n    \"\"\"\n    order = np.array(order)\n    title = 'order' if title is None else '%s order' % title\n    if not np.all(order == order.astype(int)):\n        raise ValueError('Invalid %s. Non-integer order (%s) given.' % (\n            title, order))\n    order = order.astype(int)\n    if np.any(order < 0):\n        raise ValueError('Terms in the %s cannot be negative.' % title)\n    if order.ndim == 2 and order.shape[1] == 1:\n        order = order[:, 0]\n    elif order.ndim > 1:\n        raise ValueError(\n            'Invalid %s. Must be an integer or 1-dimensional array-like object (e.g. list, ndarray, etc.). Got %s.'\n             % (title, order))\n    if order.ndim == 0:\n        order = order.item()\n    elif len(order) == 0:\n        order = 0\n    else:\n        has_zeros = 0 in order\n        has_multiple_ones = np.sum(order == 1) > 1\n        has_gt_one = np.any(order > 1)\n        if has_zeros or has_multiple_ones:\n            if has_gt_one:\n                raise ValueError(\n                    'Invalid %s. Appears to be a boolean list (since it contains a 0 element and/or multiple elements) but also contains elements greater than 1 like a list of lag orders.'\n                     % title)\n            order = np.where(order == 1)[0] + 1\n        else:\n            order = np.sort(order)\n        if len(order) == 0:\n            order = 0\n        elif np.all(order == np.arange(1, len(order) + 1)):\n            order = order[-1]\n        else:\n            order = order.tolist()\n    has_duplicate = isinstance(order, list) and np.any(np.diff(order) == 0)\n    if has_duplicate:\n        raise ValueError('Invalid %s. Cannot have duplicate elements.' % title)\n    return order"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/tests/test_ar.py", "method_code_mask": "from statsmodels.compat.pytest import pytest_warns\nimport datetime as dt\nfrom itertools import product\nfrom typing import NamedTuple\nfrom typing import Union\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nimport pandas as pd\nfrom pandas import Index\nfrom pandas import Series\nfrom pandas import date_range\nfrom pandas import period_range\nfrom pandas.testing import assert_series_equal\nimport pytest\nfrom statsmodels.datasets import macrodata\nfrom statsmodels.datasets import sunspots\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.sm_exceptions import SpecificationWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.ar_model import AutoRegResultsWrapper\nfrom statsmodels.tsa.ar_model import ar_select_order\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.deterministic import DeterministicProcess\nfrom statsmodels.tsa.deterministic import Seasonality\nfrom statsmodels.tsa.deterministic import TimeTrend\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.tests.results import results_ar\nfrom io import BytesIO\nfrom matplotlib.figure import Figure\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.ar_model import ARResults\n\n\ndef gen_data(nobs, nexog, pandas, seed=92874765): [MASK]\n", "response": "from statsmodels.compat.pytest import pytest_warns\nimport datetime as dt\nfrom itertools import product\nfrom typing import NamedTuple\nfrom typing import Union\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nimport pandas as pd\nfrom pandas import Index\nfrom pandas import Series\nfrom pandas import date_range\nfrom pandas import period_range\nfrom pandas.testing import assert_series_equal\nimport pytest\nfrom statsmodels.datasets import macrodata\nfrom statsmodels.datasets import sunspots\nfrom statsmodels.iolib.summary import Summary\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.sm_exceptions import SpecificationWarning\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.ar_model import AutoRegResultsWrapper\nfrom statsmodels.tsa.ar_model import ar_select_order\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.deterministic import DeterministicProcess\nfrom statsmodels.tsa.deterministic import Seasonality\nfrom statsmodels.tsa.deterministic import TimeTrend\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.tests.results import results_ar\nfrom io import BytesIO\nfrom matplotlib.figure import Figure\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.ar_model import ARResults\ndef gen_data(nobs, nexog, pandas, seed=92874765):\n    rs = np.random.RandomState(seed)\n    endog = rs.standard_normal(nobs)\n    exog = rs.standard_normal((nobs, nexog)) if nexog else None\n    if pandas:\n        index = pd.date_range(dt.datetime(1999, 12, 31), periods=nobs, freq\n            =MONTH_END)\n        endog = pd.Series(endog, name='endog', index=index)\n        if nexog:\n            cols = [f'exog.{i}' for i in range(exog.shape[1])]\n            exog = pd.DataFrame(exog, columns=cols, index=index)\n\n\n    class DataSet(NamedTuple):\n        endog: Union[np.ndarray, pd.Series]\n        exog: Union[np.ndarray, pd.DataFrame]\n    return DataSet(endog=endog, exog=exog)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "levinson_durbin", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/stattools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\n\n\ndef levinson_durbin(s, nlags=10, isacov=False): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\ndef levinson_durbin(s, nlags=10, isacov=False):\n    \"\"\"\n    Levinson-Durbin recursion for autoregressive processes.\n\n    Parameters\n    ----------\n    s : array_like\n        If isacov is False, then this is the time series. If iasacov is true\n        then this is interpreted as autocovariance starting with lag 0.\n    nlags : int, optional\n        The largest lag to include in recursion or order of the autoregressive\n        process.\n    isacov : bool, optional\n        Flag indicating whether the first argument, s, contains the\n        autocovariances or the data series.\n\n    Returns\n    -------\n    sigma_v : float\n        The estimate of the error variance.\n    arcoefs : ndarray\n        The estimate of the autoregressive coefficients for a model including\n        nlags.\n    pacf : ndarray\n        The partial autocorrelation function.\n    sigma : ndarray\n        The entire sigma array from intermediate result, last value is sigma_v.\n    phi : ndarray\n        The entire phi array from intermediate result, last column contains\n        autoregressive coefficients for AR(nlags).\n\n    Notes\n    -----\n    This function returns currently all results, but maybe we drop sigma and\n    phi from the returns.\n\n    If this function is called with the time series (isacov=False), then the\n    sample autocovariance function is calculated with the default options\n    (biased, no fft).\n    \"\"\"\n    s = array_like(s, 's')\n    nlags = int_like(nlags, 'nlags')\n    isacov = bool_like(isacov, 'isacov')\n    order = nlags\n    if isacov:\n        sxx_m = s\n    else:\n        sxx_m = acovf(s, fft=False)[:order + 1]\n    phi = np.zeros((order + 1, order + 1), 'd')\n    sig = np.zeros(order + 1)\n    phi[1, 1] = sxx_m[1] / sxx_m[0]\n    sig[1] = sxx_m[0] - phi[1, 1] * sxx_m[1]\n    for k in range(2, order + 1):\n        phi[k, k] = (sxx_m[k] - np.dot(phi[1:k, k - 1], sxx_m[1:k][::-1])\n            ) / sig[k - 1]\n        for j in range(1, k):\n            phi[j, k] = phi[j, k - 1] - phi[k, k] * phi[k - j, k - 1]\n        sig[k] = sig[k - 1] * (1 - phi[k, k] ** 2)\n    sigma_v = sig[-1]\n    arcoefs = phi[1:, -1]\n    pacf_ = np.diag(phi).copy()\n    pacf_[0] = 1.0\n    return sigma_v, arcoefs, pacf_, sig, phi"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "levinson_durbin_pacf", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/stattools.py", "method_code_mask": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\n\n\ndef levinson_durbin_pacf(pacf, nlags=None): [MASK]\n", "response": "from __future__ import annotations\nfrom statsmodels.compat.numpy import lstsq\nfrom statsmodels.compat.pandas import deprecate_kwarg\nfrom statsmodels.compat.python import Literal\nfrom statsmodels.compat.python import lzip\nfrom statsmodels.compat.scipy import _next_regular\nfrom typing import Union\nimport warnings\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import correlate\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.regression.linear_model import yule_walker\nfrom statsmodels.tools.sm_exceptions import CollinearityWarning\nfrom statsmodels.tools.sm_exceptions import InfeasibleTestError\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nfrom statsmodels.tools.sm_exceptions import MissingDataError\nfrom statsmodels.tools.sm_exceptions import ValueWarning\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tools.validation import bool_like\nfrom statsmodels.tools.validation import dict_like\nfrom statsmodels.tools.validation import float_like\nfrom statsmodels.tools.validation import int_like\nfrom statsmodels.tools.validation import string_like\nfrom statsmodels.tsa._bds import bds\nfrom statsmodels.tsa._innovations import innovations_algo\nfrom statsmodels.tsa._innovations import innovations_filter\nfrom statsmodels.tsa.adfvalues import mackinnoncrit\nfrom statsmodels.tsa.adfvalues import mackinnonp\nfrom statsmodels.tsa.tsatools import add_trend\nfrom statsmodels.tsa.tsatools import lagmat\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.stats.diagnostic import ResultsStore\nfrom scipy.stats import f\nfrom scipy.stats import chi2\nfrom statsmodels.tsa.arima.model import ARIMA\ndef levinson_durbin_pacf(pacf, nlags=None):\n    \"\"\"\n    Levinson-Durbin algorithm that returns the acf and ar coefficients.\n\n    Parameters\n    ----------\n    pacf : array_like\n        Partial autocorrelation array for lags 0, 1, ... p.\n    nlags : int, optional\n        Number of lags in the AR model.  If omitted, returns coefficients from\n        an AR(p) and the first p autocorrelations.\n\n    Returns\n    -------\n    arcoefs : ndarray\n        AR coefficients computed from the partial autocorrelations.\n    acf : ndarray\n        The acf computed from the partial autocorrelations. Array returned\n        contains the autocorrelations corresponding to lags 0, 1, ..., p.\n\n    References\n    ----------\n    .. [1] Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series\n        and forecasting. Springer.\n    \"\"\"\n    pacf = array_like(pacf, 'pacf')\n    nlags = int_like(nlags, 'nlags', optional=True)\n    pacf = np.squeeze(np.asarray(pacf))\n    if pacf[0] != 1:\n        raise ValueError(\n            'The first entry of the pacf corresponds to lags 0 and so must be 1.'\n            )\n    pacf = pacf[1:]\n    n = pacf.shape[0]\n    if nlags is not None:\n        if nlags > n:\n            raise ValueError(\n                'Must provide at least as many values from the pacf as the number of lags.'\n                )\n        pacf = pacf[:nlags]\n        n = pacf.shape[0]\n    acf = np.zeros(n + 1)\n    acf[1] = pacf[0]\n    nu = np.cumprod(1 - pacf ** 2)\n    arcoefs = pacf.copy()\n    for i in range(1, n):\n        prev = arcoefs[:-(n - i)].copy()\n        arcoefs[:-(n - i)] = prev - arcoefs[i] * prev[::-1]\n        acf[i + 1] = arcoefs[i] * nu[i - 1] + prev.dot(acf[1:-(n - i)][::-1])\n    acf[0] = 1\n    return arcoefs, acf"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "burg", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima/estimators/burg.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.regression import linear_model\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.arima.params import SARIMAXParams\n\n\ndef burg(endog, ar_order=0, demean=True): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tools.tools import Bunch\nfrom statsmodels.regression import linear_model\nfrom statsmodels.tsa.arima.specification import SARIMAXSpecification\nfrom statsmodels.tsa.arima.params import SARIMAXParams\ndef burg(endog, ar_order=0, demean=True):\n    \"\"\"\n    Estimate AR parameters using Burg technique.\n\n    Parameters\n    ----------\n    endog : array_like or SARIMAXSpecification\n        Input time series array, assumed to be stationary.\n    ar_order : int, optional\n        Autoregressive order. Default is 0.\n    demean : bool, optional\n        Whether to estimate and remove the mean from the process prior to\n        fitting the autoregressive coefficients.\n\n    Returns\n    -------\n    parameters : SARIMAXParams object\n        Contains the parameter estimates from the final iteration.\n    other_results : Bunch\n        Includes one component, `spec`, which is the `SARIMAXSpecification`\n        instance corresponding to the input arguments.\n\n    Notes\n    -----\n    The primary reference is [1]_, section 5.1.2.\n\n    This procedure assumes that the series is stationary.\n\n    This function is a light wrapper around `statsmodels.linear_model.burg`.\n\n    References\n    ----------\n    .. [1] Brockwell, Peter J., and Richard A. Davis. 2016.\n       Introduction to Time Series and Forecasting. Springer.\n    \"\"\"\n    spec = SARIMAXSpecification(endog, ar_order=ar_order)\n    endog = spec.endog\n    if np.issubdtype(endog.dtype, np.dtype(int)):\n        endog = endog * 1.0\n    if not spec.is_ar_consecutive:\n        raise ValueError(\n            'Burg estimation unavailable for models with seasonal or otherwise non-consecutive AR orders.'\n            )\n    p = SARIMAXParams(spec=spec)\n    if ar_order == 0:\n        p.sigma2 = np.var(endog)\n    else:\n        p.ar_params, p.sigma2 = linear_model.burg(endog, order=ar_order,\n            demean=demean)\n    other_results = Bunch({'spec': spec})\n    return p, other_results"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "arma_innovations", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/innovations/arma_innovations.py", "method_code_mask": "import numpy as np\nfrom statsmodels.tsa import arima_process\nfrom statsmodels.tsa.statespace.tools import prefix_dtype_map\nfrom statsmodels.tools.numdiff import _get_epsilon\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom scipy.linalg.blas import find_best_blas_type\n\n\ndef arma_innovations(endog, ar_params=None, ma_params=None, sigma2=1,\n    normalize=False, prefix=None): [MASK]\n", "response": "import numpy as np\nfrom statsmodels.tsa import arima_process\nfrom statsmodels.tsa.statespace.tools import prefix_dtype_map\nfrom statsmodels.tools.numdiff import _get_epsilon\nfrom statsmodels.tools.numdiff import approx_fprime_cs\nfrom scipy.linalg.blas import find_best_blas_type\ndef arma_innovations(endog, ar_params=None, ma_params=None, sigma2=1,\n    normalize=False, prefix=None):\n    \"\"\"\n    Compute innovations using a given ARMA process.\n\n    Parameters\n    ----------\n    endog : ndarray\n        The observed time-series process, may be univariate or multivariate.\n    ar_params : ndarray, optional\n        Autoregressive parameters.\n    ma_params : ndarray, optional\n        Moving average parameters.\n    sigma2 : ndarray, optional\n        The ARMA innovation variance. Default is 1.\n    normalize : bool, optional\n        Whether or not to normalize the returned innovations. Default is False.\n    prefix : str, optional\n        The BLAS prefix associated with the datatype. Default is to find the\n        best datatype based on given input. This argument is typically only\n        used internally.\n\n    Returns\n    -------\n    innovations : ndarray\n        Innovations (one-step-ahead prediction errors) for the given `endog`\n        series with predictions based on the given ARMA process. If\n        `normalize=True`, then the returned innovations have been \"whitened\" by\n        dividing through by the square root of the mean square error.\n    innovations_mse : ndarray\n        Mean square error for the innovations.\n    \"\"\"\n    endog = np.require(endog, requirements='W')\n    squeezed = endog.ndim == 1\n    if squeezed:\n        endog = endog[:, None]\n    ar_params = np.atleast_1d([] if ar_params is None else ar_params)\n    ma_params = np.atleast_1d([] if ma_params is None else ma_params)\n    nobs, k_endog = endog.shape\n    ar = np.r_[1, -ar_params]\n    ma = np.r_[1, ma_params]\n    if prefix is None:\n        prefix, dtype, _ = find_best_blas_type([endog, ar_params, ma_params,\n            np.array(sigma2)])\n    dtype = prefix_dtype_map[prefix]\n    endog = np.asfortranarray(endog, dtype=dtype)\n    ar_params = np.asfortranarray(ar_params, dtype=dtype)\n    ma_params = np.asfortranarray(ma_params, dtype=dtype)\n    sigma2 = dtype(sigma2).item()\n    arma_transformed_acovf_fast = getattr(_arma_innovations, prefix +\n        'arma_transformed_acovf_fast')\n    arma_innovations_algo_fast = getattr(_arma_innovations, prefix +\n        'arma_innovations_algo_fast')\n    arma_innovations_filter = getattr(_arma_innovations, prefix +\n        'arma_innovations_filter')\n    arma_acovf = arima_process.arma_acovf(ar, ma, sigma2=sigma2, nobs=nobs\n        ) / sigma2\n    acovf, acovf2 = arma_transformed_acovf_fast(ar, ma, arma_acovf)\n    theta, v = arma_innovations_algo_fast(nobs, ar_params, ma_params, acovf,\n        acovf2)\n    v = np.array(v)\n    if np.any(v < 0) or not np.isfinite(theta).all() or not np.isfinite(v).all(\n        ):\n        raise ValueError(NON_STATIONARY_ERROR)\n    u = []\n    for i in range(k_endog):\n        u_i = np.array(arma_innovations_filter(endog[:, i], ar_params,\n            ma_params, theta))\n        u.append(u_i)\n    u = np.vstack(u).T\n    if normalize:\n        u /= v[:, None] ** 0.5\n    if squeezed:\n        u = u.squeeze()\n    return u, v"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "arma_impulse_response", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima_process.py", "method_code_mask": "from statsmodels.compat.pandas import Appender\nimport warnings\nimport numpy as np\nfrom scipy import linalg\nfrom scipy import optimize\nfrom scipy import signal\nfrom statsmodels.tools.docstring import Docstring\nfrom statsmodels.tools.docstring import remove_parameters\nfrom statsmodels.tools.validation import array_like\nfrom scipy.special import gammaln\n\n\ndef arma_impulse_response(ar, ma, leads=100): [MASK]\n", "response": "from statsmodels.compat.pandas import Appender\nimport warnings\nimport numpy as np\nfrom scipy import linalg\nfrom scipy import optimize\nfrom scipy import signal\nfrom statsmodels.tools.docstring import Docstring\nfrom statsmodels.tools.docstring import remove_parameters\nfrom statsmodels.tools.validation import array_like\nfrom scipy.special import gammaln\ndef arma_impulse_response(ar, ma, leads=100):\n    \"\"\"\n    Compute the impulse response function (MA representation) for ARMA process.\n\n    Parameters\n    ----------\n    ar : array_like, 1d\n        The auto regressive lag polynomial.\n    ma : array_like, 1d\n        The moving average lag polynomial.\n    leads : int\n        The number of observations to calculate.\n\n    Returns\n    -------\n    ndarray\n        The impulse response function with nobs elements.\n\n    Notes\n    -----\n    This is the same as finding the MA representation of an ARMA(p,q).\n    By reversing the role of ar and ma in the function arguments, the\n    returned result is the AR representation of an ARMA(p,q), i.e\n\n    ma_representation = arma_impulse_response(ar, ma, leads=100)\n    ar_representation = arma_impulse_response(ma, ar, leads=100)\n\n    Fully tested against matlab\n\n    Examples\n    --------\n    AR(1)\n\n    >>> arma_impulse_response([1.0, -0.8], [1.], leads=10)\n    array([ 1.        ,  0.8       ,  0.64      ,  0.512     ,  0.4096    ,\n            0.32768   ,  0.262144  ,  0.2097152 ,  0.16777216,  0.13421773])\n\n    this is the same as\n\n    >>> 0.8**np.arange(10)\n    array([ 1.        ,  0.8       ,  0.64      ,  0.512     ,  0.4096    ,\n            0.32768   ,  0.262144  ,  0.2097152 ,  0.16777216,  0.13421773])\n\n    MA(2)\n\n    >>> arma_impulse_response([1.0], [1., 0.5, 0.2], leads=10)\n    array([ 1. ,  0.5,  0.2,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ])\n\n    ARMA(1,2)\n\n    >>> arma_impulse_response([1.0, -0.8], [1., 0.5, 0.2], leads=10)\n    array([ 1.        ,  1.3       ,  1.24      ,  0.992     ,  0.7936    ,\n            0.63488   ,  0.507904  ,  0.4063232 ,  0.32505856,  0.26004685])\n    \"\"\"\n    impulse = np.zeros(leads)\n    impulse[0] = 1.0\n    return signal.lfilter(ma, ar, impulse)"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "arma_acovf", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/arima_process.py", "method_code_mask": "from statsmodels.compat.pandas import Appender\nimport warnings\nimport numpy as np\nfrom scipy import linalg\nfrom scipy import optimize\nfrom scipy import signal\nfrom statsmodels.tools.docstring import Docstring\nfrom statsmodels.tools.docstring import remove_parameters\nfrom statsmodels.tools.validation import array_like\nfrom scipy.special import gammaln\n\n\ndef arma_acovf(ar, ma, nobs=10, sigma2=1, dtype=None): [MASK]\n", "response": "from statsmodels.compat.pandas import Appender\nimport warnings\nimport numpy as np\nfrom scipy import linalg\nfrom scipy import optimize\nfrom scipy import signal\nfrom statsmodels.tools.docstring import Docstring\nfrom statsmodels.tools.docstring import remove_parameters\nfrom statsmodels.tools.validation import array_like\nfrom scipy.special import gammaln\ndef arma_acovf(ar, ma, nobs=10, sigma2=1, dtype=None):\n    \"\"\"\n    Theoretical autocovariances of stationary ARMA processes\n\n    Parameters\n    ----------\n    ar : array_like, 1d\n        The coefficients for autoregressive lag polynomial, including zero lag.\n    ma : array_like, 1d\n        The coefficients for moving-average lag polynomial, including zero lag.\n    nobs : int\n        The number of terms (lags plus zero lag) to include in returned acovf.\n    sigma2 : float\n        Variance of the innovation term.\n\n    Returns\n    -------\n    ndarray\n        The autocovariance of ARMA process given by ar, ma.\n\n    See Also\n    --------\n    arma_acf : Autocorrelation function for ARMA processes.\n    acovf : Sample autocovariance estimation.\n\n    References\n    ----------\n    .. [*] Brockwell, Peter J., and Richard A. Davis. 2009. Time Series:\n        Theory and Methods. 2nd ed. 1991. New York, NY: Springer.\n    \"\"\"\n    if dtype is None:\n        dtype = np.common_type(np.array(ar), np.array(ma), np.array(sigma2))\n    p = len(ar) - 1\n    q = len(ma) - 1\n    m = max(p, q) + 1\n    if sigma2.real < 0:\n        raise ValueError('Must have positive innovation variance.')\n    if p == q == 0:\n        out = np.zeros(nobs, dtype=dtype)\n        out[0] = sigma2\n        return out\n    elif p > 0 and np.max(np.abs(np.roots(ar))) >= 1:\n        raise ValueError(NONSTATIONARY_ERROR)\n    ma_coeffs = arma2ma(ar, ma, lags=m)\n    A = np.zeros((m, m), dtype=dtype)\n    b = np.zeros((m, 1), dtype=dtype)\n    tmp_ar = np.zeros(m, dtype=dtype)\n    tmp_ar[:p + 1] = ar\n    for k in range(m):\n        A[k, :k + 1] = tmp_ar[:k + 1][::-1]\n        A[k, 1:m - k] += tmp_ar[k + 1:m]\n        b[k] = sigma2 * np.dot(ma[k:q + 1], ma_coeffs[:max(q + 1 - k, 0)])\n    acovf = np.zeros(max(nobs, m), dtype=dtype)\n    try:\n        acovf[:m] = np.linalg.solve(A, b)[:, 0]\n    except np.linalg.LinAlgError:\n        raise ValueError(NONSTATIONARY_ERROR)\n    if nobs > m:\n        zi = signal.lfiltic([1], ar, acovf[:m][::-1])\n        acovf[m:] = signal.lfilter([1], ar, np.zeros(nobs - m, dtype=dtype),\n            zi=zi)[0]\n    return acovf[:nobs]"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "seasonal_decompose", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/seasonal.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nfrom pandas.core.nanops import nanmean as pd_nanmean\nfrom statsmodels.tools.validation import PandasWrapper\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stl._stl import STL\nfrom statsmodels.tsa.filters.filtertools import convolution_filter\nfrom statsmodels.tsa.stl.mstl import MSTL\nfrom statsmodels.tsa.tsatools import freq_to_period\nfrom pandas.plotting import register_matplotlib_converters\nfrom statsmodels.graphics.utils import _import_mpl\n\n\ndef seasonal_decompose(x, model='additive', filt=None, period=None,\n    two_sided=True, extrapolate_trend=0): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nfrom pandas.core.nanops import nanmean as pd_nanmean\nfrom statsmodels.tools.validation import PandasWrapper\nfrom statsmodels.tools.validation import array_like\nfrom statsmodels.tsa.stl._stl import STL\nfrom statsmodels.tsa.filters.filtertools import convolution_filter\nfrom statsmodels.tsa.stl.mstl import MSTL\nfrom statsmodels.tsa.tsatools import freq_to_period\nfrom pandas.plotting import register_matplotlib_converters\nfrom statsmodels.graphics.utils import _import_mpl\ndef seasonal_decompose(x, model='additive', filt=None, period=None,\n    two_sided=True, extrapolate_trend=0):\n    \"\"\"\n    Seasonal decomposition using moving averages.\n\n    Parameters\n    ----------\n    x : array_like\n        Time series. If 2d, individual series are in columns. x must contain 2\n        complete cycles.\n    model : {\"additive\", \"multiplicative\"}, optional\n        Type of seasonal component. Abbreviations are accepted.\n    filt : array_like, optional\n        The filter coefficients for filtering out the seasonal component.\n        The concrete moving average method used in filtering is determined by\n        two_sided.\n    period : int, optional\n        Period of the series (eg, 1 for annual, 4 for quarterly, etc). Must be\n        used if x is not a pandas object or if the index of x does not have a\n        frequency. Overrides default periodicity of x if x is a pandas object\n        with a timeseries index.\n    two_sided : bool, optional\n        The moving average method used in filtering.\n        If True (default), a centered moving average is computed using the\n        filt. If False, the filter coefficients are for past values only.\n    extrapolate_trend : int or 'freq', optional\n        If set to > 0, the trend resulting from the convolution is\n        linear least-squares extrapolated on both ends (or the single one\n        if two_sided is False) considering this many (+1) closest points.\n        If set to 'freq', use `freq` closest points. Setting this parameter\n        results in no NaN values in trend or resid components.\n\n    Returns\n    -------\n    DecomposeResult\n        A object with seasonal, trend, and resid attributes.\n\n    See Also\n    --------\n    statsmodels.tsa.filters.bk_filter.bkfilter\n        Baxter-King filter.\n    statsmodels.tsa.filters.cf_filter.cffilter\n        Christiano-Fitzgerald asymmetric, random walk filter.\n    statsmodels.tsa.filters.hp_filter.hpfilter\n        Hodrick-Prescott filter.\n    statsmodels.tsa.filters.convolution_filter\n        Linear filtering via convolution.\n    statsmodels.tsa.seasonal.STL\n        Season-Trend decomposition using LOESS.\n\n    Notes\n    -----\n    This is a naive decomposition. More sophisticated methods should\n    be preferred.\n\n    The additive model is Y[t] = T[t] + S[t] + e[t]\n\n    The multiplicative model is Y[t] = T[t] * S[t] * e[t]\n\n    The results are obtained by first estimating the trend by applying\n    a convolution filter to the data. The trend is then removed from the\n    series and the average of this de-trended series for each period is\n    the returned seasonal component.\n    \"\"\"\n    pfreq = period\n    pw = PandasWrapper(x)\n    if period is None:\n        pfreq = getattr(getattr(x, 'index', None), 'inferred_freq', None)\n    x = array_like(x, 'x', maxdim=2)\n    nobs = len(x)\n    if not np.all(np.isfinite(x)):\n        raise ValueError('This function does not handle missing values')\n    if model.startswith('m'):\n        if np.any(x <= 0):\n            raise ValueError(\n                'Multiplicative seasonality is not appropriate for zero and negative values'\n                )\n    if period is None:\n        if pfreq is not None:\n            pfreq = freq_to_period(pfreq)\n            period = pfreq\n        else:\n            raise ValueError(\n                'You must specify a period or x must be a pandas object with a PeriodIndex or a DatetimeIndex with a freq not set to None'\n                )\n    if x.shape[0] < 2 * pfreq:\n        raise ValueError(\n            f'x must have 2 complete cycles requires {2 * pfreq} observations. x only has {x.shape[0]} observation(s)'\n            )\n    if filt is None:\n        if period % 2 == 0:\n            filt = np.array([0.5] + [1] * (period - 1) + [0.5]) / period\n        else:\n            filt = np.repeat(1.0 / period, period)\n    nsides = int(two_sided) + 1\n    trend = convolution_filter(x, filt, nsides)\n    if extrapolate_trend == 'freq':\n        extrapolate_trend = period - 1\n    if extrapolate_trend > 0:\n        trend = _extrapolate_trend(trend, extrapolate_trend + 1)\n    if model.startswith('m'):\n        detrended = x / trend\n    else:\n        detrended = x - trend\n    period_averages = seasonal_mean(detrended, period)\n    if model.startswith('m'):\n        period_averages /= np.mean(period_averages, axis=0)\n    else:\n        period_averages -= np.mean(period_averages, axis=0)\n    seasonal = np.tile(period_averages.T, nobs // period + 1).T[:nobs]\n    if model.startswith('m'):\n        resid = x / seasonal / trend\n    else:\n        resid = detrended - seasonal\n    results = []\n    for s, name in zip((seasonal, trend, resid, x), ('seasonal', 'trend',\n        'resid', None)):\n        results.append(pw.wrap(s.squeeze(), columns=name))\n    return DecomposeResult(seasonal=results[0], trend=results[1], resid=\n        results[2], observed=results[3])"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gen_dfm_data", "method_path": "../srcdata/Computation/statsmodels/statsmodels/tsa/statespace/tests/test_dynamic_factor_mq.py", "method_code_mask": "from statsmodels.compat.pandas import assert_frame_equal\nfrom statsmodels.compat.pandas import assert_series_equal\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nimport pandas as pd\nimport pytest\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools import add_constant\nfrom statsmodels.tsa.statespace import dynamic_factor\nfrom statsmodels.tsa.statespace import dynamic_factor_mq\nfrom statsmodels.tsa.statespace import sarimax\nfrom statsmodels.tsa.statespace.tests import test_dynamic_factor_mq_monte_carlo\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\n\n\ndef gen_dfm_data(k_endog=2, nobs=1000): [MASK]\n", "response": "from statsmodels.compat.pandas import assert_frame_equal\nfrom statsmodels.compat.pandas import assert_series_equal\nimport numpy as np\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_equal\nimport pandas as pd\nimport pytest\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools import add_constant\nfrom statsmodels.tsa.statespace import dynamic_factor\nfrom statsmodels.tsa.statespace import dynamic_factor_mq\nfrom statsmodels.tsa.statespace import sarimax\nfrom statsmodels.tsa.statespace.tests import test_dynamic_factor_mq_monte_carlo\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\ndef gen_dfm_data(k_endog=2, nobs=1000):\n    if k_endog > 10:\n        raise ValueError('Only allows for k_endog <= 10')\n    ix = pd.period_range(start='1950-01', periods=1, freq='M')\n    faux = pd.DataFrame([[0] * k_endog], index=ix)\n    mod = dynamic_factor.DynamicFactor(faux, k_factors=1, factor_order=1)\n    loadings = [0.5, -0.9, 0.2, 0.7, -0.1, -0.1, 0.4, 0.4, 0.8, 0.8][:k_endog]\n    phi = 0.5\n    sigma2 = 1.0\n    idio_ar1 = [0] * k_endog\n    idio_var = [1.0, 0.2, 1.5, 0.8, 0.8, 1.4, 0.1, 0.2, 0.4, 0.5][:k_endog]\n    params = np.r_[loadings, idio_var, phi]\n    endog = mod.simulate(params, nobs)\n    return endog, loadings, phi, sigma2, idio_ar1, idio_var"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "gendat", "method_path": "../srcdata/Computation/statsmodels/statsmodels/imputation/tests/test_mice.py", "method_code_mask": "import numpy as np\nimport pandas as pd\nimport pytest\nfrom statsmodels.imputation import mice\nimport statsmodels.api as sm\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_allclose\nimport warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_pdf import PdfPages\nfrom statsmodels.duration.hazard_regression import PHReg\nfrom statsmodels.regression.linear_model import RegressionResultsWrapper\nfrom statsmodels.genmod.generalized_linear_model import GLMResultsWrapper\n\n\ndef gendat(): [MASK]\n", "response": "import numpy as np\nimport pandas as pd\nimport pytest\nfrom statsmodels.imputation import mice\nimport statsmodels.api as sm\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_allclose\nimport warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_pdf import PdfPages\nfrom statsmodels.duration.hazard_regression import PHReg\nfrom statsmodels.regression.linear_model import RegressionResultsWrapper\nfrom statsmodels.genmod.generalized_linear_model import GLMResultsWrapper\ndef gendat():\n    \"\"\"\n    Create a data set with missing values.\n    \"\"\"\n    gen = np.random.RandomState(34243)\n    n = 200\n    p = 5\n    exog = gen.normal(size=(n, p))\n    exog[:, 0] = exog[:, 1] - exog[:, 2] + 2 * exog[:, 4]\n    exog[:, 0] += gen.normal(size=n)\n    exog[:, 2] = 1 * (exog[:, 2] > 0)\n    endog = exog.sum(1) + gen.normal(size=n)\n    df = pd.DataFrame(exog)\n    df.columns = [('x%d' % k) for k in range(1, p + 1)]\n    df['y'] = endog\n    df.loc[0:59, 'x1'] = np.nan\n    df.loc[0:39, 'x2'] = np.nan\n    df.loc[10:29:2, 'x3'] = np.nan\n    df.loc[20:49:3, 'x4'] = np.nan\n    df.loc[40:44, 'x5'] = np.nan\n    df.loc[30:99:2, 'y'] = np.nan\n    return df"}
{"result": "passed", "model_name": "std", "repository": "statsmodels", "full_method_name": "_data_gen", "method_path": "../srcdata/Computation/statsmodels/statsmodels/base/tests/test_distributed_estimation.py", "method_code_mask": "import numpy as np\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.base.distributed_estimation import _calc_grad\nfrom statsmodels.base.distributed_estimation import _calc_wdesign_mat\nfrom statsmodels.base.distributed_estimation import _est_regularized_debiased\nfrom statsmodels.base.distributed_estimation import _join_debiased\nfrom statsmodels.base.distributed_estimation import _est_regularized_naive\nfrom statsmodels.base.distributed_estimation import _est_unregularized_naive\nfrom statsmodels.base.distributed_estimation import _join_naive\nfrom statsmodels.base.distributed_estimation import DistributedModel\n\n\ndef _data_gen(endog, exog, partitions): [MASK]\n", "response": "import numpy as np\nfrom numpy.testing import assert_equal\nfrom numpy.testing import assert_\nfrom numpy.testing import assert_allclose\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.base.distributed_estimation import _calc_grad\nfrom statsmodels.base.distributed_estimation import _calc_wdesign_mat\nfrom statsmodels.base.distributed_estimation import _est_regularized_debiased\nfrom statsmodels.base.distributed_estimation import _join_debiased\nfrom statsmodels.base.distributed_estimation import _est_regularized_naive\nfrom statsmodels.base.distributed_estimation import _est_unregularized_naive\nfrom statsmodels.base.distributed_estimation import _join_naive\nfrom statsmodels.base.distributed_estimation import DistributedModel\ndef _data_gen(endog, exog, partitions):\n    \"\"\"partitions data\"\"\"\n    n_exog = exog.shape[0]\n    n_part = np.ceil(n_exog / partitions)\n    n_part = np.floor(n_exog / partitions)\n    rem = n_exog - n_part * partitions\n    stp = 0\n    while stp < partitions - 1:\n        ii = int(n_part * stp)\n        jj = int(n_part * (stp + 1))\n        yield endog[ii:jj], exog[ii:jj, :]\n        stp += 1\n    ii = int(n_part * stp)\n    jj = int(n_part * (stp + 1) + rem)\n    yield endog[ii:jj], exog[ii:jj, :]"}

{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "padded_everygram_pipeline", "method_path": "../srcdata/Computation/nltk/nltk/lm/preprocessing.py", "method_code_mask": "from functools import partial\nfrom itertools import chain\nfrom nltk.util import everygrams\nfrom nltk.util import pad_sequence\n\n\ndef padded_everygram_pipeline(order, text): [MASK]\n", "response": "from functools import partial\nfrom itertools import chain\nfrom nltk.util import everygrams\nfrom nltk.util import pad_sequence\ndef padded_everygram_pipeline(order, text):\n    \"\"\"Default preprocessing for a sequence of sentences.\n\n    Creates two iterators:\n\n    - sentences padded and turned into sequences of `nltk.util.everygrams`\n    - sentences padded as above and chained together for a flat stream of words\n\n    :param order: Largest ngram length produced by `everygrams`.\n    :param text: Text to iterate over. Expected to be an iterable of sentences.\n    :type text: Iterable[Iterable[str]]\n    :return: iterator over text as ngrams, iterator over text as vocabulary data\n    \"\"\"\n    padding_fn = partial(pad_both_ends, n=order)\n    return (everygrams(list(padding_fn(sent)), max_len=order) for sent in text\n        ), flatten(map(padding_fn, text))"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "brevity_penalty", "method_path": "../srcdata/Computation/nltk/nltk/translate/bleu_score.py", "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef brevity_penalty(closest_ref_len, hyp_len): [MASK]\n", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\ndef brevity_penalty(closest_ref_len, hyp_len):\n    \"\"\"\n    Calculate brevity penalty.\n\n    As the modified n-gram precision still has the problem from the short\n    length sentence, brevity penalty is used to modify the overall BLEU\n    score according to length.\n\n    An example from the paper. There are three references with length 12, 15\n    and 17. And a concise hypothesis of the length 12. The brevity penalty is 1.\n\n    >>> reference1 = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12\n    >>> reference2 = list('aaaaaaaaaaaaaaa')   # i.e. ['a'] * 15\n    >>> reference3 = list('aaaaaaaaaaaaaaaaa') # i.e. ['a'] * 17\n    >>> hypothesis = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12\n    >>> references = [reference1, reference2, reference3]\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len)\n    1.0\n\n    In case a hypothesis translation is shorter than the references, penalty is\n    applied.\n\n    >>> references = [['a'] * 28, ['a'] * 28]\n    >>> hypothesis = ['a'] * 12\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len)\n    0.2635971381157267\n\n    The length of the closest reference is used to compute the penalty. If the\n    length of a hypothesis is 12, and the reference lengths are 13 and 2, the\n    penalty is applied because the hypothesis length (12) is less then the\n    closest reference length (13).\n\n    >>> references = [['a'] * 13, ['a'] * 2]\n    >>> hypothesis = ['a'] * 12\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS\n    0.9200...\n\n    The brevity penalty doesn't depend on reference order. More importantly,\n    when two reference sentences are at the same distance, the shortest\n    reference sentence length is used.\n\n    >>> references = [['a'] * 13, ['a'] * 11]\n    >>> hypothesis = ['a'] * 12\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> bp1 = brevity_penalty(closest_ref_len, hyp_len)\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(reversed(references), hyp_len)\n    >>> bp2 = brevity_penalty(closest_ref_len, hyp_len)\n    >>> bp1 == bp2 == 1\n    True\n\n    A test example from mteval-v13a.pl (starting from the line 705):\n\n    >>> references = [['a'] * 11, ['a'] * 8]\n    >>> hypothesis = ['a'] * 7\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS\n    0.8668...\n\n    >>> references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n    >>> hypothesis = ['a'] * 7\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len)\n    1.0\n\n    :param hyp_len: The length of the hypothesis for a single sentence OR the\n        sum of all the hypotheses' lengths for a corpus\n    :type hyp_len: int\n    :param closest_ref_len: The length of the closest reference for a single\n        hypothesis OR the sum of all the closest references for every hypotheses.\n    :type closest_ref_len: int\n    :return: BLEU's brevity penalty.\n    :rtype: float\n    \"\"\"\n    if hyp_len > closest_ref_len:\n        return 1\n    elif hyp_len == 0:\n        return 0\n    else:\n        return math.exp(1 - closest_ref_len / hyp_len)"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "grow_diag_final_and", "method_path": "../srcdata/Computation/nltk/nltk/translate/gdfa.py", "method_code_mask": "from collections import defaultdict\n\n\ndef grow_diag_final_and(srclen, trglen, e2f, f2e): [MASK]\n", "response": "from collections import defaultdict\ndef grow_diag_final_and(srclen, trglen, e2f, f2e):\n    \"\"\"\n    This module symmetrisatizes the source-to-target and target-to-source\n    word alignment output and produces, aka. GDFA algorithm (Koehn, 2005).\n\n    Step 1: Find the intersection of the bidirectional alignment.\n\n    Step 2: Search for additional neighbor alignment points to be added, given\n            these criteria: (i) neighbor alignments points are not in the\n            intersection and (ii) neighbor alignments are in the union.\n\n    Step 3: Add all other alignment points that are not in the intersection, not in\n            the neighboring alignments that met the criteria but in the original\n            forward/backward alignment outputs.\n\n        >>> forw = ('0-0 2-1 9-2 21-3 10-4 7-5 11-6 9-7 12-8 1-9 3-10 '\n        ...         '4-11 17-12 17-13 25-14 13-15 24-16 11-17 28-18')\n        >>> back = ('0-0 1-9 2-9 3-10 4-11 5-12 6-6 7-5 8-6 9-7 10-4 '\n        ...         '11-6 12-8 13-12 15-12 17-13 18-13 19-12 20-13 '\n        ...         '21-3 22-12 23-14 24-17 25-15 26-17 27-18 28-18')\n        >>> srctext = (\"\u3053\u306e \u3088\u3046 \u306a \u30cf\u30ed\u30fc \u767d\u8272 \u308f\u3044 \u661f \u306e \uff2c \u95a2\u6570 \"\n        ...            \"\u306f \uff2c \u3068 \u5171 \u306b \u4e0d\u9023\u7d9a \u306b \u5897\u52a0 \u3059\u308b \u3053\u3068 \u304c \"\n        ...            \"\u671f\u5f85 \u3055 \u308c\u308b \u3053\u3068 \u3092 \u793a\u3057 \u305f \u3002\")\n        >>> trgtext = (\"Therefore , we expect that the luminosity function \"\n        ...            \"of such halo white dwarfs increases discontinuously \"\n        ...            \"with the luminosity .\")\n        >>> srclen = len(srctext.split())\n        >>> trglen = len(trgtext.split())\n        >>>\n        >>> gdfa = grow_diag_final_and(srclen, trglen, forw, back)\n        >>> gdfa == sorted(set([(28, 18), (6, 6), (24, 17), (2, 1), (15, 12), (13, 12),\n        ...         (2, 9), (3, 10), (26, 17), (25, 15), (8, 6), (9, 7), (20,\n        ...         13), (18, 13), (0, 0), (10, 4), (13, 15), (23, 14), (7, 5),\n        ...         (25, 14), (1, 9), (17, 13), (4, 11), (11, 17), (9, 2), (22,\n        ...         12), (27, 18), (24, 16), (21, 3), (19, 12), (17, 12), (5,\n        ...         12), (11, 6), (12, 8)]))\n        True\n\n    References:\n    Koehn, P., A. Axelrod, A. Birch, C. Callison, M. Osborne, and D. Talbot.\n    2005. Edinburgh System Description for the 2005 IWSLT Speech\n    Translation Evaluation. In MT Eval Workshop.\n\n    :type srclen: int\n    :param srclen: the number of tokens in the source language\n    :type trglen: int\n    :param trglen: the number of tokens in the target language\n    :type e2f: str\n    :param e2f: the forward word alignment outputs from source-to-target\n                language (in pharaoh output format)\n    :type f2e: str\n    :param f2e: the backward word alignment outputs from target-to-source\n                language (in pharaoh output format)\n    :rtype: set(tuple(int))\n    :return: the symmetrized alignment points from the GDFA algorithm\n    \"\"\"\n    e2f = [tuple(map(int, a.split('-'))) for a in e2f.split()]\n    f2e = [tuple(map(int, a.split('-'))) for a in f2e.split()]\n    neighbors = [(-1, 0), (0, -1), (1, 0), (0, 1), (-1, -1), (-1, 1), (1, -\n        1), (1, 1)]\n    alignment = set(e2f).intersection(set(f2e))\n    union = set(e2f).union(set(f2e))\n    aligned = defaultdict(set)\n    for i, j in alignment:\n        aligned['e'].add(i)\n        aligned['f'].add(j)\n\n    def grow_diag():\n        \"\"\"\n        Search for the neighbor points and them to the intersected alignment\n        points if criteria are met.\n        \"\"\"\n        prev_len = len(alignment) - 1\n        while prev_len < len(alignment):\n            no_new_points = True\n            for e in range(srclen):\n                for f in range(trglen):\n                    if (e, f) in alignment:\n                        for neighbor in neighbors:\n                            neighbor = tuple(i + j for i, j in zip((e, f),\n                                neighbor))\n                            e_new, f_new = neighbor\n                            if (e_new not in aligned and f_new not in aligned\n                                ) and neighbor in union:\n                                alignment.add(neighbor)\n                                aligned['e'].add(e_new)\n                                aligned['f'].add(f_new)\n                                prev_len += 1\n                                no_new_points = False\n            if no_new_points:\n                break\n\n    def final_and(a):\n        \"\"\"\n        Adds remaining points that are not in the intersection, not in the\n        neighboring alignments but in the original *e2f* and *f2e* alignments\n        \"\"\"\n        for e_new in range(srclen):\n            for f_new in range(trglen):\n                if e_new not in aligned and f_new not in aligned and (e_new,\n                    f_new) in union:\n                    alignment.add((e_new, f_new))\n                    aligned['e'].add(e_new)\n                    aligned['f'].add(f_new)\n    grow_diag()\n    final_and(e2f)\n    final_and(f2e)\n    return sorted(alignment)"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "closest_ref_length", "method_path": "../srcdata/Computation/nltk/nltk/translate/bleu_score.py", "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef closest_ref_length(references, hyp_len): [MASK]\n", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\ndef closest_ref_length(references, hyp_len):\n    \"\"\"\n    This function finds the reference that is the closest length to the\n    hypothesis. The closest reference length is referred to as *r* variable\n    from the brevity penalty formula in Papineni et. al. (2002)\n\n    :param references: A list of reference translations.\n    :type references: list(list(str))\n    :param hyp_len: The length of the hypothesis.\n    :type hyp_len: int\n    :return: The length of the reference that's closest to the hypothesis.\n    :rtype: int\n    \"\"\"\n    ref_lens = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len -\n        hyp_len), ref_len))\n    return closest_ref_len"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "_pair_iter", "method_path": "../srcdata/Computation/nltk/nltk/tokenize/punkt.py", "method_code_mask": "import math\nimport re\nimport string\nfrom collections import defaultdict\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Match\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize.api import TokenizerI\n\n\ndef _pair_iter(iterator): [MASK]\n", "response": "import math\nimport re\nimport string\nfrom collections import defaultdict\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Match\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize.api import TokenizerI\ndef _pair_iter(iterator):\n    \"\"\"\n    Yields pairs of tokens from the given iterator such that each input\n    token will appear as the first element in a yielded tuple. The last\n    pair will have None as its second element.\n    \"\"\"\n    iterator = iter(iterator)\n    try:\n        prev = next(iterator)\n    except StopIteration:\n        return\n    for el in iterator:\n        yield prev, el\n        prev = el\n    yield prev, None"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "_wikipedia_example_hmm", "method_path": "../srcdata/Computation/nltk/nltk/test/unit/test_hmm.py", "method_code_mask": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\n\n\ndef _wikipedia_example_hmm(): [MASK]\n", "response": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\ndef _wikipedia_example_hmm():\n    states = ['rain', 'no rain']\n    symbols = ['umbrella', 'no umbrella']\n    A = [[0.7, 0.3], [0.3, 0.7]]\n    B = [[0.9, 0.1], [0.2, 0.8]]\n    pi = [0.5, 0.5]\n    seq = ['umbrella', 'umbrella', 'no umbrella', 'umbrella', 'umbrella']\n    seq = list(zip(seq, [None] * len(seq)))\n    model = hmm._create_hmm_tagger(states, symbols, A, B, pi)\n    return model, states, symbols, seq"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "word_rank_alignment", "method_path": "../srcdata/Computation/nltk/nltk/translate/ribes_score.py", "method_code_mask": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\n\ndef word_rank_alignment(reference, hypothesis, character_based=False): [MASK]\n", "response": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\ndef word_rank_alignment(reference, hypothesis, character_based=False):\n    \"\"\"\n    This is the word rank alignment algorithm described in the paper to produce\n    the *worder* list, i.e. a list of word indices of the hypothesis word orders\n    w.r.t. the list of reference words.\n\n    Below is (H0, R0) example from the Isozaki et al. 2010 paper,\n    note the examples are indexed from 1 but the results here are indexed from 0:\n\n        >>> ref = str('he was interested in world history because he '\n        ... 'read the book').split()\n        >>> hyp = str('he read the book because he was interested in world '\n        ... 'history').split()\n        >>> word_rank_alignment(ref, hyp)\n        [7, 8, 9, 10, 6, 0, 1, 2, 3, 4, 5]\n\n    The (H1, R1) example from the paper, note the 0th index:\n\n        >>> ref = 'John hit Bob yesterday'.split()\n        >>> hyp = 'Bob hit John yesterday'.split()\n        >>> word_rank_alignment(ref, hyp)\n        [2, 1, 0, 3]\n\n    Here is the (H2, R2) example from the paper, note the 0th index here too:\n\n        >>> ref = 'the boy read the book'.split()\n        >>> hyp = 'the book was read by the boy'.split()\n        >>> word_rank_alignment(ref, hyp)\n        [3, 4, 2, 0, 1]\n\n    :param reference: a reference sentence\n    :type reference: list(str)\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: list(str)\n    \"\"\"\n    worder = []\n    hyp_len = len(hypothesis)\n    ref_ngrams = []\n    hyp_ngrams = []\n    for n in range(1, len(reference) + 1):\n        for ng in ngrams(reference, n):\n            ref_ngrams.append(ng)\n        for ng in ngrams(hypothesis, n):\n            hyp_ngrams.append(ng)\n    for i, h_word in enumerate(hypothesis):\n        if h_word not in reference:\n            continue\n        elif hypothesis.count(h_word) == reference.count(h_word) == 1:\n            worder.append(reference.index(h_word))\n        else:\n            max_window_size = max(i, hyp_len - i + 1)\n            for window in range(1, max_window_size):\n                if i + window < hyp_len:\n                    right_context_ngram = tuple(islice(hypothesis, i, i +\n                        window + 1))\n                    num_times_in_ref = ref_ngrams.count(right_context_ngram)\n                    num_times_in_hyp = hyp_ngrams.count(right_context_ngram)\n                    if num_times_in_ref == num_times_in_hyp == 1:\n                        pos = position_of_ngram(right_context_ngram, reference)\n                        worder.append(pos)\n                        break\n                if window <= i:\n                    left_context_ngram = tuple(islice(hypothesis, i -\n                        window, i + 1))\n                    num_times_in_ref = ref_ngrams.count(left_context_ngram)\n                    num_times_in_hyp = hyp_ngrams.count(left_context_ngram)\n                    if num_times_in_ref == num_times_in_hyp == 1:\n                        pos = position_of_ngram(left_context_ngram, reference)\n                        worder.append(pos + len(left_context_ngram) - 1)\n                        break\n    return worder"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "corpus_ribes", "method_path": "../srcdata/Computation/nltk/nltk/translate/ribes_score.py", "method_code_mask": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\n\ndef corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.1): [MASK]\n", "response": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\ndef corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.1):\n    \"\"\"\n    This function \"calculates RIBES for a system output (hypothesis) with\n    multiple references, and returns \"best\" score among multi-references and\n    individual scores. The scores are corpus-wise, i.e., averaged by the number\n    of sentences.\" (c.f. RIBES version 1.03.1 code).\n\n    Different from BLEU's micro-average precision, RIBES calculates the\n    macro-average precision by averaging the best RIBES score for each pair of\n    hypothesis and its corresponding references\n\n    >>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n    ...         'ensures', 'that', 'the', 'military', 'always',\n    ...         'obeys', 'the', 'commands', 'of', 'the', 'party']\n    >>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n    ...          'ensures', 'that', 'the', 'military', 'will', 'forever',\n    ...          'heed', 'Party', 'commands']\n    >>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n    ...          'guarantees', 'the', 'military', 'forces', 'always',\n    ...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']\n    >>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n    ...          'army', 'always', 'to', 'heed', 'the', 'directions',\n    ...          'of', 'the', 'party']\n\n    >>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n    ...         'interested', 'in', 'world', 'history']\n    >>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',\n    ...          'because', 'he', 'read', 'the', 'book']\n\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n    >>> hypotheses = [hyp1, hyp2]\n    >>> round(corpus_ribes(list_of_references, hypotheses),4)\n    0.3597\n\n    :param references: a corpus of lists of reference sentences, w.r.t. hypotheses\n    :type references: list(list(list(str)))\n    :param hypotheses: a list of hypothesis sentences\n    :type hypotheses: list(list(str))\n    :param alpha: hyperparameter used as a prior for the unigram precision.\n    :type alpha: float\n    :param beta: hyperparameter used as a prior for the brevity penalty.\n    :type beta: float\n    :return: The best ribes score from one of the references.\n    :rtype: float\n    \"\"\"\n    corpus_best_ribes = 0.0\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        corpus_best_ribes += sentence_ribes(references, hypothesis, alpha, beta\n            )\n    return corpus_best_ribes / len(hypotheses)"}
{"result": "passed", "model_name": "std", "repository": "nltk", "full_method_name": "sentence_bleu", "method_path": "../srcdata/Computation/nltk/nltk/translate/bleu_score.py", "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None, auto_reweigh=False): [MASK]\n", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None, auto_reweigh=False):\n    \"\"\"\n    Calculate BLEU score (Bilingual Evaluation Understudy) from\n    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\n    \"BLEU: a method for automatic evaluation of machine translation.\"\n    In Proceedings of ACL. https://www.aclweb.org/anthology/P02-1040.pdf\n\n    >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n    ...               'ensures', 'that', 'the', 'military', 'always',\n    ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\n\n    >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\n    ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\n    ...               'that', 'party', 'direct']\n\n    >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n    ...               'ensures', 'that', 'the', 'military', 'will', 'forever',\n    ...               'heed', 'Party', 'commands']\n\n    >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n    ...               'guarantees', 'the', 'military', 'forces', 'always',\n    ...               'being', 'under', 'the', 'command', 'of', 'the',\n    ...               'Party']\n\n    >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n    ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n    ...               'of', 'the', 'party']\n\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS\n    0.5045...\n\n    If there is no ngrams overlap for any order of n-grams, BLEU returns the\n    value 0. This is because the precision for the order of n-grams without\n    overlap is 0, and the geometric mean in the final BLEU score computation\n    multiplies the 0 with the precision of other n-grams. This results in 0\n    (independently of the precision of the other n-gram orders). The following\n    example has zero 3-gram and 4-gram overlaps:\n\n    >>> round(sentence_bleu([reference1, reference2, reference3], hypothesis2),4) # doctest: +ELLIPSIS\n    0.0\n\n    To avoid this harsh behaviour when no ngram overlaps are found a smoothing\n    function can be used.\n\n    >>> chencherry = SmoothingFunction()\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis2,\n    ...     smoothing_function=chencherry.method1) # doctest: +ELLIPSIS\n    0.0370...\n\n    The default BLEU calculates a score for up to 4-grams using uniform\n    weights (this is called BLEU-4). To evaluate your translations with\n    higher/lower order ngrams, use customized weights. E.g. when accounting\n    for up to 5-grams with uniform weights (this is called BLEU-5) use:\n\n    >>> weights = (1./5., 1./5., 1./5., 1./5., 1./5.)\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\n    0.3920...\n\n    Multiple BLEU scores can be computed at once, by supplying a list of weights.\n    E.g. for computing BLEU-2, BLEU-3 *and* BLEU-4 in one computation, use:\n    >>> weights = [\n    ...     (1./2., 1./2.),\n    ...     (1./3., 1./3., 1./3.),\n    ...     (1./4., 1./4., 1./4., 1./4.)\n    ... ]\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\n    [0.7453..., 0.6240..., 0.5045...]\n\n    :param references: reference sentences\n    :type references: list(list(str))\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: list(str)\n    :param weights: weights for unigrams, bigrams, trigrams and so on (one or a list of weights)\n    :type weights: tuple(float) / list(tuple(float))\n    :param smoothing_function:\n    :type smoothing_function: SmoothingFunction\n    :param auto_reweigh: Option to re-normalize the weights uniformly.\n    :type auto_reweigh: bool\n    :return: The sentence-level BLEU score. Returns a list if multiple weights were supplied.\n    :rtype: float / list(float)\n    \"\"\"\n    return corpus_bleu([references], [hypothesis], weights,\n        smoothing_function, auto_reweigh)"}

{"method_name": "eval_block", "full_method_name": "eval_block", "method_path": "../srcdata/Visualization/altair/altair/utils/execeval.py", "method_code": "import ast\nimport sys\nclass _CatchDisplay:\n    \"\"\"Class to temporarily catch sys.displayhook\"\"\"\n\n    def __init__(self):\n        self.output = None\n\n    def __enter__(self):\n        self.old_hook = sys.displayhook\n        sys.displayhook = self\n        return self\n\n    def __exit__(self, type, value, traceback):\n        sys.displayhook = self.old_hook\n        return False\n\n    def __call__(self, output):\n        self.output = output\ndef eval_block(code, namespace=None, filename='<string>'):\n    \"\"\"\n    Execute a multi-line block of code in the given namespace\n    If the final statement in the code is an expression, return\n    the result of the expression.\n    \"\"\"\n    tree = ast.parse(code, filename='<ast>', mode='exec')\n    if namespace is None:\n        namespace = {}\n    catch_display = _CatchDisplay()\n    if isinstance(tree.body[-1], ast.Expr):\n        to_exec, to_eval = tree.body[:-1], tree.body[-1:]\n    else:\n        to_exec, to_eval = tree.body, []\n    for node in to_exec:\n        compiled = compile(ast.Module([node], []), filename=filename, mode=\n            'exec')\n        exec(compiled, namespace)\n    with catch_display:\n        for node in to_eval:\n            compiled = compile(ast.Interactive([node]), filename=filename,\n                mode='single')\n            exec(compiled, namespace)\n    return catch_display.output", "test_code_list": [{"test_code": "HAS_RETURN = \"\"\"\nx = 4\ny = 2 * x\n3 * y\n\"\"\"\ndef test_eval_block_with_return():\n    _globals = {}\n    result = eval_block(HAS_RETURN, _globals)\n    assert result == 24\n    assert _globals['x'] == 4\n    assert _globals['y'] == 8\ntest_eval_block_with_return()", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_execeval.py"}, {"test_code": "NO_RETURN = \"\"\"\nx = 4\ny = 2 * x\nz = 3 * y\n\"\"\"\ndef test_eval_block_without_return():\n    _globals = {}\n    result = eval_block(NO_RETURN, _globals)\n    assert result is None\n    assert _globals['x'] == 4\n    assert _globals['y'] == 8\n    assert _globals['z'] == 24\ntest_eval_block_without_return()", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_execeval.py"}], "instruction": "Functionality: The eval_block function is designed to execute a multi-line block of Python code within a specified namespace. If the last statement in the code block is an expression, the function will return the result of evaluating that expression.\n\nInputs: \n1. code: A string containing the Python code that you want to execute.\n2. namespace: An optional dictionary that serves as the namespace for the execution of the code. If not provided, the code will be executed in an empty namespace.\n3. filename: An optional string that specifies the filename for the code. This is mainly used for debugging and error messages. The default value is '<string>', indicating that the source is not from a specific file.\n\nOutputs:\nThe function returns the output of the last expression evaluated in the code block, if any. If the last line is not an expression or if no output is generated, the function returns None.", "method_code_mask": "import ast\nimport sys\n\n\nclass _CatchDisplay:\n    \"\"\"Class to temporarily catch sys.displayhook\"\"\"\n\n    def __init__(self):\n        self.output = None\n\n    def __enter__(self):\n        self.old_hook = sys.displayhook\n        sys.displayhook = self\n        return self\n\n    def __exit__(self, type, value, traceback):\n        sys.displayhook = self.old_hook\n        return False\n\n    def __call__(self, output):\n        self.output = output\n\n\ndef eval_block(code, namespace=None, filename='<string>'): [MASK]\n"}
{"method_name": "infer_vegalite_type_for_pandas", "full_method_name": "infer_vegalite_type_for_pandas", "method_path": "../srcdata/Visualization/altair/altair/utils/core.py", "method_code": "from __future__ import annotations\nfrom collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom copy import deepcopy\nimport json\nimport itertools\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom typing import Callable\nfrom typing import TypeVar\nfrom typing import Any\nfrom typing import Iterator\nfrom typing import cast\nfrom typing import Literal\nfrom typing import TYPE_CHECKING\nfrom itertools import groupby\nfrom operator import itemgetter\nimport jsonschema\nfrom typing import runtime_checkable\nfrom typing import Protocol\nfrom typing_extensions import runtime_checkable\nfrom typing_extensions import Protocol\nfrom typing_extensions import ParamSpec\nfrom types import ModuleType\nimport typing as t\nimport pandas as pd\nfrom pandas.api.types import infer_dtype\nimport numpy as np\nfrom IPython.core.getipython import get_ipython\ndef infer_vegalite_type_for_pandas(data: object) ->(InferredVegaLiteType |\n    tuple[InferredVegaLiteType, list[Any]]):\n    \"\"\"\n    From an array-like input, infer the correct vega typecode\n    ('ordinal', 'nominal', 'quantitative', or 'temporal')\n\n    Parameters\n    ----------\n    data: object\n    \"\"\"\n    from pandas.api.types import infer_dtype\n    typ = infer_dtype(data, skipna=False)\n    if typ in {'floating', 'mixed-integer-float', 'integer',\n        'mixed-integer', 'complex'}:\n        return 'quantitative'\n    elif typ == 'categorical' and hasattr(data, 'cat') and data.cat.ordered:\n        return 'ordinal', data.cat.categories.tolist()\n    elif typ in {'string', 'bytes', 'categorical', 'boolean', 'mixed',\n        'unicode'}:\n        return 'nominal'\n    elif typ in {'datetime', 'datetime64', 'timedelta', 'timedelta64',\n        'date', 'time', 'period'}:\n        return 'temporal'\n    else:\n        warnings.warn(\n            f\"I don't know how to infer vegalite type from '{typ}'.  Defaulting to nominal.\"\n            , stacklevel=1)\n        return 'nominal'", "test_code_list": [{"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\ndef test_infer_vegalite_type():\n\n    def _check(arr, typ):\n        assert infer_vegalite_type_for_pandas(arr) == typ\n    _check(np.arange(5, dtype=float), 'quantitative')\n    _check(np.arange(5, dtype=int), 'quantitative')\n    _check(np.zeros(5, dtype=bool), 'nominal')\n    _check(pd.date_range('2012', '2013'), 'temporal')\n    _check(pd.timedelta_range(365, periods=12), 'temporal')\n    nulled = pd.Series(np.random.randint(10, size=10))\n    nulled[0] = None\n    _check(nulled, 'quantitative')\n    _check(['a', 'b', 'c'], 'nominal')\n    if hasattr(pytest, 'warns'):\n        with pytest.warns(UserWarning):\n            _check([], 'nominal')\n    else:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore')\n            _check([], 'nominal')\n\ntest_infer_vegalite_type()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}], "instruction": "Functionality: The function infer_vegalite_type_for_pandas is designed to infer the correct typecode ('ordinal', 'nominal', 'quantitative', or 'temporal') for an array-like input that is compatible with the Vega-Lite visualization grammar. It leverages the pandas library's infer_dtype function to determine the data type and then maps it to a suitable Vega-Lite typecode. If the data type is not directly mappable, it provides a default typecode and issues a warning.\n\nInputs: \n- data: object\n  This parameter represents the array-like input data for which the Vega-Lite typecode needs to be inferred. The input can be of any type that pandas can handle, such as a list, a pandas series, or a numpy array.\n\nOutputs: \n- InferredVegaLiteType | tuple[InferredVegaLiteType, list[Any]]\n  The function returns the inferred Vega-Lite typecode, which can be 'ordinal', 'nominal', 'quantitative', or 'temporal'. For categorical data with ordered categories, it returns a tuple containing the typecode and a list of the category labels. If the data type is not directly mappable to a Vega-Lite typecode, it returns 'nominal' by default and issues a warning.", "method_code_mask": "from __future__ import annotations\nfrom collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom copy import deepcopy\nimport json\nimport itertools\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom typing import Callable\nfrom typing import TypeVar\nfrom typing import Any\nfrom typing import Iterator\nfrom typing import cast\nfrom typing import Literal\nfrom typing import TYPE_CHECKING\nfrom itertools import groupby\nfrom operator import itemgetter\nimport jsonschema\nfrom typing import runtime_checkable\nfrom typing import Protocol\nfrom typing_extensions import runtime_checkable\nfrom typing_extensions import Protocol\nfrom typing_extensions import ParamSpec\nfrom types import ModuleType\nimport typing as t\nimport pandas as pd\nfrom pandas.api.types import infer_dtype\nimport numpy as np\nfrom IPython.core.getipython import get_ipython\n\n\ndef infer_vegalite_type_for_pandas(data: object) ->(InferredVegaLiteType |\n    tuple[InferredVegaLiteType, list[Any]]): [MASK]\n"}
{"method_name": "sanitize_pandas_dataframe", "full_method_name": "sanitize_pandas_dataframe", "method_path": "../srcdata/Visualization/altair/altair/utils/core.py", "method_code": "from __future__ import annotations\nfrom collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom copy import deepcopy\nimport json\nimport itertools\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom typing import Callable\nfrom typing import TypeVar\nfrom typing import Any\nfrom typing import Iterator\nfrom typing import cast\nfrom typing import Literal\nfrom typing import TYPE_CHECKING\nfrom itertools import groupby\nfrom operator import itemgetter\nimport jsonschema\nfrom typing import runtime_checkable\nfrom typing import Protocol\nfrom typing_extensions import runtime_checkable\nfrom typing_extensions import Protocol\nfrom typing_extensions import ParamSpec\nfrom types import ModuleType\nimport typing as t\nimport pandas as pd\nfrom pandas.api.types import infer_dtype\nimport numpy as np\nfrom IPython.core.getipython import get_ipython\ndef numpy_is_subtype(dtype: Any, subtype: Any) ->bool:\n    import numpy as np\n    try:\n        return np.issubdtype(dtype, subtype)\n    except (NotImplementedError, TypeError):\n        return False\ndef sanitize_pandas_dataframe(df: pd.DataFrame) ->pd.DataFrame:\n    \"\"\"Sanitize a DataFrame to prepare it for serialization.\n    * Make a copy\n    * Convert RangeIndex columns to strings\n    * Raise ValueError if column names are not strings\n    * Raise ValueError if it has a hierarchical index.\n    * Convert categoricals to strings.\n    * Convert np.bool_ dtypes to Python bool objects\n    * Convert np.int dtypes to Python int objects\n    * Convert floats to objects and replace NaNs/infs with None.\n    * Convert DateTime dtypes into appropriate string representations\n    * Convert Nullable integers to objects and replace NaN with None\n    * Convert Nullable boolean to objects and replace NaN with None\n    * convert dedicated string column to objects and replace NaN with None\n    * Raise a ValueError for TimeDelta dtypes\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    df = df.copy()\n    if isinstance(df.columns, pd.RangeIndex):\n        df.columns = df.columns.astype(str)\n    for col_name in df.columns:\n        if not isinstance(col_name, str):\n            msg = (\n                f'Dataframe contains invalid column name: {col_name!r}. Column names must be strings'\n                )\n            raise ValueError(msg)\n    if isinstance(df.index, pd.MultiIndex):\n        msg = 'Hierarchical indices not supported'\n        raise ValueError(msg)\n    if isinstance(df.columns, pd.MultiIndex):\n        msg = 'Hierarchical indices not supported'\n        raise ValueError(msg)\n    def to_list_if_array(val):\n        if isinstance(val, np.ndarray):\n            return val.tolist()\n        else:\n            return val\n    for dtype_item in df.dtypes.items():\n        col_name = cast(str, dtype_item[0])\n        dtype = dtype_item[1]\n        dtype_name = str(dtype)\n        if dtype_name == 'category':\n            col = df[col_name].astype(object)\n            df[col_name] = col.where(col.notnull(), None)\n        elif dtype_name == 'string':\n            col = df[col_name].astype(object)\n            df[col_name] = col.where(col.notnull(), None)\n        elif dtype_name == 'bool':\n            df[col_name] = df[col_name].astype(object)\n        elif dtype_name == 'boolean':\n            col = df[col_name].astype(object)\n            df[col_name] = col.where(col.notnull(), None)\n        elif dtype_name.startswith(('datetime', 'timestamp')):\n            df[col_name] = df[col_name].apply(lambda x: x.isoformat()).replace(\n                'NaT', '')\n        elif dtype_name.startswith('timedelta'):\n            msg = (\n                f'Field \"{col_name}\" has type \"{dtype}\" which is not supported by Altair. Please convert to either a timestamp or a numerical value.'\n                )\n            raise ValueError(msg)\n        elif dtype_name.startswith('geometry'):\n            continue\n        elif dtype_name in {'Int8', 'Int16', 'Int32', 'Int64', 'UInt8',\n            'UInt16', 'UInt32', 'UInt64', 'Float32', 'Float64'}:\n            col = df[col_name].astype(object)\n            df[col_name] = col.where(col.notnull(), None)\n        elif numpy_is_subtype(dtype, np.integer):\n            df[col_name] = df[col_name].astype(object)\n        elif numpy_is_subtype(dtype, np.floating):\n            col = df[col_name]\n            bad_values = col.isnull() | np.isinf(col)\n            df[col_name] = col.astype(object).where(~bad_values, None)\n        elif dtype == object:\n            col = df[col_name].astype(object).apply(to_list_if_array)\n            df[col_name] = col.where(col.notnull(), None)\n    return df", "test_code_list": [{"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\n@pytest.mark.filterwarnings(\"ignore:'H' is deprecated.*:FutureWarning\")\ndef test_sanitize_dataframe():\n    df = pd.DataFrame({'s': list('abcde'), 'f': np.arange(5, dtype=float),\n        'i': np.arange(5, dtype=int), 'b': np.array([True, False, True, \n        True, False]), 'd': pd.date_range('2012-01-01', periods=5, freq='h'\n        ), 'c': pd.Series(list('ababc'), dtype='category'), 'c2': pd.Series\n        ([1, 'A', 2.5, 'B', None], dtype='category'), 'o': pd.Series([np.\n        array(i) for i in range(5)]), 'p': pd.date_range('2012-01-01',\n        periods=5, freq='h').tz_localize('UTC')})\n    df.iloc[0, df.columns.get_loc('s')] = None\n    df.iloc[0, df.columns.get_loc('f')] = np.nan\n    df.iloc[0, df.columns.get_loc('d')] = pd.NaT\n    df.iloc[0, df.columns.get_loc('o')] = np.array(np.nan)\n    print(df[['s', 'c2']])\n    df_clean = sanitize_pandas_dataframe(df)\n    print(df_clean[['s', 'c2']])\n    print(df_clean[['s', 'c2']].to_dict())\n    s = json.dumps(df_clean.to_dict(orient='records'))\n    print(s)\n    df2 = pd.read_json(io.StringIO(s))\n    df2 = df2[df.columns]\n    for col in df:\n        if str(df[col].dtype).startswith('datetime'):\n            utc = isinstance(df[col].dtype, pd.core.dtypes.dtypes.\n                DatetimeTZDtype)\n            df2[col] = pd.to_datetime(df2[col], utc=utc)\n        else:\n            df2[col] = df2[col].astype(df[col].dtype)\n    df.iloc[0, df.columns.get_loc('o')] = np.nan\n    assert df.equals(df2)\n\ntest_sanitize_dataframe()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}, {"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\n@pytest.mark.filterwarnings(\"ignore:'H' is deprecated.*:FutureWarning\")\n@pytest.mark.skipif(pa is None, reason='pyarrow not installed')\ndef test_sanitize_dataframe_arrow_columns():\n    df = pd.DataFrame({'s': list('abcde'), 'f': np.arange(5, dtype=float),\n        'i': np.arange(5, dtype=int), 'b': np.array([True, False, True, \n        True, False]), 'd': pd.date_range('2012-01-01', periods=5, freq='h'\n        ), 'c': pd.Series(list('ababc'), dtype='category'), 'p': pd.\n        date_range('2012-01-01', periods=5, freq='h').tz_localize('UTC')})\n    df_arrow = pa.Table.from_pandas(df).to_pandas(types_mapper=pd.ArrowDtype)\n    df_clean = sanitize_pandas_dataframe(df_arrow)\n    records = df_clean.to_dict(orient='records')\n    assert records[0] == {'s': 'a', 'f': 0.0, 'i': 0, 'b': True, 'd':\n        '2012-01-01T00:00:00', 'c': 'a', 'p': '2012-01-01T00:00:00+00:00'}\n    json.dumps(records)\n\ntest_sanitize_dataframe_arrow_columns()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}, {"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\ndef test_sanitize_dataframe_colnames():\n    df = pd.DataFrame(np.arange(12).reshape(4, 3))\n    df = sanitize_pandas_dataframe(df)\n    assert [isinstance(col, str) for col in df.columns]\n    df.columns = [4, 'foo', 'bar']\n    with pytest.raises(ValueError) as err:\n        sanitize_pandas_dataframe(df)\n    assert str(err.value).startswith(\n        'Dataframe contains invalid column name: 4.')\n\ntest_sanitize_dataframe_colnames()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}, {"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\ndef test_sanitize_dataframe_timedelta():\n    df = pd.DataFrame({'r': pd.timedelta_range(start='1 day', periods=4)})\n    with pytest.raises(ValueError) as err:\n        sanitize_pandas_dataframe(df)\n    assert str(err.value).startswith('Field \"r\" has type \"timedelta')\n\ntest_sanitize_dataframe_timedelta()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}, {"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\ndef test_sanitize_dataframe_infs():\n    df = pd.DataFrame({'x': [0, 1, 2, np.inf, -np.inf, np.nan]})\n    df_clean = sanitize_pandas_dataframe(df)\n    assert list(df_clean.dtypes) == [object]\n    assert list(df_clean['x']) == [0, 1, 2, None, None, None]\n\ntest_sanitize_dataframe_infs()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}, {"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\n@pytest.mark.skipif(not hasattr(pd, 'Int64Dtype'), reason=\n    f'Nullable integers not supported in pandas v{pd.__version__}')\ndef test_sanitize_nullable_integers():\n    df = pd.DataFrame({'int_np': [1, 2, 3, 4, 5], 'int64': pd.Series([1, 2,\n        3, None, 5], dtype='UInt8'), 'int64_nan': pd.Series([1, 2, 3, float\n        ('nan'), 5], dtype='Int64'), 'float': [1.0, 2.0, 3.0, 4, 5.0],\n        'float_null': [1, 2, None, 4, 5], 'float_inf': [1, 2, None, 4,\n        float('inf')]})\n    df_clean = sanitize_pandas_dataframe(df)\n    assert {col.dtype.name for _, col in df_clean.items()} == {'object'}\n    result_python = {col_name: list(col) for col_name, col in df_clean.items()}\n    assert result_python == {'int_np': [1, 2, 3, 4, 5], 'int64': [1, 2, 3,\n        None, 5], 'int64_nan': [1, 2, 3, None, 5], 'float': [1.0, 2.0, 3.0,\n        4.0, 5.0], 'float_null': [1.0, 2.0, None, 4.0, 5.0], 'float_inf': [\n        1.0, 2.0, None, 4.0, None]}\n\ntest_sanitize_nullable_integers()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}, {"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\n@pytest.mark.skipif(not hasattr(pd, 'StringDtype'), reason=\n    f'dedicated String dtype not supported in pandas v{pd.__version__}')\ndef test_sanitize_string_dtype():\n    df = pd.DataFrame({'string_object': ['a', 'b', 'c', 'd'],\n        'string_string': pd.array(['a', 'b', 'c', 'd'], dtype='string'),\n        'string_object_null': ['a', 'b', None, 'd'], 'string_string_null':\n        pd.array(['a', 'b', None, 'd'], dtype='string')})\n    df_clean = sanitize_pandas_dataframe(df)\n    assert {col.dtype.name for _, col in df_clean.items()} == {'object'}\n    result_python = {col_name: list(col) for col_name, col in df_clean.items()}\n    assert result_python == {'string_object': ['a', 'b', 'c', 'd'],\n        'string_string': ['a', 'b', 'c', 'd'], 'string_object_null': ['a',\n        'b', None, 'd'], 'string_string_null': ['a', 'b', None, 'd']}\n\ntest_sanitize_string_dtype()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}, {"test_code": "import io\nimport json\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pyarrow as pa\n@pytest.mark.skipif(not hasattr(pd, 'BooleanDtype'), reason=\n    f'Nullable boolean dtype not supported in pandas v{pd.__version__}')\ndef test_sanitize_boolean_dtype():\n    df = pd.DataFrame({'bool_none': pd.array([True, False, None], dtype=\n        'boolean'), 'none': pd.array([None, None, None], dtype='boolean'),\n        'bool': pd.array([True, False, True], dtype='boolean')})\n    df_clean = sanitize_pandas_dataframe(df)\n    assert {col.dtype.name for _, col in df_clean.items()} == {'object'}\n    result_python = {col_name: list(col) for col_name, col in df_clean.items()}\n    assert result_python == {'bool_none': [True, False, None], 'none': [\n        None, None, None], 'bool': [True, False, True]}\n\ntest_sanitize_boolean_dtype()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_utils.py"}], "instruction": "Functionality: The sanitize_pandas_dataframe function is designed to prepare a pandas DataFrame for serialization by sanitizing its data. It performs various operations on the DataFrame including making a copy, converting RangeIndex columns to strings, checking and ensuring column names are strings, checking for hierarchical indices, converting categoricals to strings, converting numpy boolean dtypes to Python bool objects, converting numpy integer dtypes to Python int objects, replacing NaNs and infs in floats with None, converting DateTime dtypes to string representations, and handling Nullable integers, booleans, and strings appropriately. It also raises exceptions for unsupported TimeDelta dtypes.\n\nInputs: \n- df: A pandas DataFrame that needs to be sanitized for serialization. The DataFrame can contain various data types including categoricals, numpy boolean, integer and floating types, and datetime types.\n\nOutputs: \n- A sanitized pandas DataFrame. This DataFrame will have all the necessary conversions applied to make it suitable for serialization. It will not have any RangeIndex columns, hierarchical indices, or unsupported data types. All column names will be strings, and categoricals, nullable types, and problematic float values will be converted to objects or strings where necessary, with NaNs replaced by None.", "method_code_mask": "from __future__ import annotations\nfrom collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom copy import deepcopy\nimport json\nimport itertools\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom typing import Callable\nfrom typing import TypeVar\nfrom typing import Any\nfrom typing import Iterator\nfrom typing import cast\nfrom typing import Literal\nfrom typing import TYPE_CHECKING\nfrom itertools import groupby\nfrom operator import itemgetter\nimport jsonschema\nfrom typing import runtime_checkable\nfrom typing import Protocol\nfrom typing_extensions import runtime_checkable\nfrom typing_extensions import Protocol\nfrom typing_extensions import ParamSpec\nfrom types import ModuleType\nimport typing as t\nimport pandas as pd\nfrom pandas.api.types import infer_dtype\nimport numpy as np\nfrom IPython.core.getipython import get_ipython\n\n\ndef numpy_is_subtype(dtype: Any, subtype: Any) ->bool:\n    import numpy as np\n    try:\n        return np.issubdtype(dtype, subtype)\n    except (NotImplementedError, TypeError):\n        return False\n\n\ndef sanitize_pandas_dataframe(df: pd.DataFrame) ->pd.DataFrame: [MASK]\n"}
{"method_name": "update_nested", "full_method_name": "update_nested", "method_path": "../srcdata/Visualization/altair/altair/utils/core.py", "method_code": "from __future__ import annotations\nfrom collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom copy import deepcopy\nimport json\nimport itertools\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom typing import Callable\nfrom typing import TypeVar\nfrom typing import Any\nfrom typing import Iterator\nfrom typing import cast\nfrom typing import Literal\nfrom typing import TYPE_CHECKING\nfrom itertools import groupby\nfrom operator import itemgetter\nimport jsonschema\nfrom typing import runtime_checkable\nfrom typing import Protocol\nfrom typing_extensions import runtime_checkable\nfrom typing_extensions import Protocol\nfrom typing_extensions import ParamSpec\nfrom types import ModuleType\nimport typing as t\nimport pandas as pd\nfrom pandas.api.types import infer_dtype\nimport numpy as np\nfrom IPython.core.getipython import get_ipython\ndef update_nested(original: t.MutableMapping[Any, Any], update: t.Mapping[\n    Any, Any], copy: bool=False) ->t.MutableMapping[Any, Any]:\n    \"\"\"Update nested dictionaries\n\n    Parameters\n    ----------\n    original : MutableMapping\n        the original (nested) dictionary, which will be updated in-place\n    update : Mapping\n        the nested dictionary of updates\n    copy : bool, default False\n        if True, then copy the original dictionary rather than modifying it\n\n    Returns\n    -------\n    original : MutableMapping\n        a reference to the (modified) original dict\n\n    Examples\n    --------\n    >>> original = {'x': {'b': 2, 'c': 4}}\n    >>> update = {'x': {'b': 5, 'd': 6}, 'y': 40}\n    >>> update_nested(original, update)  # doctest: +SKIP\n    {'x': {'b': 5, 'c': 4, 'd': 6}, 'y': 40}\n    >>> original  # doctest: +SKIP\n    {'x': {'b': 5, 'c': 4, 'd': 6}, 'y': 40}\n    \"\"\"\n    if copy:\n        original = deepcopy(original)\n    for key, val in update.items():\n        if isinstance(val, Mapping):\n            orig_val = original.get(key, {})\n            if isinstance(orig_val, MutableMapping):\n                original[key] = update_nested(orig_val, val)\n            else:\n                original[key] = val\n        else:\n            original[key] = val\n    return original", "test_code_list": [{"test_code": "import types\nfrom packaging.version import Version\nfrom importlib.metadata import version as importlib_version\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom pandas.api.types import infer_dtype\nimport pyarrow as pa\ndef test_update_nested():\n    original = {'x': {'b': {'foo': 2}, 'c': 4}}\n    update = {'x': {'b': {'foo': 5}, 'd': 6}, 'y': 40}\n    output = update_nested(original, update, copy=True)\n    assert output is not original\n    assert output == {'x': {'b': {'foo': 5}, 'c': 4, 'd': 6}, 'y': 40}\n    output2 = update_nested(original, update)\n    assert output2 is original\n    assert output == output2\n\ntest_update_nested()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_core.py"}], "instruction": "original = {'x': {'b': 2, 'c': 4}}\nupdate = {'x': {'b': 5, 'd': 6}, 'y': 40}\nresult = update_nested(original, update)", "method_code_mask": "from __future__ import annotations\nfrom collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom copy import deepcopy\nimport json\nimport itertools\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom typing import Callable\nfrom typing import TypeVar\nfrom typing import Any\nfrom typing import Iterator\nfrom typing import cast\nfrom typing import Literal\nfrom typing import TYPE_CHECKING\nfrom itertools import groupby\nfrom operator import itemgetter\nimport jsonschema\nfrom typing import runtime_checkable\nfrom typing import Protocol\nfrom typing_extensions import runtime_checkable\nfrom typing_extensions import Protocol\nfrom typing_extensions import ParamSpec\nfrom types import ModuleType\nimport typing as t\nimport pandas as pd\nfrom pandas.api.types import infer_dtype\nimport numpy as np\nfrom IPython.core.getipython import get_ipython\n\n\ndef update_nested(original: t.MutableMapping[Any, Any], update: t.Mapping[\n    Any, Any], copy: bool=False) ->t.MutableMapping[Any, Any]: [MASK]\n"}
{"method_name": "deprecated", "full_method_name": "deprecated", "method_path": "../srcdata/Visualization/altair/altair/utils/deprecation.py", "method_code": "from __future__ import annotations\nimport sys\nimport warnings\nfrom typing import TYPE_CHECKING\nfrom typing_extensions import deprecated as _deprecated\nfrom typing_extensions import LiteralString\ndef _format_message(version: LiteralString, alternative: (LiteralString |\n    None), message: (LiteralString | None), /) ->LiteralString:\n    output = f'Deprecated in `altair={version}`.'\n    if alternative:\n        output = f'{output} Use {alternative} instead.'\n    return f'{output}\\n{message}' if message else output\nclass AltairDeprecationWarning(DeprecationWarning):\n    ...\ndef deprecated(*, version: LiteralString, alternative: (LiteralString |\n    None)=None, message: (LiteralString | None)=None, category: (type[\n    AltairDeprecationWarning] | None)=AltairDeprecationWarning, stacklevel:\n    int=1):\n    \"\"\"Indicate that a class, function or overload is deprecated.\n    When this decorator is applied to an object, the type checker\n    will generate a diagnostic on usage of the deprecated object.\n    Parameters\n    ----------\n    version\n        ``altair`` version the deprecation first appeared.\n    alternative\n        Suggested replacement class/method/function.\n    message\n        Additional message appended to ``version``, ``alternative``.\n    category\n        If the *category* is ``None``, no warning is emitted at runtime.\n    stacklevel\n        The *stacklevel* determines where the\n        warning is emitted. If it is ``1`` (the default), the warning\n        is emitted at the direct caller of the deprecated object; if it\n        is higher, it is emitted further up the stack.\n        Static type checker behavior is not affected by the *category*\n        and *stacklevel* arguments.\n    References\n    ----------\n    [PEP 702](https://peps.python.org/pep-0702/)\n    \"\"\"\n    msg = _format_message(version, alternative, message)\n    return _deprecated(msg, category=category, stacklevel=stacklevel)", "test_code_list": [{"test_code": "import pytest\nimport re\ndef test_deprecation_decorator():\n\n    @deprecated(version='999', alternative='func_12345')\n    def func(x):\n        return x + 1\n    with pytest.warns(AltairDeprecationWarning, match=\n        'altair=999.+func_12345 instead'):\n        y = func(1)\n    assert y == 2\n\ntest_deprecation_decorator()\n", "code_start": "", "test_path": "../srcdata/Visualization/altair/tests/utils/test_deprecation.py"}], "instruction": "Functionality: The deprecated function is a decorator that indicates a class, function, or overload is deprecated. It generates a diagnostic warning on usage of the deprecated object in a type checker environment. The decorator can be applied to an object to signal its deprecation and suggest an alternative.\n\nInputs:\n- version: A string representing the `altair` version in which the deprecation first appeared.\n- alternative: An optional string suggesting a replacement class, method, or function.\n- message: An optional string with an additional message to be appended to the version and alternative information.\n- category: An optional type, representing the warning category. If None, no warning is emitted at runtime.\n- stacklevel: An integer determining where the warning is emitted. Default is 1, indicating the warning is emitted at the direct caller of the deprecated object. Higher values result in the warning being emitted further up the stack.\n\nOutputs:\n- The deprecated function returns a decorator that, when applied, modifies the behavior of the decorated object by emitting a warning upon usage, based on the provided parameters.", "method_code_mask": "from __future__ import annotations\nimport sys\nimport warnings\nfrom typing import TYPE_CHECKING\nfrom typing_extensions import deprecated as _deprecated\nfrom typing_extensions import LiteralString\n\n\ndef _format_message(version: LiteralString, alternative: (LiteralString |\n    None), message: (LiteralString | None), /) ->LiteralString:\n    output = f'Deprecated in `altair={version}`.'\n    if alternative:\n        output = f'{output} Use {alternative} instead.'\n    return f'{output}\\n{message}' if message else output\n\n\nclass AltairDeprecationWarning(DeprecationWarning):\n    ...\n\n\ndef deprecated(*, version: LiteralString, alternative: (LiteralString |\n    None)=None, message: (LiteralString | None)=None, category: (type[\n    AltairDeprecationWarning] | None)=AltairDeprecationWarning, stacklevel:\n    int=1): [MASK]\n"}

{"method_name": "rgb", "full_method_name": "rgb", "method_path": "../srcdata/Visualization/datashader/datashader/colors.py", "method_code": "from __future__ import annotations\ndef rgb(x):\n    \"\"\"Return a triple representing rgb color.\n\n    Can convert colors by name or hexcode. Passing in a valid rgb tuple is\n    idempotent.\n\n    Example\n    -------\n    >>> rgb('plum')\n    (221, 160, 221)\n    >>> rgb('#FFFFFF')\n    (255, 255, 255)\n    >>> rgb((255, 255, 255))\n    (255, 255, 255)\n    \"\"\"\n    if isinstance(x, str):\n        if x.startswith('#'):\n            return hex_to_rgb(x)\n        elif x in color_lookup:\n            return hex_to_rgb(color_lookup[x])\n        else:\n            raise ValueError(\"Unknown color: '{0}'\".format(x))\n    elif isinstance(x, tuple) and len(x) == 3:\n        if min(x) < 0 or max(x) > 255:\n            raise ValueError('Invalid RGB tuple')\n    else:\n        raise TypeError(\"Don't know how to convert {0} to RGB\".format(x))\n    return x", "test_code_list": [{"test_code": "from datashader.colors import rgb\nfrom datashader.colors import hex_to_rgb\nimport pytest\ndef test_rgb():\n    assert rgb(u'#FAFBFC') == (250, 251, 252)\n    assert rgb('#FAFBFC') == (250, 251, 252)\n    assert rgb('blue') == (0, 0, 255)\n    assert rgb(u'blue') == (0, 0, 255)\n    assert rgb((255, 255, 255)) == (255, 255, 255)\n    with pytest.raises(ValueError):\n        rgb((255, 256, 255))\n    with pytest.raises(ValueError):\n        rgb((-1, 255, 255))\n    with pytest.raises(ValueError):\n        rgb('foobar')\n\ntest_rgb()\n", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_colors.py"}], "instruction": "Functionality: The rgb function is designed to convert various representations of color into an RGB (Red, Green, Blue) color format. It can interpret color names, hex color codes, and RGB tuples, and return the corresponding RGB tuple.\n\nInputs: The function rgb(x) takes a single argument 'x', which can be:\n- A string representing a color name (e.g., 'plum')\n- A string representing a hex color code (e.g., '#FFFFFF')\n- A tuple of three integers (each between 0 and 255) representing a color in the RGB format (e.g., (255, 255, 255))\n\nOutputs: The function returns a tuple of three integers, each between 0 and 255, representing the color in the RGB format. For example:\n- rgb('plum') returns (221, 160, 221)\n- rgb('#FFFFFF') returns (255, 255, 255)\n- rgb((255, 255, 255)) returns (255, 255, 255)", "method_code_mask": "from __future__ import annotations\n\n\ndef rgb(x): [MASK]\n"}
{"method_name": "isreal", "full_method_name": "isreal", "method_path": "../srcdata/Visualization/datashader/datashader/utils.py", "method_code": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\ndef isreal(dt):\n    \"\"\"Check if a datashape is numeric and real.\n\n    Example\n    -------\n    >>> isreal('int32')\n    True\n    >>> isreal('float64')\n    True\n    >>> isreal('string')\n    False\n    >>> isreal('complex64')\n    False\n    \"\"\"\n    dt = datashape.predicates.launder(dt)\n    return isinstance(dt, datashape.Unit) and dt in datashape.typesets.real", "test_code_list": [{"test_code": "import numpy as np\nfrom xarray import DataArray\nfrom datashader.datashape import dshape\nfrom datashader.utils import Dispatcher\nfrom datashader.utils import apply\nfrom datashader.utils import calc_res\nfrom datashader.utils import isreal\nfrom datashader.utils import orient_array\ndef test_isreal():\n    assert isreal('int32')\n    assert isreal(dshape('int32'))\n    assert isreal('?int32')\n    assert isreal('float64')\n    assert not isreal('complex64')\n    assert not isreal('{x: int64, y: float64}')\n\ntest_isreal()\n", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_utils.py"}], "instruction": "Functionality: The isreal function checks if a datashape type is numeric and represents real numbers. This includes integer and floating-point types but excludes complex numbers, strings, and any other non-numeric datashape types.\n\nInputs: \n- dt: A datashape type (string) that needs to be checked for being numeric and real. The datashape type can be in any format that datashape can interpret, such as 'int32', 'float64', 'complex64', 'string', etc.\n\nOutputs: \n- A boolean value indicating whether the datashape type dt is numeric and represents real numbers. The function returns True if dt is a real numeric type like 'int32' or 'float64', and False if it is a complex number type, string, or any other non-numeric type.", "method_code_mask": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\n\n\ndef isreal(dt): [MASK]\n"}
{"method_name": "apply", "full_method_name": "apply", "method_path": "../srcdata/Visualization/datashader/datashader/utils.py", "method_code": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\ndef apply(func, args, kwargs=None):\n    if kwargs:\n        return func(*args, **kwargs)\n    else:\n        return func(*args)", "test_code_list": [{"test_code": "import numpy as np\nfrom xarray import DataArray\nfrom datashader.datashape import dshape\nfrom datashader.utils import Dispatcher\nfrom datashader.utils import apply\nfrom datashader.utils import calc_res\nfrom datashader.utils import isreal\nfrom datashader.utils import orient_array\ndef test_apply():\n\n    def f(a, b, c=1, d=2):\n        return a + b + c + d\n    assert apply(f, (1, 2)) == 6\n    assert apply(f, (1, 2), dict(c=3)) == 8\n\ntest_apply()\n", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_utils.py"}], "instruction": "Functionality: The apply function is designed to execute a given function with specified positional and keyword arguments. This function is a versatile tool for software engineers to apply any user-defined function with a set of arguments, enabling a wide range of operations from data processing to algorithm execution.\n\nInputs: \n1. func: A callable object (function, lambda, etc.) that the apply function will execute. This function can accept any number of positional and keyword arguments.\n2. args: A tuple containing the positional arguments that will be passed to the 'func' when it is called. The number of elements in 'args' should match the number of positional arguments expected by 'func'.\n3. kwargs: An optional dictionary containing keyword arguments that will be passed to the 'func' when it is called. These arguments provide a way to pass named parameters to 'func', allowing for more flexibility in function invocation.\n\nOutputs: \nThe return value of the apply function is the result of calling 'func' with 'args' and 'kwargs'. This output can be of any type, depending on the function being applied. For instance, if 'func' is a mathematical function, the output might be a number. If 'func' manipulates data structures, the output could be a modified list, dictionary, or another data structure. It is essential to understand the expected output type of 'func' to appropriately handle the result of the apply function.", "method_code_mask": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\n\n\ndef apply(func, args, kwargs=None): [MASK]\n"}
{"method_name": "calc_res", "full_method_name": "calc_res", "method_path": "../srcdata/Visualization/datashader/datashader/utils.py", "method_code": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\ndef calc_res(raster):\n    \"\"\"Calculate the resolution of xarray.DataArray raster and return it as the\n    two-tuple (xres, yres). yres is positive if it is decreasing.\n    \"\"\"\n    h, w = raster.shape[-2:]\n    ydim, xdim = raster.dims[-2:]\n    xcoords = raster[xdim].values\n    ycoords = raster[ydim].values\n    xres = (xcoords[-1] - xcoords[0]) / (w - 1)\n    yres = (ycoords[0] - ycoords[-1]) / (h - 1)\n    return xres, yres", "test_code_list": [{"test_code": "import numpy as np\nfrom xarray import DataArray\nfrom datashader.datashape import dshape\nfrom datashader.utils import Dispatcher\nfrom datashader.utils import apply\nfrom datashader.utils import calc_res\nfrom datashader.utils import isreal\nfrom datashader.utils import orient_array\ndef test_calc_res():\n    x = [5, 7]\n    y = [0, 1]\n    z = [[0, 0], [0, 0]]\n    dims = 'y', 'x'\n    xarr = DataArray(z, coords=dict(x=x, y=y), dims=dims)\n    xres, yres = calc_res(xarr)\n    assert xres == 2\n    assert yres == -1\n    xarr = DataArray(z, coords=dict(x=x, y=y[::-1]), dims=dims)\n    xres, yres = calc_res(xarr)\n    assert xres == 2\n    assert yres == 1\n    xarr = DataArray(z, coords=dict(x=x[::-1], y=y), dims=dims)\n    xres, yres = calc_res(xarr)\n    assert xres == -2\n    assert yres == -1\n    xarr = DataArray(z, coords=dict(x=x[::-1], y=y[::-1]), dims=dims)\n    xres, yres = calc_res(xarr)\n    assert xres == -2\n    assert yres == 1\n\ntest_calc_res()\n", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_utils.py"}], "instruction": "Functionality: The calc_res function is designed to calculate the resolution of an xarray.DataArray raster. It computes the resolution along the x and y dimensions and returns these values as a two-tuple, where the first element is the x-resolution and the second element is the y-resolution. The y-resolution is returned as a positive value if the y-coordinates are decreasing.\n\nInputs: \n- raster: An xarray.DataArray object representing the raster data. It is assumed to have at least two dimensions, with the last two dimensions being the y and x dimensions, respectively.\n\nOutputs: \n- A two-tuple containing the x-resolution and y-resolution of the raster. The x-resolution is the difference in the x-coordinates divided by the number of x-dimension elements minus one. The y-resolution is the difference in the y-coordinates divided by the number of y-dimension elements minus one, and it is returned as a positive value if the y-coordinates are in a decreasing order.", "method_code_mask": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\n\n\ndef calc_res(raster): [MASK]\n"}
{"method_name": "orient_array", "full_method_name": "orient_array", "method_path": "../srcdata/Visualization/datashader/datashader/utils.py", "method_code": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\ndef orient_array(raster, res=None, layer=None):\n    \"\"\"\n    Reorients the array to a canonical orientation depending on\n    whether the x and y-resolution values are positive or negative.\n\n    Parameters\n    ----------\n    raster : DataArray\n        xarray DataArray to be reoriented\n    res : tuple\n        Two-tuple (int, int) which includes x and y resolutions (aka \"grid/cell\n        sizes\"), respectively.\n    layer : int\n        Index of the raster layer to be reoriented (optional)\n\n    Returns\n    -------\n    array : numpy.ndarray\n        Reoriented 2d NumPy ndarray\n    \"\"\"\n    if res is None:\n        res = calc_res(raster)\n    array = raster.data\n    if layer is not None:\n        array = array[layer - 1]\n    r0zero = np.timedelta64(0, 'ns') if isinstance(res[0], np.timedelta64\n        ) else 0\n    r1zero = np.timedelta64(0, 'ns') if isinstance(res[1], np.timedelta64\n        ) else 0\n    xflip = res[0] < r0zero\n    yflip = res[1] > r1zero\n    array = _flip_array(array, xflip, yflip)\n    return array", "test_code_list": [{"test_code": "import numpy as np\nfrom xarray import DataArray\nfrom datashader.datashape import dshape\nfrom datashader.utils import Dispatcher\nfrom datashader.utils import apply\nfrom datashader.utils import calc_res\nfrom datashader.utils import isreal\nfrom datashader.utils import orient_array\ndef test_orient_array():\n    x = [5, 7]\n    y = [0, 1]\n    z = np.array([[0, 1], [2, 3]])\n    dims = 'y', 'x'\n    xarr = DataArray(z, coords=dict(x=x, y=y), dims=dims)\n    arr = orient_array(xarr)\n    assert np.array_equal(arr, z)\n    xarr = DataArray(z, coords=dict(x=x, y=y[::-1]), dims=dims)\n    arr = orient_array(xarr)\n    assert np.array_equal(arr, z[::-1])\n    xarr = DataArray(z, coords=dict(x=x[::-1], y=y), dims=dims)\n    arr = orient_array(xarr)\n    assert np.array_equal(arr, z[:, ::-1])\n    xarr = DataArray(z, coords=dict(x=x[::-1], y=y[::-1]), dims=dims)\n    arr = orient_array(xarr)\n    assert np.array_equal(arr, z[::-1, ::-1])\n\ntest_orient_array()\n", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_utils.py"}], "instruction": "Functionality: The orient_array function reorients an xarray DataArray to a canonical orientation based on the positivity or negativity of the x and y-resolution values. It optionally selects a specific layer from a multi-layered raster for reorientation.\n\nInputs:\n- raster: An xarray DataArray object that represents the raster data to be reoriented.\n- res: A two-element tuple (int, int) representing the x and y resolutions (grid/cell sizes), respectively. If not provided, the function calculates the resolutions automatically.\n- layer: An optional integer specifying the index of the raster layer to be reoriented. If not provided, the entire raster is processed.\n\nOutputs:\n- array: A 2D NumPy ndarray object that is the reoriented version of the input raster. The orientation is adjusted according to the positivity or negativity of the provided or calculated x and y resolutions.", "method_code_mask": "from __future__ import annotations\nimport os\nimport re\nfrom inspect import getmro\nimport numba as nb\nimport numpy as np\nimport pandas as pd\nfrom toolz import memoize\nfrom xarray import DataArray\nimport dask.dataframe as dd\nimport datashader.datashape as datashape\nfrom datashader.datatypes import RaggedDtype\nfrom geopandas.array import GeometryDtype as gpd_GeometryDtype\nfrom datashader.transfer_functions import set_background\n\n\ndef orient_array(raster, res=None, layer=None): [MASK]\n"}
{"method_name": "expand_varargs", "full_method_name": "expand_varargs", "method_path": "../srcdata/Visualization/datashader/datashader/macros.py", "method_code": "import re\nimport copy\nimport inspect\nimport ast\nimport textwrap\nimport astor\ndef expand_varargs(expand_number):\n    \"\"\"\n    Decorator to expand the variable length (starred) argument in a function\n    signature with a fixed number of arguments.\n\n    Parameters\n    ----------\n    expand_number: int\n        The number of fixed arguments that should replace the variable length\n        argument\n\n    Returns\n    -------\n    function\n        Decorator Function\n    \"\"\"\n    if not isinstance(expand_number, int) or expand_number < 0:\n        raise ValueError('expand_number must be a non-negative integer')\n\n    def _expand_varargs(fn):\n        fn_ast = function_to_ast(fn)\n        fn_expanded_ast = expand_function_ast_varargs(fn_ast, expand_number)\n        return function_ast_to_function(fn_expanded_ast, stacklevel=2)\n    return _expand_varargs", "test_code_list": [{"test_code": "import warnings\nimport pytest\nfrom datashader.macros import expand_varargs\nimport inspect\nfrom numba import jit\ndef function_no_vararg(a, b):\n    return a + b\ndef test_invalid_expand_number():\n    with pytest.raises(ValueError) as e:\n        expand_varargs(function_no_vararg)\n    assert e.match('non\\\\-negative integer')\ntest_invalid_expand_number()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_macros.py"}], "instruction": "Functionality: The expand_varargs function is a decorator designed to modify the behavior of functions that accept variable-length arguments (also known as starred arguments). It converts the variable-length argument into a fixed number of arguments as specified by the decorator's parameter. This is particularly useful for testing or debugging scenarios where you want to ensure a function is called with a consistent number of arguments, or when you need to replicate consistent test conditions.\n\nInputs: \n- expand_number: int\n    This is the only input parameter for the expand_varargs decorator. It specifies the exact number of fixed arguments that should replace the variable-length argument in the function's signature. It must be a non-negative integer.\n\nOutputs: \n- function\n    The output of the expand_varargs decorator is a modified version of the original function. The modified function no longer accepts a variable number of arguments; instead, it expects a fixed number of arguments specified by the expand_number parameter. This decorated function can be called just like the original function, but it will behave as if it were designed to accept exactly expand_number arguments.", "method_code_mask": "import re\nimport copy\nimport inspect\nimport ast\nimport textwrap\nimport astor\n\n\ndef expand_varargs(expand_number): [MASK]\n"}
{"method_name": "calculate_zoom_level_stats", "full_method_name": "calculate_zoom_level_stats", "method_path": "../srcdata/Visualization/datashader/datashader/tiles.py", "method_code": "from __future__ import annotations\nfrom io import BytesIO\nimport math\nimport os\nimport dask\nimport dask.bag as db\nimport numpy as np\nfrom PIL.Image import fromarray\nimport errno\nfrom bokeh.plotting import figure\nfrom bokeh.models.tiles import WMTSTileSource\nfrom bokeh.io import output_file\nfrom bokeh.io import save\nfrom os import path\nimport boto3\nfrom urllib.parse import urlparse\ndef calculate_zoom_level_stats(super_tiles, load_data_func, rasterize_func,\n    color_ranging_strategy='fullscan'):\n    if color_ranging_strategy == 'fullscan':\n        stats = []\n        is_bool = False\n        for super_tile in super_tiles:\n            agg = _get_super_tile_min_max(super_tile, load_data_func,\n                rasterize_func)\n            super_tile['agg'] = agg\n            if agg.dtype.kind == 'b':\n                is_bool = True\n            else:\n                stats.append(np.nanmin(agg.data))\n                stats.append(np.nanmax(agg.data))\n        if is_bool:\n            span = 0, 1\n        else:\n            b = db.from_sequence(stats)\n            span = dask.compute(b.min(), b.max())\n        return super_tiles, span\n    else:\n        raise ValueError('Invalid color_ranging_strategy option')", "test_code_list": [{"test_code": "import datashader as ds\nimport datashader.transfer_functions as tf\nfrom datashader.colors import viridis\nfrom datashader.tiles import render_tiles\nfrom datashader.tiles import gen_super_tiles\nfrom datashader.tiles import _get_super_tile_min_max\nfrom datashader.tiles import calculate_zoom_level_stats\nfrom datashader.tiles import MercatorTileDefinition\nimport numpy as np\nimport pandas as pd\nfrom PIL import ImageDraw\ndef assert_is_numeric(value):\n    is_int_or_float = isinstance(value, (int, float))\n    type_name = type(value).__name__\n    is_numpy_int_or_float = 'int' in type_name or 'float' in type_name\n    assert any([is_int_or_float, is_numpy_int_or_float])\ndf = None\ndef mock_rasterize_func(df, x_range, y_range, height, width):\n    cvs = ds.Canvas(x_range=x_range, y_range=y_range, plot_height=height,\n        plot_width=width)\n    agg = cvs.points(df, 'x', 'y')\n    return agg\ndef mock_load_data_func(x_range, y_range):\n    global df\n    if df is None:\n        xs = np.random.normal(loc=0, scale=500000, size=10000000)\n        ys = np.random.normal(loc=0, scale=500000, size=10000000)\n        df = pd.DataFrame(dict(x=xs, y=ys))\n    return df.loc[df['x'].between(*x_range) & df['y'].between(*y_range)]\nMERCATOR_CONST = 20037508.34\ndef test_calculate_zoom_level_stats_with_fullscan_ranging_strategy():\n    full_extent = (-MERCATOR_CONST, -MERCATOR_CONST, MERCATOR_CONST,\n        MERCATOR_CONST)\n    level = 0\n    color_ranging_strategy = 'fullscan'\n    super_tiles, span = calculate_zoom_level_stats(list(gen_super_tiles(\n        full_extent, level)), mock_load_data_func, mock_rasterize_func,\n        color_ranging_strategy=color_ranging_strategy)\n    assert isinstance(span, (list, tuple))\n    assert len(span) == 2\n    assert_is_numeric(span[0])\n    assert_is_numeric(span[1])\ntest_calculate_zoom_level_stats_with_fullscan_ranging_strategy()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_tiles.py"}], "instruction": "Functionality: The calculate_zoom_level_stats function is designed to compute statistics (min and max) for rasterized data across a series of super-tiles, using a specified color ranging strategy. This function is particularly useful for generating visualizations or analyzing large-scale raster data where each super-tile represents a section of the overall data set. The function supports different color ranging strategies, such as 'fullscan', which will scan all data to determine the color range.\n\nInputs: \n- super_tiles: A list of dictionaries, each representing a super-tile with its metadata and potentially preprocessed data. The function will modify this list by adding an 'agg' key with the aggregated data for each super-tile.\n- load_data_func: A function that takes a super-tile as input and returns the data associated with it. This function is used to load data as needed for processing.\n- rasterize_func: A function that takes the data of a super-tile as input and returns a rasterized representation of that data. This is used to convert the data into a format that can be aggregated and analyzed.\n- color_ranging_strategy: A string specifying the strategy to use for determining the color range of the data. Currently, only 'fullscan' is supported, which computes the min and max values across all super-tiles.\n\nOutputs:\n- A tuple containing two elements:\n  1. A modified list of super-tiles, where each super-tile dictionary now includes an 'agg' key with the aggregated data.\n  2. A tuple representing the span of the color range as determined by the color ranging strategy. This is (min_value, max_value) for the aggregated data across all super-tiles. If the data is boolean, the span is always (0, 1).", "method_code_mask": "from __future__ import annotations\nfrom io import BytesIO\nimport math\nimport os\nimport dask\nimport dask.bag as db\nimport numpy as np\nfrom PIL.Image import fromarray\nimport errno\nfrom bokeh.plotting import figure\nfrom bokeh.models.tiles import WMTSTileSource\nfrom bokeh.io import output_file\nfrom bokeh.io import save\nfrom os import path\nimport boto3\nfrom urllib.parse import urlparse\n\n\ndef calculate_zoom_level_stats(super_tiles, load_data_func, rasterize_func,\n    color_ranging_strategy='fullscan'): [MASK]\n"}
{"method_name": "gen_super_tiles", "full_method_name": "gen_super_tiles", "method_path": "../srcdata/Visualization/datashader/datashader/tiles.py", "method_code": "from __future__ import annotations\nfrom io import BytesIO\nimport math\nimport os\nimport dask\nimport dask.bag as db\nimport numpy as np\nfrom PIL.Image import fromarray\nimport errno\nfrom bokeh.plotting import figure\nfrom bokeh.models.tiles import WMTSTileSource\nfrom bokeh.io import output_file\nfrom bokeh.io import save\nfrom os import path\nimport boto3\nfrom urllib.parse import urlparse\ndef gen_super_tiles(extent, zoom_level, span=None):\n    xmin, ymin, xmax, ymax = extent\n    super_tile_size = min(2 ** 4 * 256, 2 ** zoom_level * 256)\n    super_tile_def = MercatorTileDefinition(x_range=(xmin, xmax), y_range=(\n        ymin, ymax), tile_size=super_tile_size)\n    super_tiles = super_tile_def.get_tiles_by_extent(extent, zoom_level)\n    for s in super_tiles:\n        st_extent = s[3]\n        x_range = st_extent[0], st_extent[2]\n        y_range = st_extent[1], st_extent[3]\n        yield {'level': zoom_level, 'x_range': x_range, 'y_range': y_range,\n            'tile_size': super_tile_def.tile_size, 'span': span}", "test_code_list": [{"test_code": "import datashader as ds\nimport datashader.transfer_functions as tf\nfrom datashader.colors import viridis\nfrom datashader.tiles import render_tiles\nfrom datashader.tiles import gen_super_tiles\nfrom datashader.tiles import _get_super_tile_min_max\nfrom datashader.tiles import calculate_zoom_level_stats\nfrom datashader.tiles import MercatorTileDefinition\nimport numpy as np\nimport pandas as pd\nfrom PIL import ImageDraw\ndef assert_is_numeric(value):\n    is_int_or_float = isinstance(value, (int, float))\n    type_name = type(value).__name__\n    is_numpy_int_or_float = 'int' in type_name or 'float' in type_name\n    assert any([is_int_or_float, is_numpy_int_or_float])\ndf = None\ndef mock_rasterize_func(df, x_range, y_range, height, width):\n    cvs = ds.Canvas(x_range=x_range, y_range=y_range, plot_height=height,\n        plot_width=width)\n    agg = cvs.points(df, 'x', 'y')\n    return agg\ndef mock_load_data_func(x_range, y_range):\n    global df\n    if df is None:\n        xs = np.random.normal(loc=0, scale=500000, size=10000000)\n        ys = np.random.normal(loc=0, scale=500000, size=10000000)\n        df = pd.DataFrame(dict(x=xs, y=ys))\n    return df.loc[df['x'].between(*x_range) & df['y'].between(*y_range)]\nMERCATOR_CONST = 20037508.34\ndef test_calculate_zoom_level_stats_with_fullscan_ranging_strategy():\n    full_extent = (-MERCATOR_CONST, -MERCATOR_CONST, MERCATOR_CONST,\n        MERCATOR_CONST)\n    level = 0\n    color_ranging_strategy = 'fullscan'\n    super_tiles, span = calculate_zoom_level_stats(list(gen_super_tiles(\n        full_extent, level)), mock_load_data_func, mock_rasterize_func,\n        color_ranging_strategy=color_ranging_strategy)\n    assert isinstance(span, (list, tuple))\n    assert len(span) == 2\n    assert_is_numeric(span[0])\n    assert_is_numeric(span[1])\ntest_calculate_zoom_level_stats_with_fullscan_ranging_strategy()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_tiles.py"}], "instruction": "Functionality: The gen_super_tiles function generates super tiles at a specified zoom level for a given geographical extent. Super tiles are larger tiles that cover a specified extent at a given zoom level, which can be useful for optimizing the display of maps in applications. The function supports an optional parameter to specify the size of the super tile.\n\nInputs: \n- extent: A tuple containing the geographical extent as xmin, ymin, xmax, ymax.\n- zoom_level: An integer that represents the zoom level for the tiles. The zoom level determines the detail of the tiles.\n- span (optional): An integer that represents the size of the super tile. If not provided, the function will use a default value that is calculated based on the zoom level.\n\nOutputs: \n- A generator that yields dictionaries. Each dictionary contains the following keys:\n  - level: The zoom level of the super tile.\n  - x_range: A tuple containing the x-coordinate range of the super tile.\n  - y_range: A tuple containing the y-coordinate range of the super tile.\n  - tile_size: The size of the super tile.\n  - span: The optional size parameter provided to the function, or None if not provided.", "method_code_mask": "from __future__ import annotations\nfrom io import BytesIO\nimport math\nimport os\nimport dask\nimport dask.bag as db\nimport numpy as np\nfrom PIL.Image import fromarray\nimport errno\nfrom bokeh.plotting import figure\nfrom bokeh.models.tiles import WMTSTileSource\nfrom bokeh.io import output_file\nfrom bokeh.io import save\nfrom os import path\nimport boto3\nfrom urllib.parse import urlparse\n\n\ndef gen_super_tiles(extent, zoom_level, span=None): [MASK]\n"}
{"method_name": "draw_segment", "full_method_name": "draw_segment", "method_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py", "method_code": "from __future__ import annotations\nimport pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\nexpand_aggs_and_cols = Glyph._expand_aggs_and_cols(append, 1, False)\nmapper = ngjit(lambda x: x)\nmap_onto_pixel_for_line = _build_map_onto_pixel_for_line(mapper, mapper)\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\n_draw_segment = _build_draw_segment(append, map_onto_pixel_for_line,\n    expand_aggs_and_cols, 0, False)\ndef draw_segment(x0, y0, x1, y1, i, segment_start, agg):\n    \"\"\"\n    Helper to draw line with fixed bounds and scale values.\n    \"\"\"\n    sx, tx, sy, ty = 1, 0, 1, 0\n    xmin, xmax, ymin, ymax = 0, 5, 0, 5\n    buffer = np.empty(0)\n    _draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start,\n        False, x0, x1, y0, y1, 0.0, 0.0, buffer, agg)", "test_code_list": [{"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_line_same_point():\n    x0, y0 = 4, 4\n    x1, y1 = 4, 4\n    agg = new_agg()\n    draw_segment(x0, y0, x1, y1, 0, True, agg)\n    assert agg.sum() == 1\n    assert agg[4, 4] == 1\n    agg = new_agg()\n    draw_segment(x0, y0, x1, y1, 0, False, agg)\n    assert agg.sum() == 1\n    assert agg[4, 4] == 1\n    x0, y0 = 4, 4\n    x1, y1 = 10, 10\n    agg = new_agg()\n    draw_segment(x0, y0, x1, y1, 0, True, agg)\n    assert agg.sum() == 1\n    assert agg[4, 4] == 1\n    agg = new_agg()\n    draw_segment(x0, y0, x1, y1, 0, False, agg)\n    assert agg.sum() == 0\n    assert agg[4, 4] == 0\ntest_draw_line_same_point()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_line_vertical_horizontal():\n    x0, y0 = 3, 3\n    x1, y1 = 3, 0\n    agg = new_agg()\n    draw_segment(x0, y0, x1, y1, 0, True, agg)\n    out = new_agg()\n    out[:4, 3] = 1\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_segment(y0, x0, y1, x1, 0, True, agg)\n    out = new_agg()\n    out[3, :4] = 1\n    np.testing.assert_equal(agg, out)\ntest_draw_line_vertical_horizontal()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}], "instruction": "Functionality: The function 'draw_segment' is designed to draw a line segment on a specified 2D grid, represented by an aggregation buffer (agg). It operates by mapping the line segment's endpoints to pixel coordinates on the grid and then applying an accumulation function to draw the line.\n\nInputs: \n1. x0 (float): The x-coordinate of the start point of the line segment.\n2. y0 (float): The y-coordinate of the start point of the line segment.\n3. x1 (float): The x-coordinate of the end point of the line segment.\n4. y1 (float): The y-coordinate of the end point of the line segment.\n5. i (int): An index, typically used in loops for drawing multiple segments.\n6. segment_start (bool): A flag that indicates the start of a new segment.\n7. agg (numpy array): An aggregation buffer representing a 2D grid, where line drawing takes place.\n\nOutputs:\nThe function does not return any value. Instead, it modifies the 'agg' array in place by accumulating values on the pixels that the line segment covers.", "method_code_mask": "from __future__ import annotations\nimport pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\n\n\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\n\n\nexpand_aggs_and_cols = Glyph._expand_aggs_and_cols(append, 1, False)\nmapper = ngjit(lambda x: x)\nmap_onto_pixel_for_line = _build_map_onto_pixel_for_line(mapper, mapper)\n\n\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\n\n\n_draw_segment = _build_draw_segment(append, map_onto_pixel_for_line,\n    expand_aggs_and_cols, 0, False)\n\n\ndef draw_segment(x0, y0, x1, y1, i, segment_start, agg): [MASK]\n"}
{"method_name": "draw_trapezoid", "full_method_name": "draw_trapezoid", "method_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py", "method_code": "from __future__ import annotations\nimport pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\nexpand_aggs_and_cols = Glyph._expand_aggs_and_cols(append, 1, False)\nmapper = ngjit(lambda x: x)\nmap_onto_pixel_for_line = _build_map_onto_pixel_for_line(mapper, mapper)\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\n_draw_trapezoid = _build_draw_trapezoid_y(append, map_onto_pixel_for_line,\n    expand_aggs_and_cols)\ndef draw_trapezoid(x0, x1, y0, y1, y2, y3, i, trapezoid_start, stacked, agg):\n    \"\"\"\n    Helper to draw line with fixed bounds and scale values.\n    \"\"\"\n    sx, tx, sy, ty = 1, 0, 1, 0\n    xmin, xmax, ymin, ymax = 0, 5, 0, 5\n    _draw_trapezoid(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, x0, x1, y0,\n        y1, y2, y3, trapezoid_start, stacked, agg)", "test_code_list": [{"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_acute_not_stacked():\n    x0, x1 = 0, 3\n    y0, y1, y2, y3 = 1, 3, 4, 0\n    out = np.array([[0, 0, 1, 1, 0], [1, 1, 1, 1, 0], [1, 1, 1, 1, 0], [1,\n        1, 1, 1, 0], [0, 0, 1, 1, 0]])\n    trapezoid_start = True\n    stacked = False\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_acute_not_stacked()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_right():\n    x0, x1 = 0, 3\n    y0, y1, y2, y3 = 1, 3, 4, 1\n    out = np.array([[0, 0, 0, 0, 0], [1, 1, 1, 1, 0], [1, 1, 1, 1, 0], [0,\n        0, 1, 1, 0], [0, 0, 0, 0, 0]])\n    trapezoid_start = True\n    stacked = True\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_right()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_obtuse():\n    x0, x1 = 0, 3\n    y0, y1, y2, y3 = 0, 3, 5, 1\n    out = np.array([[1, 1, 0, 0, 0], [1, 1, 1, 1, 0], [1, 1, 1, 1, 0], [0,\n        0, 1, 1, 0], [0, 0, 0, 0, 0]])\n    trapezoid_start = True\n    stacked = True\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_obtuse()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_intersecting():\n    x0, x1 = 0, 3\n    y0, y1, y2, y3 = 0, 5, 1, 4\n    out = np.array([[1, 0, 0, 0, 0], [1, 1, 0, 0, 0], [1, 1, 0, 1, 0], [1,\n        0, 1, 1, 0], [0, 0, 0, 1, 0]])\n    trapezoid_start = True\n    stacked = True\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_intersecting()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_vertical_line_start_and_not_clipped():\n    x0, x1 = 2, 2\n    y0, y1, y2, y3 = 1, 3, 4, 0\n    out = np.array([[0, 0, 1, 0, 0], [0, 0, 2, 0, 0], [0, 0, 2, 0, 0], [0,\n        0, 1, 0, 0], [0, 0, 0, 0, 0]])\n    trapezoid_start = True\n    stacked = True\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_vertical_line_start_and_not_clipped()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_vertical_line_not_start_and_not_clipped():\n    x0, x1 = 2, 2\n    y0, y1, y2, y3 = 1, 3, 4, 0\n    trapezoid_start = False\n    stacked = True\n    out = np.array([[0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0,\n        0, 1, 0, 0], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    out = np.array([[0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0,\n        0, 0, 0, 0], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_vertical_line_not_start_and_not_clipped()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_clipped():\n    x0, x1 = 4, 6\n    y0, y1, y2, y3 = 1, 3, 5, 0\n    trapezoid_start = True\n    stacked = True\n    out = np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0,\n        0, 0, 0, 0], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    out = np.array([[0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0,\n        0, 0, 0, 1], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_clipped()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_vertical_line_not_start_and_clipped():\n    x0, x1 = 4, 6\n    y0, y1, y2, y3 = 1, 3, 4, 0\n    trapezoid_start = False\n    stacked = True\n    out = np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0,\n        0, 0, 0, 0], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_vertical_line_not_start_and_clipped()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_horizontal_line():\n    x0, x1 = 0, 3\n    y0, y1, y2, y3 = 2, 2, 2, 2\n    trapezoid_start = True\n    stacked = False\n    out = np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 1, 1, 1, 0], [0,\n        0, 0, 0, 0], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    stacked = True\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg.sum(), 0)\ntest_draw_trapezoid_horizontal_line()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_diagonal_line():\n    x0, x1 = 0, 3\n    y0, y1, y2, y3 = 0, 0, 2, 2\n    trapezoid_start = True\n    stacked = False\n    out = np.array([[1, 0, 0, 0, 0], [0, 1, 1, 0, 0], [0, 0, 0, 1, 0], [0,\n        0, 0, 0, 0], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    stacked = True\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg.sum(), 0)\ntest_draw_trapezoid_diagonal_line()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}, {"test_code": "import pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\ndef new_agg():\n    return np.zeros((5, 5), dtype='i4')\ndef test_draw_trapezoid_point():\n    x0, x1 = 3, 3\n    y0, y1, y2, y3 = 2, 2, 2, 2\n    trapezoid_start = True\n    stacked = False\n    out = np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 2, 0], [0,\n        0, 0, 0, 0], [0, 0, 0, 0, 0]])\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    trapezoid_start = False\n    out[2, 3] = 1\n    agg = new_agg()\n    draw_trapezoid(x0, x1, y0, y1, y2, y3, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\n    trapezoid_start = True\n    stacked = True\n    out[2, 3] = 0\n    agg = new_agg()\n    draw_trapezoid(x1, x0, y3, y2, y1, y0, 0, trapezoid_start, stacked, agg)\n    np.testing.assert_equal(agg, out)\ntest_draw_trapezoid_point()", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_glyphs.py"}], "instruction": "Functionality: The function 'draw_trapezoid' is designed to render a trapezoid onto an accumulation grid (agg), which is typically used in the context of visual data rendering, such as in datashader. This function handles the geometry of the trapezoid, mapping it to the pixel coordinates on the grid, and incrementing the aggregated value at the computed pixel positions. The main purpose is to facilitate the creation of visual images from data, particularly in scenarios where large datasets are involved, and high-performance visualization is required.\n\nInputs:\n- x0: The x-coordinate of the first vertex of the trapezoid.\n- x1: The x-coordinate of the second vertex of the trapezoid.\n- y0: The y-coordinate of the first vertex of the trapezoid.\n- y1: The y-coordinate of the second vertex of the trapezoid.\n- y2: The y-coordinate of the third vertex of the trapezoid.\n- y3: The y-coordinate of the fourth vertex of the trapezoid.\n- i: An index, usually related to the current data point being processed.\n- trapezoid_start: A boolean indicating whether this is the start of a new trapezoid.\n- stacked: A boolean indicating if the trapezoid should be stacked on top of existing data.\n- agg: The accumulation grid onto which the trapezoid will be drawn. This is typically a 2D array or a similar structure that aggregates visual information.\n\nOutputs:\n- None: The function modifies the 'agg' parameter in place, incrementing the values at the pixel positions where the trapezoid is drawn. There is no return value from the function.", "method_code_mask": "from __future__ import annotations\nimport pandas as pd\nimport numpy as np\nimport pytest\nfrom datashader.datashape import dshape\nfrom datashader.glyphs import Point\nfrom datashader.glyphs import LinesAxis1\nfrom datashader.glyphs import Glyph\nfrom datashader.glyphs.area import _build_draw_trapezoid_y\nfrom datashader.glyphs.line import _build_map_onto_pixel_for_line\nfrom datashader.glyphs.line import _build_draw_segment\nfrom datashader.glyphs.line import _build_extend_line_axis0\nfrom datashader.glyphs.trimesh import _build_map_onto_pixel_for_triangle\nfrom datashader.glyphs.trimesh import _build_draw_triangle\nfrom datashader.glyphs.trimesh import _build_extend_triangles\nfrom datashader.utils import ngjit\n\n\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\n\n\nexpand_aggs_and_cols = Glyph._expand_aggs_and_cols(append, 1, False)\nmapper = ngjit(lambda x: x)\nmap_onto_pixel_for_line = _build_map_onto_pixel_for_line(mapper, mapper)\n\n\n@ngjit\ndef append(i, x, y, agg):\n    agg[y, x] += 1\n\n\n_draw_trapezoid = _build_draw_trapezoid_y(append, map_onto_pixel_for_line,\n    expand_aggs_and_cols)\n\n\ndef draw_trapezoid(x0, x1, y0, y1, y2, y3, i, trapezoid_start, stacked, agg): [\n    MASK]\n"}
{"method_name": "compute_chunksize", "full_method_name": "compute_chunksize", "method_path": "../srcdata/Visualization/datashader/datashader/resampling.py", "method_code": "from __future__ import annotations\nfrom itertools import groupby\nfrom math import floor\nfrom math import ceil\nimport dask.array as da\nimport numpy as np\nfrom dask.delayed import delayed\nfrom numba import prange\ndef compute_chunksize(src, w, h, chunksize=None, max_mem=None):\n    \"\"\"\n    Attempts to compute a chunksize for the resampling output array\n    that is as close as possible to the input array chunksize, while\n    also respecting the maximum memory constraint to avoid loading\n    to much data into memory at the same time.\n\n    Parameters\n    ----------\n    src : dask.array.Array\n        The source array to resample\n    w : int\n        New grid width\n    h : int\n        New grid height\n    chunksize : tuple(int, int) (optional)\n        Size of the output chunks. By default the chunk size is\n        inherited from the *src* array.\n    max_mem : int (optional)\n        The maximum number of bytes that should be loaded into memory\n        during the regridding operation.\n\n    Returns\n    -------\n    chunksize : tuple(int, int)\n        Size of the output chunks.\n    \"\"\"\n    start_chunksize = src.chunksize if chunksize is None else chunksize\n    if max_mem is None:\n        return start_chunksize\n    sh, sw = src.shape\n    height_fraction = float(sh) / h\n    width_fraction = float(sw) / w\n    ch, cw = start_chunksize\n    dim = True\n    nbytes = src.dtype.itemsize\n    while ch * height_fraction * (cw * width_fraction) * nbytes > max_mem:\n        if dim:\n            cw -= 1\n        else:\n            ch -= 1\n        dim = not dim\n    if ch == 0 or cw == 0:\n        min_mem = height_fraction * width_fraction * nbytes\n        raise ValueError(\n            'Given the memory constraints the resampling operation could not find a chunksize that avoids loading too much data into memory. Either relax the memory constraint to a minimum of %d bytes or resample to a larger grid size. Note: A future implementation could handle this condition by declaring temporary arrays.'\n             % min_mem)\n    return ch, cw", "test_code_list": [{"test_code": "import pytest\nfrom dask.context import config\nfrom os import path\nfrom itertools import product\nimport datashader as ds\nimport xarray as xr\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nfrom datashader.resampling import compute_chunksize\nimport datashader.transfer_functions as tf\nfrom packaging.version import Version\ndef test_resample_compute_chunksize():\n    \"\"\"\n    Ensure chunksize computation is correct.\n    \"\"\"\n    darr = da.from_array(np.zeros((100, 100)), (10, 10))\n    mem_limited_chunksize = compute_chunksize(darr, 10, 10, max_mem=2000)\n    assert mem_limited_chunksize == (2, 1)\n    explicit_chunksize = compute_chunksize(darr, 10, 10, chunksize=(5, 4))\n    assert explicit_chunksize == (5, 4)\n\ntest_resample_compute_chunksize()\n", "code_start": "from __future__ import annotations\n", "test_path": "../srcdata/Visualization/datashader/datashader/tests/test_raster.py"}], "instruction": "Functionality: \nThe compute_chunksize function calculates an optimal chunk size for the output array of a resampling operation, aiming to keep it as close as possible to the input array's chunk size while adhering to a specified maximum memory constraint. This is crucial to prevent excessive data loading into memory concurrently.\n\nInputs:\n- src: A dask.array.Array object representing the source array to be resampled.\n- w: An integer representing the new grid width for the resampled output array.\n- h: An integer representing the new grid height for the resampled output array.\n- chunksize (optional): A tuple of integers indicating the desired size of the output chunks. If not provided, the chunksize is inherited from the source array.\n- max_mem (optional): An integer specifying the maximum number of bytes that should be loaded into memory during the resampling operation.\n\nOutputs:\n- chunksize: A tuple of integers representing the calculated size of the output chunks for the resampled array.", "method_code_mask": "from __future__ import annotations\nfrom itertools import groupby\nfrom math import floor\nfrom math import ceil\nimport dask.array as da\nimport numpy as np\nfrom dask.delayed import delayed\nfrom numba import prange\n\n\ndef compute_chunksize(src, w, h, chunksize=None, max_mem=None): [MASK]\n"}
{"method_name": "lex", "full_method_name": "lex", "method_path": "../srcdata/Visualization/datashader/datashader/datashape/lexer.py", "method_code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport re\nimport ast\nimport collections\nToken = collections.namedtuple('Token', 'id, name, span, val')\ndef _str_val(s):\n    return ast.parse('u' + s).body[0].value.value\n_tokens = [('BOOLEAN', 'True|False', ast.literal_eval), ('NAME_LOWER',\n    '[a-z][a-zA-Z0-9_]*', lambda x: x), ('NAME_UPPER', '[A-Z][a-zA-Z0-9_]*',\n    lambda x: x), ('NAME_OTHER', '_[a-zA-Z0-9_]*', lambda x: x), (\n    'ASTERISK', '\\\\*'), ('COMMA', ','), ('EQUAL', '='), ('COLON', ':'), (\n    'LBRACKET', '\\\\['), ('RBRACKET', '\\\\]'), ('LBRACE', '\\\\{'), ('RBRACE',\n    '\\\\}'), ('LPAREN', '\\\\('), ('RPAREN', '\\\\)'), ('ELLIPSIS', '\\\\.\\\\.\\\\.'),\n    ('RARROW', '->'), ('QUESTIONMARK', '\\\\?'), ('INTEGER',\n    '0(?![0-9])|-?[1-9][0-9]*', int), ('STRING',\n    '(?:\"(?:[^\"\\\\n\\\\r\\\\\\\\]|(?:\\\\\\\\u[0-9a-fA-F]{4})|(?:\\\\\\\\[\"bfnrt]))*\")|' +\n    \"(?:'(?:[^'\\\\n\\\\r\\\\\\\\]|(?:\\\\\\\\u[0-9a-fA-F]{4})|(?:\\\\\\\\['bfnrt]))*')\",\n    _str_val)]\n_tokens_re = re.compile('|'.join('(' + tok[1] + ')' for tok in _tokens), re\n    .MULTILINE)\n_whitespace = '(?:\\\\s|(?:#.*$))*'\n_whitespace_re = re.compile(_whitespace, re.MULTILINE)\ndef lex(ds_str):\n    \"\"\"A generator which lexes a datashape string into a\n    sequence of tokens.\n    Example\n    -------\n        import datashape\n        s = '   -> ... A... \"string\" 1234 Blah _eil(# comment'\n        print('lexing %r' % s)\n        for tok in datashape.lexer.lex(s):\n            print(tok.id, tok.name, tok.span, repr(tok.val))\n    \"\"\"\n    pos = 0\n    m = _whitespace_re.match(ds_str, pos)\n    if m:\n        pos = m.end()\n    while pos < len(ds_str):\n        m = _tokens_re.match(ds_str, pos)\n        if m:\n            id = m.lastindex\n            tokinfo = _tokens[id - 1]\n            name = tokinfo[0]\n            span = m.span()\n            if len(tokinfo) > 2:\n                val = tokinfo[2](ds_str[span[0]:span[1]])\n            else:\n                val = None\n            pos = m.end()\n            yield Token(id, name, span, val)\n        else:\n            raise error.DataShapeSyntaxError(pos, '<nofile>', ds_str,\n                'Invalid DataShape token')\n        m = _whitespace_re.match(ds_str, pos)\n        if m:\n            pos = m.end()", "test_code_list": [{"test_code": "import unittest\nfrom datashader import datashape\nfrom datashader.datashape import lexer\n\nclass TestDataShapeLexer(unittest.TestCase):\n    def test_whitespace(self):\n        expected_idval = [(lexer.COLON, None), (lexer.STRING, 'a'), (lexer.\n            INTEGER, 12345), (lexer.RARROW, None), (lexer.EQUAL, None), (lexer.\n            ASTERISK, None), (lexer.NAME_OTHER, '_b')]\n        toks = list(lex(':\"a\"12345->=*_b'))\n        self.assertEqual([(tok.id, tok.val) for tok in toks], expected_idval)\n        toks = list(lex(' : \"a\" 12345 -> = * _b '))\n        self.assertEqual([(tok.id, tok.val) for tok in toks], expected_idval)\n        toks = list(lex('\\t:\\t\"a\"\\t12345\\t->\\t=\\t*\\t_b\\t'))\n        self.assertEqual([(tok.id, tok.val) for tok in toks], expected_idval)\n        toks = list(lex('\\n:\\n\"a\"\\n12345\\n->\\n=\\n*\\n_b\\n'))\n        self.assertEqual([(tok.id, tok.val) for tok in toks], expected_idval)\n        toks = list(lex('# comment\\n' + ': # X\\n' + ' \"a\" # \"b\"\\t\\n' +\n            '\\t12345\\n\\n' + '->\\n' + '=\\n' + '*\\n' + '_b # comment\\n' +\n            ' \\t # end'))\n        self.assertEqual([(tok.id, tok.val) for tok in toks], expected_idval)\n    \nTestDataShapeLexer().test_whitespace()\n", "code_start": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_lexer.py"}], "instruction": "Functionality: The lex function is a generator that tokenizes a given datashape string. It scans the string, identifies various datashape tokens such as boolean values, variable names, operators, brackets, and numbers, and yields a Token namedtuple with information about each token found.\n\nInputs: \n- ds_str: A string representing a datashape expression. This string can contain various datashape components such as variables, literals, and operators, which need to be tokenized.\n\nOutputs: \n- Yields a Token object for each token found in the input datashape string. The Token object is a namedtuple with the following fields:\n  - id: An integer representing the type of the token.\n  - name: A string representing the name of the token.\n  - span: A tuple containing the start and end positions of the token in the input string.\n  - val: The value associated with the token, which can be a string, integer, or other type depending on the token.", "method_code_mask": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport re\nimport ast\nimport collections\nToken = collections.namedtuple('Token', 'id, name, span, val')\n\n\ndef _str_val(s):\n    return ast.parse('u' + s).body[0].value.value\n\n\n_tokens = [('BOOLEAN', 'True|False', ast.literal_eval), ('NAME_LOWER',\n    '[a-z][a-zA-Z0-9_]*', lambda x: x), ('NAME_UPPER', '[A-Z][a-zA-Z0-9_]*',\n    lambda x: x), ('NAME_OTHER', '_[a-zA-Z0-9_]*', lambda x: x), (\n    'ASTERISK', '\\\\*'), ('COMMA', ','), ('EQUAL', '='), ('COLON', ':'), (\n    'LBRACKET', '\\\\['), ('RBRACKET', '\\\\]'), ('LBRACE', '\\\\{'), ('RBRACE',\n    '\\\\}'), ('LPAREN', '\\\\('), ('RPAREN', '\\\\)'), ('ELLIPSIS', '\\\\.\\\\.\\\\.'),\n    ('RARROW', '->'), ('QUESTIONMARK', '\\\\?'), ('INTEGER',\n    '0(?![0-9])|-?[1-9][0-9]*', int), ('STRING', \n    '(?:\"(?:[^\"\\\\n\\\\r\\\\\\\\]|(?:\\\\\\\\u[0-9a-fA-F]{4})|(?:\\\\\\\\[\"bfnrt]))*\")|' +\n    \"(?:'(?:[^'\\\\n\\\\r\\\\\\\\]|(?:\\\\\\\\u[0-9a-fA-F]{4})|(?:\\\\\\\\['bfnrt]))*')\",\n    _str_val)]\n_tokens_re = re.compile('|'.join('(' + tok[1] + ')' for tok in _tokens), re\n    .MULTILINE)\n_whitespace = '(?:\\\\s|(?:#.*$))*'\n_whitespace_re = re.compile(_whitespace, re.MULTILINE)\n\n\ndef lex(ds_str): [MASK]\n"}
{"method_name": "optionify", "full_method_name": "optionify", "method_path": "../srcdata/Visualization/datashader/datashader/datashape/promote.py", "method_code": "from __future__ import absolute_import\nimport numpy as np\nfrom datashader import datashape\ndef optionify(lhs, rhs, dshape):\n    \"\"\"Check whether a binary operation's dshape came from\n    :class:`~datashape.coretypes.Option` typed operands and construct an\n    :class:`~datashape.coretypes.Option` type accordingly.\n\n    Examples\n    --------\n    >>> from datashader.datashape import int32, int64, Option\n    >>> x = Option(int32)\n    >>> x\n    Option(ty=ctype(\"int32\"))\n    >>> y = int64\n    >>> y\n    ctype(\"int64\")\n    >>> optionify(x, y, int64)\n    Option(ty=ctype(\"int64\"))\n    \"\"\"\n    if hasattr(dshape.measure, 'ty'):\n        return dshape\n    if hasattr(lhs, 'ty') or hasattr(rhs, 'ty'):\n        return datashape.Option(dshape)\n    return dshape", "test_code_list": [{"test_code": "import pytest\nfrom datashader.datashape import promote\nfrom datashader.datashape import Option\nfrom datashader.datashape import float64\nfrom datashader.datashape import int64\nfrom datashader.datashape import float32\nfrom datashader.datashape import optionify\nfrom datashader.datashape import string\nfrom datashader.datashape import datetime_ as datetime\nfrom datashader.datashape import dshape\ndef test_option_in_parent():\n    x = int64\n    y = Option(float32)\n    z = optionify(x, y, y)\n    assert z == y\n\ntest_option_in_parent()\n", "code_start": "", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_promote.py"}], "instruction": "Functionality: The function 'optionify' is designed to determine if a binary operation involving two operands results in a data type that is encapsulated within an Option type. If one or both of the operands (lhs, rhs) are of type 'Option', or if the data shape (dshape) of the result is from an 'Option' type, the function returns a modified data shape that reflects this. If no 'Option' types are involved, it returns the original data shape.\n\nInputs: \n- lhs: The left-hand side operand of the binary operation.\n- rhs: The right-hand side operand of the binary operation.\n- dshape: The data shape of the resulting type from the binary operation. This is an instance of the 'datashape' module's data shape representation.\n\nOutputs: \n- The function returns a data shape. If the operation involves 'Option' types or results in a type that could be encapsulated within an 'Option' type, it returns a data shape of type 'Option'. Otherwise, it returns the original data shape.", "method_code_mask": "from __future__ import absolute_import\nimport numpy as np\nfrom datashader import datashape\n\n\ndef optionify(lhs, rhs, dshape): [MASK]\n"}
{"method_name": "parse", "full_method_name": "parse", "method_path": "../srcdata/Visualization/datashader/datashader/datashape/parser.py", "method_code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\ndef parse(ds_str, sym):\n    \"\"\"Parses a single datashape from a string.\n\n    Parameters\n    ----------\n    ds_str : string\n        The datashape string to parse.\n    sym : TypeSymbolTable\n        The symbol tables of dimensions, dtypes, and type constructors for each.\n\n    \"\"\"\n    dsp = DataShapeParser(ds_str, sym)\n    ds = dsp.parse_datashape()\n    if ds is None:\n        dsp.raise_error('Invalid datashape')\n    if dsp.pos != dsp.end_pos:\n        dsp.raise_error('Unexpected token in datashape')\n    return ds", "test_code_list": [{"test_code": "import unittest\nimport pytest\nfrom datashader import datashape\nfrom datashader.datashape.util.testing import assert_dshape_equal\nfrom datashader.datashape.parser import parse\nfrom datashader.datashape import coretypes as ct\nfrom datashader.datashape import DataShapeSyntaxError\n\nclass TestDataShapeParserDTypeConstr(unittest.TestCase):\n    def test_unary_dtype_constr(self):\n        sym = datashape.TypeSymbolTable(bare=True)\n        sym.dtype['int8'] = ct.int8\n        sym.dtype['uint16'] = ct.uint16\n        sym.dtype['float64'] = ct.float64\n        sym.dtype_constr['typevar'] = ct.TypeVar\n        expected_blah = [None]\n    \n        def _unary_type_constr(blah):\n            self.assertEqual(blah, expected_blah[0])\n            expected_blah[0] = None\n            return ct.float32\n        sym.dtype_constr['unary'] = _unary_type_constr\n    \n        def assertExpectedParse(ds_str, expected):\n            expected_blah[0] = expected\n            self.assertEqual(parse(ds_str, sym), ct.DataShape(ct.float32))\n            self.assertEqual(expected_blah[0], None,\n                'The test unary type constructor did not run')\n        assertExpectedParse('unary[0]', 0)\n        assertExpectedParse('unary[100000]', 100000)\n        assertExpectedParse('unary[\"test\"]', 'test')\n        assertExpectedParse(\"unary['test']\", 'test')\n        assertExpectedParse('unary[\"\\\\uc548\\\\ub155\"]', u'\uc548\ub155')\n        assertExpectedParse(u'unary[\"\uc548\ub155\"]', u'\uc548\ub155')\n        assertExpectedParse('unary[int8]', ct.DataShape(ct.int8))\n        assertExpectedParse('unary[X]', ct.DataShape(ct.TypeVar('X')))\n        assertExpectedParse('unary[[]]', [])\n        assertExpectedParse('unary[[0, 3, 12]]', [0, 3, 12])\n        assertExpectedParse('unary[[\"test\", \"one\", \"two\"]]', ['test', 'one', 'two']\n            )\n        assertExpectedParse('unary[[float64, int8, uint16]]', [ct.DataShape(ct.\n            float64), ct.DataShape(ct.int8), ct.DataShape(ct.uint16)])\n        assertExpectedParse('unary[blah=0]', 0)\n        assertExpectedParse('unary[blah=100000]', 100000)\n        assertExpectedParse('unary[blah=\"test\"]', 'test')\n        assertExpectedParse(\"unary[blah='test']\", 'test')\n        assertExpectedParse('unary[blah=\"\\\\uc548\\\\ub155\"]', u'\uc548\ub155')\n        assertExpectedParse(u'unary[blah=\"\uc548\ub155\"]', u'\uc548\ub155')\n        assertExpectedParse('unary[blah=int8]', ct.DataShape(ct.int8))\n        assertExpectedParse('unary[blah=X]', ct.DataShape(ct.TypeVar('X')))\n        assertExpectedParse('unary[blah=[]]', [])\n        assertExpectedParse('unary[blah=[0, 3, 12]]', [0, 3, 12])\n        assertExpectedParse('unary[blah=[\"test\", \"one\", \"two\"]]', ['test',\n            'one', 'two'])\n        assertExpectedParse('unary[blah=[float64, int8, uint16]]', [ct.\n            DataShape(ct.float64), ct.DataShape(ct.int8), ct.DataShape(ct.uint16)])\n    \nTestDataShapeParserDTypeConstr().test_unary_dtype_constr()\n", "code_start": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_parser.py"}, {"test_code": "import unittest\nimport pytest\nfrom datashader import datashape\nfrom datashader.datashape.util.testing import assert_dshape_equal\nfrom datashader.datashape.parser import parse\nfrom datashader.datashape import coretypes as ct\nfrom datashader.datashape import DataShapeSyntaxError\n\nclass TestDataShapeParserDTypeConstr(unittest.TestCase):\n    def test_binary_dtype_constr(self):\n        sym = datashape.TypeSymbolTable(bare=True)\n        sym.dtype['int8'] = ct.int8\n        sym.dtype['uint16'] = ct.uint16\n        sym.dtype['float64'] = ct.float64\n        sym.dtype_constr['typevar'] = ct.TypeVar\n        expected_arg = [None, None]\n    \n        def _binary_type_constr(a, b):\n            self.assertEqual(a, expected_arg[0])\n            self.assertEqual(b, expected_arg[1])\n            expected_arg[0] = None\n            expected_arg[1] = None\n            return ct.float32\n        sym.dtype_constr['binary'] = _binary_type_constr\n    \n        def assertExpectedParse(ds_str, expected_a, expected_b):\n            expected_arg[0] = expected_a\n            expected_arg[1] = expected_b\n            self.assertEqual(parse(ds_str, sym), ct.DataShape(ct.float32))\n            self.assertEqual(expected_arg, [None, None],\n                'The test binary type constructor did not run')\n        assertExpectedParse('binary[1, 0]', 1, 0)\n        assertExpectedParse('binary[0, \"test\"]', 0, 'test')\n        assertExpectedParse('binary[int8, \"test\"]', ct.DataShape(ct.int8), 'test')\n        assertExpectedParse('binary[[1,3,5], \"test\"]', [1, 3, 5], 'test')\n        assertExpectedParse('binary[0, b=1]', 0, 1)\n        assertExpectedParse('binary[\"test\", b=A]', 'test', ct.DataShape(ct.\n            TypeVar('A')))\n        assertExpectedParse('binary[[3, 6], b=int8]', [3, 6], ct.DataShape(ct.int8)\n            )\n        assertExpectedParse('binary[Arg, b=[\"x\", \"test\"]]', ct.DataShape(ct.\n            TypeVar('Arg')), ['x', 'test'])\n        assertExpectedParse('binary[a=1, b=0]', 1, 0)\n        assertExpectedParse('binary[a=[int8, A, uint16], b=\"x\"]', [ct.DataShape\n            (ct.int8), ct.DataShape(ct.TypeVar('A')), ct.DataShape(ct.uint16)], 'x'\n            )\n    \nTestDataShapeParserDTypeConstr().test_binary_dtype_constr()\n", "code_start": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_parser.py"}], "instruction": "Functionality: The parse function is designed to interpret a string that represents a datashape and convert it into a structured datashape object. This object reflects the dimensions, data types, and type constructors specified within the datashape string.\n\nInputs: \n- ds_str: A string, which is the datashape string to be parsed. This string should accurately represent a datashape with dimensions and data types.\n- sym: A TypeSymbolTable object, which contains symbol tables for dimensions, data types, and type constructors. These symbol tables help in the accurate interpretation of the datashape string.\n\nOutputs: \n- The function returns a structured datashape object if the parsing is successful. This object encapsulates the dimensions and data types from the input string.\n- If the datashape string is invalid or contains unexpected tokens, the function does not return a valid object but rather raises an error with an appropriate message.", "method_code_mask": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef parse(ds_str, sym): [MASK]\n"}
{"method_name": "isfixed", "full_method_name": "isfixed", "method_path": "../srcdata/Visualization/datashader/datashader/datashape/predicates.py", "method_code": "import numpy as np\ndef isfixed(ds):\n    \"\"\" Contains no variable dimensions\n\n    >>> isfixed('10 * int')\n    True\n    >>> isfixed('var * int')\n    False\n    >>> isfixed('10 * {name: string, amount: int}')\n    True\n    >>> isfixed('10 * {name: string, amounts: var * int}')\n    False\n    \"\"\"\n    ds = dshape(ds)\n    if isinstance(ds[0], TypeVar):\n        return None\n    if isinstance(ds[0], Var):\n        return False\n    if isinstance(ds[0], Record):\n        return all(map(isfixed, ds[0].types))\n    if len(ds) > 1:\n        return isfixed(ds.subarray(1))\n    return True", "test_code_list": [{"test_code": "from datashader.datashape.predicates import isfixed\nfrom datashader.datashape.predicates import _dimensions\nfrom datashader.datashape.predicates import isnumeric\nfrom datashader.datashape.predicates import isscalar\nfrom datashader.datashape.coretypes import TypeVar\nfrom datashader.datashape.coretypes import int32\nfrom datashader.datashape.coretypes import Categorical\ndef test_isfixed():\n    assert not isfixed(TypeVar('M') * int32)\n\ntest_isfixed()\n", "code_start": "", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_predicates.py"}], "instruction": "Functionality: The function isfixed checks whether a given data schema (ds) contains no variable dimensions. It verifies if the data schema is fixed or not by analyzing its structure.\n\nInputs: \n- ds: A string that represents a data schema. The data schema can be a simple data type, an array, or a record of fields, where 'var' represents a variable dimension.\n\nOutputs: \n- A boolean value indicating whether the data schema is fixed (True) or contains variable dimensions (False).", "method_code_mask": "import numpy as np\n\n\ndef isfixed(ds): [MASK]\n"}
{"method_name": "isnumeric", "full_method_name": "isnumeric", "method_path": "../srcdata/Visualization/datashader/datashader/datashape/predicates.py", "method_code": "import numpy as np\ndef isnumeric(ds):\n    \"\"\" Has a numeric measure\n\n    >>> isnumeric('int32')\n    True\n    >>> isnumeric('3 * ?real')\n    True\n    >>> isnumeric('string')\n    False\n    >>> isnumeric('var * {amount: ?int32}')\n    False\n    \"\"\"\n    ds = launder(ds)\n    try:\n        npdtype = to_numpy_dtype(ds)\n    except TypeError:\n        return False\n    else:\n        return isinstance(ds, Unit) and np.issubdtype(npdtype, np.number)", "test_code_list": [{"test_code": "from datashader.datashape.predicates import isfixed\nfrom datashader.datashape.predicates import _dimensions\nfrom datashader.datashape.predicates import isnumeric\nfrom datashader.datashape.predicates import isscalar\nfrom datashader.datashape.coretypes import TypeVar\nfrom datashader.datashape.coretypes import int32\nfrom datashader.datashape.coretypes import Categorical\ndef test_time():\n    assert not isnumeric('time')\n\ntest_time()\n", "code_start": "", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_predicates.py"}], "instruction": "Functionality: The isnumeric function checks whether a given data type string represents a numeric measure. It returns True if the data type is numeric, and False if it is not.\n\nInputs: A single argument, ds, which is a string representing a data type.\n\nOutputs: A boolean value indicating whether ds is numeric. True if ds represents a numeric data type, False if ds represents a non-numeric data type.", "method_code_mask": "import numpy as np\n\n\ndef isnumeric(ds): [MASK]\n"}
{"method_name": "unite_base", "full_method_name": "unite_base", "method_path": "../srcdata/Visualization/datashader/datashader/datashape/discovery.py", "method_code": "from __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom datetime import date\nfrom datetime import time\nfrom datetime import timedelta\nfrom itertools import chain\nimport re\nfrom textwrap import dedent\nfrom types import MappingProxyType\nfrom warnings import warn\nfrom dateutil.parser import parse as dateparse\nimport numpy as np\nfrom unittest.mock import Mock\ndef unite_base(dshapes):\n    \"\"\" Performs lowest common dshape and also null aware\n\n    >>> unite_base([float64, float64, int64])\n    dshape(\"3 * float64\")\n\n    >>> unite_base([int32, int64, null])\n    dshape(\"3 * ?int64\")\n    \"\"\"\n    dshapes = [unpack(ds) for ds in dshapes]\n    bynull = groupby(isnull, dshapes)\n    try:\n        good_dshapes = bynull[False]\n    except KeyError:\n        return len(dshapes) * null\n    if all(isinstance(ds, Unit) for ds in good_dshapes):\n        base = lowest_common_dshape(good_dshapes)\n    elif (all(isinstance(ds, Record) for ds in good_dshapes) and ds.names ==\n        dshapes[0].names for ds in good_dshapes):\n        names = good_dshapes[0].names\n        base = Record([[name, unite_base([ds.dict.get(name, null) for ds in\n            good_dshapes]).subshape[0]] for name in names])\n    if base:\n        if bynull.get(True):\n            base = Option(base)\n        return len(dshapes) * base", "test_code_list": [{"test_code": "from collections import OrderedDict\nfrom itertools import starmap\nfrom types import MappingProxyType\nfrom warnings import catch_warnings\nfrom warnings import simplefilter\nimport numpy as np\nimport pytest\nfrom datashader.datashape.discovery import discover\nfrom datashader.datashape.discovery import null\nfrom datashader.datashape.discovery import unite_identical\nfrom datashader.datashape.discovery import unite_base\nfrom datashader.datashape.discovery import unite_merge_dimensions\nfrom datashader.datashape.discovery import do_one\nfrom datashader.datashape.discovery import lowest_common_dshape\nfrom datashader.datashape.coretypes import int64\nfrom datashader.datashape.coretypes import float64\nfrom datashader.datashape.coretypes import complex128\nfrom datashader.datashape.coretypes import string\nfrom datashader.datashape.coretypes import bool_\nfrom datashader.datashape.coretypes import Tuple\nfrom datashader.datashape.coretypes import Record\nfrom datashader.datashape.coretypes import date_\nfrom datashader.datashape.coretypes import datetime_\nfrom datashader.datashape.coretypes import time_\nfrom datashader.datashape.coretypes import timedelta_\nfrom datashader.datashape.coretypes import int32\nfrom datashader.datashape.coretypes import var\nfrom datashader.datashape.coretypes import Option\nfrom datashader.datashape.coretypes import real\nfrom datashader.datashape.coretypes import Null\nfrom datashader.datashape.coretypes import TimeDelta\nfrom datashader.datashape.coretypes import String\nfrom datashader.datashape.coretypes import float32\nfrom datashader.datashape.coretypes import R\nfrom datashader.datashape.util.testing import assert_dshape_equal\nfrom datashader.datashape import dshape\nfrom datetime import date\nfrom datetime import time\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom unittest.mock import Mock\ndef test_unite_base():\n    assert unite_base([date_, datetime_]) == 2 * datetime_\n\ntest_unite_base()\n", "code_start": "", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_discovery.py"}, {"test_code": "from collections import OrderedDict\nfrom itertools import starmap\nfrom types import MappingProxyType\nfrom warnings import catch_warnings\nfrom warnings import simplefilter\nimport numpy as np\nimport pytest\nfrom datashader.datashape.discovery import discover\nfrom datashader.datashape.discovery import null\nfrom datashader.datashape.discovery import unite_identical\nfrom datashader.datashape.discovery import unite_base\nfrom datashader.datashape.discovery import unite_merge_dimensions\nfrom datashader.datashape.discovery import do_one\nfrom datashader.datashape.discovery import lowest_common_dshape\nfrom datashader.datashape.coretypes import int64\nfrom datashader.datashape.coretypes import float64\nfrom datashader.datashape.coretypes import complex128\nfrom datashader.datashape.coretypes import string\nfrom datashader.datashape.coretypes import bool_\nfrom datashader.datashape.coretypes import Tuple\nfrom datashader.datashape.coretypes import Record\nfrom datashader.datashape.coretypes import date_\nfrom datashader.datashape.coretypes import datetime_\nfrom datashader.datashape.coretypes import time_\nfrom datashader.datashape.coretypes import timedelta_\nfrom datashader.datashape.coretypes import int32\nfrom datashader.datashape.coretypes import var\nfrom datashader.datashape.coretypes import Option\nfrom datashader.datashape.coretypes import real\nfrom datashader.datashape.coretypes import Null\nfrom datashader.datashape.coretypes import TimeDelta\nfrom datashader.datashape.coretypes import String\nfrom datashader.datashape.coretypes import float32\nfrom datashader.datashape.coretypes import R\nfrom datashader.datashape.util.testing import assert_dshape_equal\nfrom datashader.datashape import dshape\nfrom datetime import date\nfrom datetime import time\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom unittest.mock import Mock\ndef test_unite_base_on_records():\n    dshapes = [dshape('{name: string, amount: int32}'), dshape(\n        '{name: string, amount: int32}')]\n    assert unite_base(dshapes) == dshape('2 * {name: string, amount: int32}')\n    dshapes = [Null(), dshape('{name: string, amount: int32}')]\n    assert unite_base(dshapes) == dshape('2 * ?{name: string, amount: int32}')\n    dshapes = [dshape('{name: string, amount: int32}'), dshape(\n        '{name: string, amount: int64}')]\n    assert unite_base(dshapes) == dshape('2 * {name: string, amount: int64}')\n\ntest_unite_base_on_records()\n", "code_start": "", "test_path": "../srcdata/Visualization/datashader/datashader/datashape/tests/test_discovery.py"}], "instruction": "Functionality: The unite_base function is designed to find the lowest common data shape (dshape) from a list of given data shapes. It takes into account the presence of null values and adjusts the output accordingly. When all the data shapes are of the same unit type or record type with the same names, it computes the lowest common dshape. If there are null values, it wraps the common dshape in an Option type to signify that the values can be null.\n\nInputs: \n- dshapes: A list of data shapes. Each data shape in the list could be of type float64, int64, int32, etc., or a record type with various fields, or null.\n\nOutputs: \n- A new data shape that represents the lowest common dshape of all the input data shapes. If any of the input shapes can be null, the output dshape will also indicate this possibility.", "method_code_mask": "from __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom datetime import date\nfrom datetime import time\nfrom datetime import timedelta\nfrom itertools import chain\nimport re\nfrom textwrap import dedent\nfrom types import MappingProxyType\nfrom warnings import warn\nfrom dateutil.parser import parse as dateparse\nimport numpy as np\nfrom unittest.mock import Mock\n\n\ndef unite_base(dshapes): [MASK]\n"}

{"method_name": "strip_numeric", "full_method_name": "strip_numeric", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef strip_numeric(s):\n    \"\"\"Remove digits from `s` using :const:`~gensim.parsing.preprocessing.RE_NUMERIC`.\n\n    Parameters\n    ----------\n    s : str\n\n    Returns\n    -------\n    str\n        Unicode  string without digits.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.parsing.preprocessing import strip_numeric\n        >>> strip_numeric(\"0text24gensim365test\")\n        u'textgensimtest'\n\n    \"\"\"\n    s = utils.to_unicode(s)\n    return RE_NUMERIC.sub('', s)", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_strip_numeric(self):\n\t    self.assertEqual(strip_numeric('salut les amis du 59'),\n\t        'salut les amis du ')\n\t\nTestPreprocessing().test_strip_numeric()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: The strip_numeric function removes all digits from the given string. It utilizes the RE_NUMERIC regular expression from the gensim library to identify and eliminate numeric characters, returning the resulting string without any digits.\n\nInputs: \n- s: A string from which digits are to be removed.\n\nOutputs: \n- str: A Unicode string with all digits removed.\n\nNote: The function uses the gensim library's utils module to ensure the input is treated as a Unicode string, which enhances compatibility with various text processing methods.", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef strip_numeric(s): [MASK]\n"}
{"method_name": "strip_short", "full_method_name": "strip_short", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef strip_short(s, minsize=3):\n    \"\"\"Remove words with length lesser than `minsize` from `s`.\n\n    Parameters\n    ----------\n    s : str\n    minsize : int, optional\n\n    Returns\n    -------\n    str\n        Unicode string without short words.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.parsing.preprocessing import strip_short\n        >>> strip_short(\"salut les amis du 59\")\n        u'salut les amis'\n        >>>\n        >>> strip_short(\"one two three four five six seven eight nine ten\", minsize=5)\n        u'three seven eight'\n\n    \"\"\"\n    s = utils.to_unicode(s)\n    return ' '.join(remove_short_tokens(s.split(), minsize))", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_strip_short(self):\n\t    self.assertEqual(strip_short('salut les amis du 59', 3), 'salut les amis')\n\t\nTestPreprocessing().test_strip_short()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: The strip_short function is designed to remove words from a given string that are shorter than a specified minimum length. This is particularly useful for text preprocessing tasks where noise reduction, such as removing short filler words, is required.\n\nInputs: \n1. s : str\n   - A Unicode string from which short words are to be removed.\n2. minsize : int, optional (default=3)\n   - The minimum length that a word must have to be retained in the output. Words shorter than this length will be removed from the input string.\n\nOutputs: \n1. str\n   - Returns a Unicode string with all words shorter than `minsize` removed.\n\nNotes:\n- The function expects the input string to be a Unicode string.\n- The function splits the input string into words, removes those that do not meet the length criteria, and then joins the remaining words back into a string.", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef strip_short(s, minsize=3): [MASK]\n"}
{"method_name": "strip_tags", "full_method_name": "strip_tags", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef strip_tags(s):\n    \"\"\"Remove tags from `s` using :const:`~gensim.parsing.preprocessing.RE_TAGS`.\n\n    Parameters\n    ----------\n    s : str\n\n    Returns\n    -------\n    str\n        Unicode string without tags.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.parsing.preprocessing import strip_tags\n        >>> strip_tags(\"<i>Hello</i> <b>World</b>!\")\n        u'Hello World!'\n\n    \"\"\"\n    s = utils.to_unicode(s)\n    return RE_TAGS.sub('', s)", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_strip_tags(self):\n\t    self.assertEqual(strip_tags('<i>Hello</i> <b>World</b>!'), 'Hello World!')\n\t\nTestPreprocessing().test_strip_tags()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: The strip_tags function is designed to remove HTML tags from a given string. It takes a string that may contain HTML tags and returns the string without those tags, ensuring the text is clean and free of markup.\n\nInputs: \n- s : str\n    A string that potentially contains HTML tags.\n\nOutputs: \n- str : \n    A Unicode string with all HTML tags removed, preserving only the text content.", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef strip_tags(s): [MASK]\n"}
{"method_name": "strip_multiple_whitespaces", "full_method_name": "strip_multiple_whitespaces", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef strip_multiple_whitespaces(s):\n    \"\"\"Remove repeating whitespace characters (spaces, tabs, line breaks) from `s`\n    and turns tabs & line breaks into spaces using :const:`~gensim.parsing.preprocessing.RE_WHITESPACE`.\n\n    Parameters\n    ----------\n    s : str\n\n    Returns\n    -------\n    str\n        Unicode string without repeating in a row whitespace characters.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.parsing.preprocessing import strip_multiple_whitespaces\n        >>> strip_multiple_whitespaces(\"salut\" + '\\\\r' + \" les\" + '\\\\n' + \"         loulous!\")\n        u'salut les loulous!'\n\n    \"\"\"\n    s = utils.to_unicode(s)\n    return RE_WHITESPACE.sub(' ', s)", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_strip_multiple_whitespaces(self):\n\t    self.assertEqual(strip_multiple_whitespaces('salut  les\\r\\nloulous!'),\n\t        'salut les loulous!')\n\t\nTestPreprocessing().test_strip_multiple_whitespaces()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: The function strip_multiple_whitespaces is designed to remove repeating whitespace characters (including spaces, tabs, and line breaks) from a given string. It also converts tabs and line breaks into spaces.\n\nInputs: \n- s: A string (str) that may contain repeating whitespace characters.\n\nOutputs: \n- A string (str) that is the processed version of the input string, where repeating whitespaces are removed and tabs/line breaks are turned into spaces.\n\nExample instructions for the interview:", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef strip_multiple_whitespaces(s): [MASK]\n"}
{"method_name": "strip_non_alphanum", "full_method_name": "strip_non_alphanum", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef strip_non_alphanum(s):\n    \"\"\"Remove non-alphabetic characters from `s` using :const:`~gensim.parsing.preprocessing.RE_NONALPHA`.\n\n    Parameters\n    ----------\n    s : str\n\n    Returns\n    -------\n    str\n        Unicode string with alphabetic characters only.\n\n    Notes\n    -----\n    Word characters - alphanumeric & underscore.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.parsing.preprocessing import strip_non_alphanum\n        >>> strip_non_alphanum(\"if-you#can%read$this&then@this#method^works\")\n        u'if you can read this then this method works'\n\n    \"\"\"\n    s = utils.to_unicode(s)\n    return RE_NONALPHA.sub(' ', s)", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_strip_non_alphanum(self):\n\t    self.assertEqual(strip_non_alphanum('toto nf-kappa titi'),\n\t        'toto nf kappa titi')\n\t\nTestPreprocessing().test_strip_non_alphanum()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: Remove non-alphabetic characters from a given string, replacing non-alphabetic characters with spaces.\n\nInputs:\n- s : str\n    The input string from which non-alphabetic characters will be removed.\n\nOutputs:\n- str\n    A new string with all non-alphabetic characters replaced by spaces. The output will contain only alphabetic characters and spaces.\n\nNotes:\n- The function considers word characters to include alphanumeric and underscore characters. However, it specifically targets the removal of non-alphabetic characters, not affecting digits or underscores.\n- The function uses a regular expression (RE_NONALPHA) to identify and replace non-alphabetic characters.\n- The input string is first converted to a Unicode string to ensure compatibility with various character sets.\n\nExamples:\n- strip_non_alphanum(\"if-you#can%read$this&then@this#method^works\")\n  Output: \"if you can read this then this method works\"", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef strip_non_alphanum(s): [MASK]\n"}
{"method_name": "split_alphanum", "full_method_name": "split_alphanum", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef split_alphanum(s):\n    \"\"\"Add spaces between digits & letters in `s` using :const:`~gensim.parsing.preprocessing.RE_AL_NUM`.\n\n    Parameters\n    ----------\n    s : str\n\n    Returns\n    -------\n    str\n        Unicode string with spaces between digits & letters.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.parsing.preprocessing import split_alphanum\n        >>> split_alphanum(\"24.0hours7 days365 a1b2c3\")\n        u'24.0 hours 7 days 365 a 1 b 2 c 3'\n\n    \"\"\"\n    s = utils.to_unicode(s)\n    s = RE_AL_NUM.sub('\\\\1 \\\\2', s)\n    return RE_NUM_AL.sub('\\\\1 \\\\2', s)", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_split_alphanum(self):\n\t    self.assertEqual(split_alphanum('toto diet1 titi'), 'toto diet 1 titi')\n\t    self.assertEqual(split_alphanum('toto 1diet titi'), 'toto 1 diet titi')\n\t\nTestPreprocessing().test_split_alphanum()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: The split_alphanum function is designed to add spaces between digits and letters in a given string. It utilizes a predefined regular expression pattern from the gensim library to identify sequences where a digit is immediately followed by a letter or vice versa, inserting a space between them.\n\nInputs: \n- s : str\n    The input string that may contain alphanumeric characters without spaces between digits and letters.\n\nOutputs: \n- str\n    A Unicode string with spaces inserted between digits and letters, enhancing readability by separating numbers and words.", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef split_alphanum(s): [MASK]\n"}
{"method_name": "remove_stopwords", "full_method_name": "remove_stopwords", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef remove_stopwords(s, stopwords=None):\n    \"\"\"Remove :const:`~gensim.parsing.preprocessing.STOPWORDS` from `s`.\n\n    Parameters\n    ----------\n    s : str\n    stopwords : iterable of str, optional\n        Sequence of stopwords\n        If None - using :const:`~gensim.parsing.preprocessing.STOPWORDS`\n\n    Returns\n    -------\n    str\n        Unicode string without `stopwords`.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.parsing.preprocessing import remove_stopwords\n        >>> remove_stopwords(\"Better late than never, but better never late.\")\n        u'Better late never, better late.'\n\n    \"\"\"\n    s = utils.to_unicode(s)\n    return ' '.join(remove_stopword_tokens(s.split(), stopwords))", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_strip_stopwords(self):\n\t    self.assertEqual(remove_stopwords('the world is square'), 'world square')\n\t    with mock.patch('gensim.parsing.preprocessing.STOPWORDS', frozenset([\n\t        'the'])):\n\t        self.assertEqual(remove_stopwords('the world is square'),\n\t            'world is square')\n\t\nTestPreprocessing().test_strip_stopwords()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: The remove_stopwords function is designed to remove stopwords from a given string. Stopwords are common words (like 'is', 'the', 'and', etc.) that do not carry significant meaning and are usually removed from texts to improve the efficiency of text processing tasks.\n\nInputs: \n1. s: A string from which the stopwords will be removed.\n2. stopwords: An optional iterable of strings representing a custom list of stopwords. If not provided, the function uses its default list of stopwords.\n\nOutputs: \n1. A string: The function returns the input string 's' with all stopwords removed. The returned string is a Unicode string, ensuring compatibility with various text processing tasks.", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef remove_stopwords(s, stopwords=None): [MASK]\n"}
{"method_name": "remove_stopword_tokens", "full_method_name": "remove_stopword_tokens", "method_path": "../srcdata/Computation/gensim/gensim/parsing/preprocessing.py", "method_code": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\ndef remove_stopword_tokens(tokens, stopwords=None):\n    \"\"\"Remove stopword tokens using list `stopwords`.\n\n    Parameters\n    ----------\n    tokens : iterable of str\n        Sequence of tokens.\n    stopwords : iterable of str, optional\n        Sequence of stopwords\n        If None - using :const:`~gensim.parsing.preprocessing.STOPWORDS`\n\n    Returns\n    -------\n    list of str\n        List of tokens without `stopwords`.\n\n    \"\"\"\n    if stopwords is None:\n        stopwords = STOPWORDS\n    return [token for token in tokens if token not in stopwords]", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom unittest import mock\nimport numpy as np\nfrom gensim.parsing.preprocessing import remove_short_tokens\nfrom gensim.parsing.preprocessing import remove_stopword_tokens\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import stem_text\nfrom gensim.parsing.preprocessing import split_alphanum\nfrom gensim.parsing.preprocessing import split_on_space\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces\nfrom gensim.parsing.preprocessing import strip_non_alphanum\nfrom gensim.parsing.preprocessing import strip_numeric\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom gensim.parsing.preprocessing import strip_short\nfrom gensim.parsing.preprocessing import strip_tags\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_strip_stopword_tokens(self):\n\t    self.assertEqual(remove_stopword_tokens(['the', 'world', 'is', 'sphere'\n\t        ]), ['world', 'sphere'])\n\t    with mock.patch('gensim.parsing.preprocessing.STOPWORDS', frozenset([\n\t        'the'])):\n\t        self.assertEqual(remove_stopword_tokens(['the', 'world', 'is',\n\t            'sphere']), ['world', 'is', 'sphere'])\n\t\nTestPreprocessing().test_strip_stopword_tokens()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_parsing.py"}], "instruction": "Functionality: The remove_stopword_tokens function is designed to filter out stopwords from a list of tokens. Stopwords are words that are filtered out before or after processing of text as they do not carry significant meaning and are very common in a language (e.g., 'the', 'is', 'at', 'which', 'on', etc.). This function will return a list of tokens without these stopwords, which can be useful in text processing tasks such as search, analysis, and machine learning.\n\nInputs:\n- tokens: An iterable (e.g., list, set) of strings representing the tokens (words) from which stopwords will be removed.\n- stopwords: An optional iterable (e.g., list, set) of strings representing the stopwords to be removed from the tokens. If None is specified, a default set of stopwords as defined in gensim's preprocessing module will be used.\n\nOutputs:\n- A list of strings, representing the tokens after removing stopwords. This list will contain only those tokens that are not present in the stopwords list provided or gensim's default STOPWORDS list if no stopwords list is given.", "method_code_mask": "import re\nimport string\nimport glob\nfrom gensim import utils\nfrom gensim.parsing.porter import PorterStemmer\n\n\ndef remove_stopword_tokens(tokens, stopwords=None): [MASK]\n"}
{"method_name": "is_corpus", "full_method_name": "is_corpus", "method_path": "../srcdata/Computation/gensim/gensim/utils.py", "method_code": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\ndef is_corpus(obj):\n    \"\"\"Check whether `obj` is a corpus, by peeking at its first element. Works even on streamed generators.\n    The peeked element is put back into a object returned by this function, so always use\n    that returned object instead of the original `obj`.\n\n    Parameters\n    ----------\n    obj : object\n        An `iterable of iterable` that contains (int, numeric).\n\n    Returns\n    -------\n    (bool, object)\n        Pair of (is `obj` a corpus, `obj` with peeked element restored)\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.utils import is_corpus\n        >>> corpus = [[(1, 1.0)], [(2, -0.3), (3, 0.12)]]\n        >>> corpus_or_not, corpus = is_corpus(corpus)\n\n    Warnings\n    --------\n    An \"empty\" corpus (empty input sequence) is ambiguous, so in this case\n    the result is forcefully defined as (False, `obj`).\n\n    \"\"\"\n    try:\n        if 'Corpus' in obj.__class__.__name__:\n            return True, obj\n    except Exception:\n        pass\n    try:\n        if hasattr(obj, 'next') or hasattr(obj, '__next__'):\n            doc1 = next(obj)\n            obj = itertools.chain([doc1], obj)\n        else:\n            doc1 = next(iter(obj))\n        if len(doc1) == 0:\n            return True, obj\n        id1, val1 = next(iter(doc1))\n        id1, val1 = int(id1), float(val1)\n    except Exception:\n        return False, obj\n    return True, obj", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestIsCorpus(unittest.TestCase):\n\tdef test_None(self):\n\t    result = is_corpus(None)\n\t    expected = False, None\n\t    self.assertEqual(expected, result)\n\t\nTestIsCorpus().test_None()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestIsCorpus(unittest.TestCase):\n\tdef test_simple_lists_of_tuples(self):\n\t    potentialCorpus = [[(0, 4.0)]]\n\t    result = is_corpus(potentialCorpus)\n\t    expected = True, potentialCorpus\n\t    self.assertEqual(expected, result)\n\t    potentialCorpus = [[(0, 4.0), (1, 2.0)]]\n\t    result = is_corpus(potentialCorpus)\n\t    expected = True, potentialCorpus\n\t    self.assertEqual(expected, result)\n\t    potentialCorpus = [[(0, 4.0), (1, 2.0), (2, 5.0), (3, 8.0)]]\n\t    result = is_corpus(potentialCorpus)\n\t    expected = True, potentialCorpus\n\t    self.assertEqual(expected, result)\n\t    potentialCorpus = [[(0, 4.0)], [(1, 2.0)]]\n\t    result = is_corpus(potentialCorpus)\n\t    expected = True, potentialCorpus\n\t    self.assertEqual(expected, result)\n\t    potentialCorpus = [[(0, 4.0)], [(1, 2.0)], [(2, 5.0)], [(3, 8.0)]]\n\t    result = is_corpus(potentialCorpus)\n\t    expected = True, potentialCorpus\n\t    self.assertEqual(expected, result)\n\t\nTestIsCorpus().test_simple_lists_of_tuples()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestIsCorpus(unittest.TestCase):\n\tdef test_int_tuples(self):\n\t    potentialCorpus = [[(0, 4)]]\n\t    result = is_corpus(potentialCorpus)\n\t    expected = True, potentialCorpus\n\t    self.assertEqual(expected, result)\n\t\nTestIsCorpus().test_int_tuples()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestIsCorpus(unittest.TestCase):\n\tdef test_invalid_formats(self):\n\t    potentials = list()\n\t    potentials.append(['human'])\n\t    potentials.append('human')\n\t    potentials.append(['human', 'star'])\n\t    potentials.append([1, 2, 3, 4, 5, 5])\n\t    potentials.append([[(0, 'string')]])\n\t    for noCorpus in potentials:\n\t        result = is_corpus(noCorpus)\n\t        expected = False, noCorpus\n\t        self.assertEqual(expected, result)\n\t\nTestIsCorpus().test_invalid_formats()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}], "instruction": "Functionality: Determine if the provided object, `obj`, is a corpus. A corpus is defined as an iterable of iterables that contains pairs of integers and numbers, representing (id, value) pairs. This function checks the nature of the first element and ensures that the object adheres to the corpus definition. It also restores the peeked element back into the object to maintain its original sequence.\n\nInputs: \n    obj : object\n        The object to be checked. It should be an iterable of iterables, potentially a generator, that might contain numerical pairs representing (id, value).\n\nOutputs:\n    (bool, object)\n        A tuple where the first element is a boolean indicating whether `obj` is a corpus, and the second element is the `obj` with the peeked element restored if it was a valid corpus.", "method_code_mask": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n\n\ndef is_corpus(obj): [MASK]\n"}
{"method_name": "open_file", "full_method_name": "open_file", "method_path": "../srcdata/Computation/gensim/gensim/utils.py", "method_code": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n@contextmanager\ndef open_file(input):\n    \"\"\"Provide \"with-like\" behaviour without closing the file object.\n\n    Parameters\n    ----------\n    input : str or file-like\n        Filename or file-like object.\n\n    Yields\n    -------\n    file\n        File-like object based on input (or input if this already file-like).\n\n    \"\"\"\n    mgr = file_or_filename(input)\n    exc = False\n    try:\n        yield mgr\n    except Exception:\n        exc = True\n        if not isinstance(input, str) or not mgr.__exit__(*sys.exc_info()):\n            raise\n    finally:\n        if not exc and isinstance(input, str):\n            mgr.__exit__(None, None, None)", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestUtils(unittest.TestCase):\n\tdef test_open_file_non_existent_file(self):\n\t    with self.assertRaises(Exception):\n\t        with open_file('non_existent_file.txt'):\n\t            pass\n\t\nTestUtils().test_open_file_non_existent_file()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestUtils(unittest.TestCase):\n\tdef test_open_file_non_existent_file_object(self):\n\t    file_obj = None\n\t    with self.assertRaises(Exception):\n\t        with open_file(file_obj):\n\t            pass\n\t\nTestUtils().test_open_file_non_existent_file_object()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}], "instruction": "Functionality: The open_file function is designed to provide a context manager that allows for \"with-like\" behavior when handling file operations. It aims to ensure that files are properly managed and closed after operations are completed, without actually closing the file object if it was already open. This function supports both strings (representing filenames) and file-like objects as inputs.\n\nInputs: \n- input: A string representing a filename or a file-like object. This argument is mandatory and should either be the path to a file that needs to be opened or a pre-existing file-like object that needs to be used within the context.\n\nOutputs:\n- file: A file-like object. If the input is a string, the function will open the file specified by the string and yield a file-like object. If the input is already a file-like object, it will be returned directly without modification. The function will manage the lifetime of the file object within the context, ensuring proper cleanup and handling of exceptions.", "method_code_mask": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n\n\n@contextmanager\ndef open_file(input): [MASK]\n"}
{"method_name": "sample_dict", "full_method_name": "sample_dict", "method_path": "../srcdata/Computation/gensim/gensim/utils.py", "method_code": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\ndef sample_dict(d, n=10, use_random=True):\n    \"\"\"Selected `n` (possibly random) items from the dictionary `d`.\n\n    Parameters\n    ----------\n    d : dict\n        Input dictionary.\n    n : int, optional\n        Number of items to select.\n    use_random : bool, optional\n        Select items randomly (without replacement), instead of by the natural dict iteration order?\n\n    Returns\n    -------\n    list of (object, object)\n        Selected items from dictionary, as a list.\n\n    \"\"\"\n    selected_keys = random.sample(list(d), min(len(d), n)\n        ) if use_random else itertools.islice(d.keys(), n)\n    return [(key, d[key]) for key in selected_keys]", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestSampleDict(unittest.TestCase):\n\tdef test_sample_dict(self):\n\t    d = {(1): 2, (2): 3, (3): 4, (4): 5}\n\t    expected_dict = [(1, 2), (2, 3)]\n\t    expected_dict_random = [(k, v) for k, v in d.items()]\n\t    sampled_dict = sample_dict(d, 2, False)\n\t    self.assertEqual(sampled_dict, expected_dict)\n\t    sampled_dict_random = sample_dict(d, 2)\n\t    if sampled_dict_random in expected_dict_random:\n\t        self.assertTrue(True)\n\t\nTestSampleDict().test_sample_dict()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}], "instruction": "Functionality: The sample_dict function is designed to select a specific number of items from a given dictionary. It can either select these items randomly or in the order they appear in the dictionary, depending on a flag passed to the function.\n\nInputs:\n1. d : dict\n   - The input dictionary from which items are to be selected.\n\n2. n : int, optional (default=10)\n   - The number of items to select from the dictionary. If n exceeds the number of items in the dictionary, all items will be selected.\n\n3. use_random : bool, optional (default=True)\n   - A flag to determine if items should be selected randomly (without replacement) or in the order they appear in the dictionary. If True, items are selected randomly. If False, items are selected in order.\n\nOutputs:\n1. list of (object, object)\n   - A list containing the selected items from the dictionary. Each item in the list is a tuple, where the first element is the key and the second element is the value corresponding to that key in the dictionary.", "method_code_mask": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n\n\ndef sample_dict(d, n=10, use_random=True): [MASK]\n"}
{"method_name": "merge_counts", "full_method_name": "merge_counts", "method_path": "../srcdata/Computation/gensim/gensim/utils.py", "method_code": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\ndef merge_counts(dict1, dict2):\n    \"\"\"Merge `dict1` of (word, freq1) and `dict2` of (word, freq2) into `dict1` of (word, freq1+freq2).\n    Parameters\n    ----------\n    dict1 : dict of (str, int)\n        First dictionary.\n    dict2 : dict of (str, int)\n        Second dictionary.\n    Returns\n    -------\n    result : dict\n        Merged dictionary with sum of frequencies as values.\n    \"\"\"\n    for word, freq in dict2.items():\n        if word in dict1:\n            dict1[word] += freq\n        else:\n            dict1[word] = freq\n    return dict1", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestMergeDicts(unittest.TestCase):\n\tdef test_merge_dicts(self):\n\t    d1 = {'word1': 5, 'word2': 1, 'word3': 2}\n\t    d2 = {'word1': 2, 'word3': 3, 'word4': 10}\n\t    res_dict = merge_counts(d1, d2)\n\t    expected_dict = {'word1': 7, 'word2': 1, 'word3': 5, 'word4': 10}\n\t    self.assertEqual(res_dict, expected_dict)\n\t\nTestMergeDicts().test_merge_dicts()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}], "instruction": "Functionality: Merge two dictionaries of word frequencies, dict1 and dict2, into dict1 with the frequencies summed up for each word that appears in both dictionaries.\nInputs:\n- dict1: A dictionary where keys are strings representing words and values are integers representing the frequencies of these words.\n- dict2: A dictionary where keys are strings representing words and values are integers representing the frequencies of these words.\nOutputs:\n- result: A dictionary where keys are strings representing words and values are integers representing the summed frequencies of these words from both input dictionaries.\n\nNote: The function modifies dict1 in place and also returns it as the result.", "method_code_mask": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n\n\ndef merge_counts(dict1, dict2): [MASK]\n"}
{"method_name": "any2unicode", "full_method_name": "any2unicode", "method_path": "../srcdata/Computation/gensim/gensim/utils.py", "method_code": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\ndef any2unicode(text, encoding='utf8', errors='strict'):\n    \"\"\"Convert `text` (bytestring in given encoding or unicode) to unicode.\n\n    Parameters\n    ----------\n    text : str\n        Input text.\n    errors : str, optional\n        Error handling behaviour if `text` is a bytestring.\n    encoding : str, optional\n        Encoding of `text` if it is a bytestring.\n\n    Returns\n    -------\n    str\n        Unicode version of `text`.\n\n    \"\"\"\n    if isinstance(text, str):\n        return text\n    return str(text, encoding, errors=errors)", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestSaveAsLineSentence(unittest.TestCase):\n\tdef test_save_as_line_sentence_en(self):\n\t    corpus_file = get_tmpfile('gensim_utils.tst')\n\t    ref_sentences = [line.split() for line in any2unicode(\n\t        'hello world\\nhow are you').split('\\n')]\n\t    utils.save_as_line_sentence(ref_sentences, corpus_file)\n\t    with utils.open(corpus_file, 'rb', encoding='utf8') as fin:\n\t        sentences = [line.strip().split() for line in fin.read().strip().\n\t            split('\\n')]\n\t        self.assertEqual(sentences, ref_sentences)\n\t\nTestSaveAsLineSentence().test_save_as_line_sentence_en()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom gensim import utils\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\n\nclass TestSaveAsLineSentence(unittest.TestCase):\n\tdef test_save_as_line_sentence_ru(self):\n\t    corpus_file = get_tmpfile('gensim_utils.tst')\n\t    ref_sentences = [line.split() for line in any2unicode(\n\t        '\u043f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440\\n\u043a\u0430\u043a \u0442\u044b \u043f\u043e\u0436\u0438\u0432\u0430\u0435\u0448\u044c').split('\\n')]\n\t    utils.save_as_line_sentence(ref_sentences, corpus_file)\n\t    with utils.open(corpus_file, 'rb', encoding='utf8') as fin:\n\t        sentences = [line.strip().split() for line in fin.read().strip().\n\t            split('\\n')]\n\t        self.assertEqual(sentences, ref_sentences)\n\t\nTestSaveAsLineSentence().test_save_as_line_sentence_ru()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_utils.py"}, {"test_code": "import logging\nimport unittest\nimport os\nfrom collections import namedtuple\nimport numpy as np\nfrom gensim import utils\nfrom gensim.models import doc2vec\nfrom gensim.models import keyedvectors\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import temporary_file\nfrom gensim.test.utils import common_texts as raw_sentences\nfrom gensim.models.word2vec_corpusfile import CythonLineSentence\n\nclass TestDoc2VecModel(unittest.TestCase):\n\t@unittest.skipIf(os.name == 'nt', 'See another test for Windows below')\n\tdef test_get_offsets_and_start_doctags(self):\n\t    lines = ['line1\\n', 'line2\\n', 'line3\\n', 'line4\\n', 'line5\\n']\n\t    tmpf = get_tmpfile('gensim_doc2vec.tst')\n\t    with utils.open(tmpf, 'wb', encoding='utf8') as fout:\n\t        for line in lines:\n\t            fout.write(any2unicode(line))\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 1))\n\t    self.assertEqual(offsets, [0])\n\t    self.assertEqual(start_doctags, [0])\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 2))\n\t    self.assertEqual(offsets, [0, 12])\n\t    self.assertEqual(start_doctags, [0, 2])\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 3))\n\t    self.assertEqual(offsets, [0, 6, 18])\n\t    self.assertEqual(start_doctags, [0, 1, 3])\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 4))\n\t    self.assertEqual(offsets, [0, 6, 12, 18])\n\t    self.assertEqual(start_doctags, [0, 1, 2, 3])\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 5))\n\t    self.assertEqual(offsets, [0, 6, 12, 18, 24])\n\t    self.assertEqual(start_doctags, [0, 1, 2, 3, 4])\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 6))\n\t    self.assertEqual(offsets, [0, 0, 6, 12, 18, 24])\n\t    self.assertEqual(start_doctags, [0, 0, 1, 2, 3, 4])\n\t\nTestDoc2VecModel().test_get_offsets_and_start_doctags()\n", "code_start": "from __future__ import with_statement\nfrom __future__ import division\n", "test_path": "../srcdata/Computation/gensim/gensim/test/test_doc2vec.py"}, {"test_code": "import logging\nimport unittest\nimport os\nfrom collections import namedtuple\nimport numpy as np\nfrom gensim import utils\nfrom gensim.models import doc2vec\nfrom gensim.models import keyedvectors\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import temporary_file\nfrom gensim.test.utils import common_texts as raw_sentences\nfrom gensim.models.word2vec_corpusfile import CythonLineSentence\n\nclass TestDoc2VecModel(unittest.TestCase):\n\tdef test_cython_linesentence_readline_after_getting_offsets(self):\n\t    lines = ['line1\\n', 'line2\\n', 'line3\\n', 'line4\\n', 'line5\\n']\n\t    tmpf = get_tmpfile('gensim_doc2vec.tst')\n\t    with utils.open(tmpf, 'wb', encoding='utf8') as fout:\n\t        for line in lines:\n\t            fout.write(any2unicode(line))\n\t    from gensim.models.word2vec_corpusfile import CythonLineSentence\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 5))\n\t    for offset, line in zip(offsets, lines):\n\t        ls = CythonLineSentence(tmpf, offset)\n\t        sentence = ls.read_sentence()\n\t        self.assertEqual(len(sentence), 1)\n\t        self.assertEqual(sentence[0], utils.any2utf8(line.strip()))\n\t\nTestDoc2VecModel().test_cython_linesentence_readline_after_getting_offsets()\n", "code_start": "from __future__ import with_statement\nfrom __future__ import division\n", "test_path": "../srcdata/Computation/gensim/gensim/test/test_doc2vec.py"}], "instruction": "Functionality: The 'any2unicode' function is designed to convert a given input text to a unicode string. It is capable of handling both bytestrings and unicode strings as input. For bytestrings, it uses the specified encoding to decode the input into a unicode string.\n\nInputs: \n- text (str): The input text to be converted. It can be either a bytestring or a unicode string.\n- encoding (str, optional): The encoding to use for decoding the input text if it is a bytestring. The default is 'utf8'.\n- errors (str, optional): The error handling behavior if the input text is a bytestring. The default is 'strict', which means that encoding errors raise a UnicodeDecodeError.\n\nOutputs: \n- str: The function returns a unicode string, which is the converted form of the input text. If the input is already a unicode string, it is returned as is.", "method_code_mask": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n\n\ndef any2unicode(text, encoding='utf8', errors='strict'): [MASK]\n"}
{"method_name": "isbow", "full_method_name": "isbow", "method_path": "../srcdata/Computation/gensim/gensim/matutils.py", "method_code": "from __future__ import with_statement\nimport logging\nimport math\nfrom gensim import utils\nimport numpy as np\nimport scipy.sparse\nfrom scipy.stats import entropy\nfrom scipy.linalg import get_blas_funcs\nfrom scipy.linalg.lapack import get_lapack_funcs\nfrom scipy.special import psi\nfrom numpy import triu\nfrom scipy.linalg import triu\nfrom gensim._matutils import logsumexp\nfrom gensim._matutils import mean_absolute_difference\nfrom gensim._matutils import dirichlet_expectation\nfrom gensim.corpora._mmreader import MmReader\ndef isbow(vec):\n    \"\"\"Checks if a vector is in the sparse Gensim bag-of-words format.\n\n    Parameters\n    ----------\n    vec : object\n        Object to check.\n\n    Returns\n    -------\n    bool\n        Is `vec` in BoW format.\n\n    \"\"\"\n    if scipy.sparse.issparse(vec):\n        vec = vec.todense().tolist()\n    try:\n        id_, val_ = vec[0]\n        int(id_), float(val_)\n    except IndexError:\n        return True\n    except (ValueError, TypeError):\n        return False\n    return True", "test_code_list": [{"test_code": "import logging\nimport unittest\nfrom gensim import matutils\nfrom scipy.sparse import csr_matrix\nimport numpy as np\nimport math\nfrom gensim.corpora.mmcorpus import MmCorpus\nfrom gensim.models import ldamodel\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import common_dictionary\nfrom gensim.test.utils import common_corpus\n\nclass TestIsBow(unittest.TestCase):\n\tdef test_None(self):\n\t    result = isbow(None)\n\t    expected = False\n\t    self.assertEqual(expected, result)\n\t\nTestIsBow().test_None()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_similarity_metrics.py"}, {"test_code": "import logging\nimport unittest\nfrom gensim import matutils\nfrom scipy.sparse import csr_matrix\nimport numpy as np\nimport math\nfrom gensim.corpora.mmcorpus import MmCorpus\nfrom gensim.models import ldamodel\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import common_dictionary\nfrom gensim.test.utils import common_corpus\n\nclass TestIsBow(unittest.TestCase):\n\tdef test_bow(self):\n\t    potentialbow = [(0, 0.4)]\n\t    result = isbow(potentialbow)\n\t    expected = True\n\t    self.assertEqual(expected, result)\n\t    potentialbow = [(0, 4.0), (1, 2.0), (2, 5.0), (3, 8.0)]\n\t    result = isbow(potentialbow)\n\t    expected = True\n\t    self.assertEqual(expected, result)\n\t    potentialbow = []\n\t    result = isbow(potentialbow)\n\t    expected = True\n\t    self.assertEqual(expected, result)\n\t    potentialbow = [[(2, 1), (3, 1), (4, 1), (5, 1), (1, 1), (7, 1)]]\n\t    result = isbow(potentialbow)\n\t    expected = False\n\t    self.assertEqual(expected, result)\n\t    potentialbow = [(1, 3, 6)]\n\t    result = isbow(potentialbow)\n\t    expected = False\n\t    self.assertEqual(expected, result)\n\t    potentialbow = csr_matrix([[1, 0.4], [0, 0.3], [2, 0.1]])\n\t    result = isbow(potentialbow)\n\t    expected = True\n\t    self.assertEqual(expected, result)\n\t    potentialbow = np.array([[1, 0.4], [0, 0.2], [2, 0.2]])\n\t    result = isbow(potentialbow)\n\t    expected = True\n\t    self.assertEqual(expected, result)\n\t\nTestIsBow().test_bow()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_similarity_metrics.py"}], "instruction": "Functionality: The function isbow checks whether a given vector is in the sparse Gensim bag-of-words (BoW) format. It supports both dense and sparse representations of the vector, converting sparse vectors to a dense format for the check if necessary.\n\nInputs: \n- vec: An object of any type. This object could represent a vector in either a dense or sparse format, and the function will determine if it adheres to the BoW format commonly used in text processing with Gensim.\n\nOutputs:\n- bool: A boolean value indicating whether the input vector is in the BoW format. True if the vector is in the BoW format, and False otherwise.", "method_code_mask": "from __future__ import with_statement\nimport logging\nimport math\nfrom gensim import utils\nimport numpy as np\nimport scipy.sparse\nfrom scipy.stats import entropy\nfrom scipy.linalg import get_blas_funcs\nfrom scipy.linalg.lapack import get_lapack_funcs\nfrom scipy.special import psi\nfrom numpy import triu\nfrom scipy.linalg import triu\nfrom gensim._matutils import logsumexp\nfrom gensim._matutils import mean_absolute_difference\nfrom gensim._matutils import dirichlet_expectation\nfrom gensim.corpora._mmreader import MmReader\n\n\ndef isbow(vec): [MASK]\n"}
{"method_name": "any2utf8", "full_method_name": "any2utf8", "method_path": "../srcdata/Computation/gensim/gensim/utils.py", "method_code": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\ndef any2utf8(text, errors='strict', encoding='utf8'):\n    \"\"\"Convert a unicode or bytes string in the given encoding into a utf8 bytestring.\n\n    Parameters\n    ----------\n    text : str\n        Input text.\n    errors : str, optional\n        Error handling behaviour if `text` is a bytestring.\n    encoding : str, optional\n        Encoding of `text` if it is a bytestring.\n\n    Returns\n    -------\n    str\n        Bytestring in utf8.\n\n    \"\"\"\n    if isinstance(text, str):\n        return text.encode('utf8')\n    return str(text, encoding, errors=errors).encode('utf8')", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport os\nfrom collections import namedtuple\nimport numpy as np\nfrom gensim import utils\nfrom gensim.models import doc2vec\nfrom gensim.models import keyedvectors\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import temporary_file\nfrom gensim.test.utils import common_texts as raw_sentences\nfrom gensim.models.word2vec_corpusfile import CythonLineSentence\n\nclass TestDoc2VecModel(unittest.TestCase):\n\tdef test_cython_linesentence_readline_after_getting_offsets(self):\n\t    lines = ['line1\\n', 'line2\\n', 'line3\\n', 'line4\\n', 'line5\\n']\n\t    tmpf = get_tmpfile('gensim_doc2vec.tst')\n\t    with utils.open(tmpf, 'wb', encoding='utf8') as fout:\n\t        for line in lines:\n\t            fout.write(utils.any2unicode(line))\n\t    from gensim.models.word2vec_corpusfile import CythonLineSentence\n\t    offsets, start_doctags = (doc2vec.Doc2Vec.\n\t        _get_offsets_and_start_doctags_for_corpusfile(tmpf, 5))\n\t    for offset, line in zip(offsets, lines):\n\t        ls = CythonLineSentence(tmpf, offset)\n\t        sentence = ls.read_sentence()\n\t        self.assertEqual(len(sentence), 1)\n\t        self.assertEqual(sentence[0], any2utf8(line.strip()))\n\t\nTestDoc2VecModel().test_cython_linesentence_readline_after_getting_offsets()\n", "code_start": "from __future__ import with_statement\nfrom __future__ import division\n", "test_path": "../srcdata/Computation/gensim/gensim/test/test_doc2vec.py"}], "instruction": "Functionality: The any2utf8 function is designed to convert a given text, which could be a unicode string or a bytes string in a specific encoding, into a utf8 encoded bytestring. It supports custom error handling and encoding, allowing flexibility in processing text data.\n\nInputs:\n    text : str\n        The input text that needs to be converted. It can be a unicode string or a bytes string.\n    errors : str, optional\n        Specifies the error handling behavior when decoding bytes. Default is 'strict', which raises an exception for encoding errors. Other common values are 'ignore' to ignore errors, 'replace' to replace undecodable bytes with a replacement character, and 'backslashreplace' to use a backslash replacement.\n    encoding : str, optional\n        The encoding of the bytes string if it is not already in utf8. Default is 'utf8'. Common alternative encodings include 'latin1', 'ascii', and 'iso-8859-1'.\n\nOutputs:\n    str\n        The converted text as a utf8 encoded bytestring.", "method_code_mask": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n\n\ndef any2utf8(text, errors='strict', encoding='utf8'): [MASK]\n"}
{"method_name": "manual_unitvec", "full_method_name": "manual_unitvec", "method_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py", "method_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\ndef manual_unitvec(vec):\n    vec = vec.astype(float)\n    if sparse.issparse(vec):\n        vec_sum_of_squares = vec.multiply(vec)\n        unit = 1.0 / np.sqrt(vec_sum_of_squares.sum())\n        return vec.multiply(unit)\n    elif not sparse.issparse(vec):\n        sum_vec_squared = np.sum(vec ** 2)\n        vec /= np.sqrt(sum_vec_squared)\n        return vec", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_sparse_npfloat32(self):\n\t    input_vector = sparse.csr_matrix(np.asarray([[1, 0, 0, 0, 3], [0, 0, 4,\n\t        3, 0]])).astype(np.float32)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector.data, man_unit_vector.data,\n\t        atol=0.001))\n\t    self.assertEqual(input_vector.dtype, unit_vector.dtype)\n\t\nUnitvecTestCase().test_sparse_npfloat32()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_sparse_npfloat64(self):\n\t    input_vector = sparse.csr_matrix(np.asarray([[1, 0, 0, 0, 3], [0, 0, 4,\n\t        3, 0]])).astype(np.float64)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector.data, man_unit_vector.data,\n\t        atol=0.001))\n\t    self.assertEqual(input_vector.dtype, unit_vector.dtype)\n\t\nUnitvecTestCase().test_sparse_npfloat64()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_sparse_npint32(self):\n\t    input_vector = sparse.csr_matrix(np.asarray([[1, 0, 0, 0, 3], [0, 0, 4,\n\t        3, 0]])).astype(np.int32)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector.data, man_unit_vector.data,\n\t        atol=0.001))\n\t    self.assertTrue(np.issubdtype(unit_vector.dtype, np.floating))\n\t\nUnitvecTestCase().test_sparse_npint32()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_sparse_npint64(self):\n\t    input_vector = sparse.csr_matrix(np.asarray([[1, 0, 0, 0, 3], [0, 0, 4,\n\t        3, 0]])).astype(np.int64)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector.data, man_unit_vector.data,\n\t        atol=0.001))\n\t    self.assertTrue(np.issubdtype(unit_vector.dtype, np.floating))\n\t\nUnitvecTestCase().test_sparse_npint64()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_dense_npfloat32(self):\n\t    input_vector = np.random.uniform(size=(5,)).astype(np.float32)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector, man_unit_vector))\n\t    self.assertEqual(input_vector.dtype, unit_vector.dtype)\n\t\nUnitvecTestCase().test_dense_npfloat32()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_dense_npfloat64(self):\n\t    input_vector = np.random.uniform(size=(5,)).astype(np.float64)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector, man_unit_vector))\n\t    self.assertEqual(input_vector.dtype, unit_vector.dtype)\n\t\nUnitvecTestCase().test_dense_npfloat64()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_dense_npint32(self):\n\t    input_vector = np.random.randint(10, size=5).astype(np.int32)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector, man_unit_vector))\n\t    self.assertTrue(np.issubdtype(unit_vector.dtype, np.floating))\n\t\nUnitvecTestCase().test_dense_npint32()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_dense_npint64(self):\n\t    input_vector = np.random.randint(10, size=5).astype(np.int32)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector, man_unit_vector))\n\t    self.assertTrue(np.issubdtype(unit_vector.dtype, np.floating))\n\t\nUnitvecTestCase().test_dense_npint64()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_sparse_python_float(self):\n\t    input_vector = sparse.csr_matrix(np.asarray([[1, 0, 0, 0, 3], [0, 0, 4,\n\t        3, 0]])).astype(float)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector.data, man_unit_vector.data,\n\t        atol=0.001))\n\t    self.assertEqual(input_vector.dtype, unit_vector.dtype)\n\t\nUnitvecTestCase().test_sparse_python_float()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_sparse_python_int(self):\n\t    input_vector = sparse.csr_matrix(np.asarray([[1, 0, 0, 0, 3], [0, 0, 4,\n\t        3, 0]])).astype(int)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector.data, man_unit_vector.data,\n\t        atol=0.001))\n\t    self.assertTrue(np.issubdtype(unit_vector.dtype, np.floating))\n\t\nUnitvecTestCase().test_sparse_python_int()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_dense_python_float(self):\n\t    input_vector = np.random.uniform(size=(5,)).astype(float)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector, man_unit_vector))\n\t    self.assertEqual(input_vector.dtype, unit_vector.dtype)\n\t\nUnitvecTestCase().test_dense_python_float()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}, {"test_code": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\nclass UnitvecTestCase(unittest.TestCase):\n\tdef test_dense_python_int(self):\n\t    input_vector = np.random.randint(10, size=5).astype(int)\n\t    unit_vector = matutils.unitvec(input_vector)\n\t    man_unit_vector = manual_unitvec(input_vector)\n\t    self.assertTrue(np.allclose(unit_vector, man_unit_vector))\n\t    self.assertTrue(np.issubdtype(unit_vector.dtype, np.floating))\n\t\nUnitvecTestCase().test_dense_python_int()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_matutils.py"}], "instruction": "Functionality: The function 'manual_unitvec' is designed to normalize a given vector, which could be either a dense array or a sparse matrix, to a unit vector. Normalization is the process of scaling the vector to have a unit norm (Euclidean length of 1).\n\nInputs: \n- vec: A vector to be normalized. This input can either be a dense array (NumPy array) or a sparse matrix (SciPy sparse matrix). The vector can contain numeric values.\n\nOutputs: \n- Returns the normalized vector. If the input was a sparse matrix, the output will also be a sparse matrix. If the input was a dense array, the output will be a dense array (NumPy array). The returned vector will have a unit norm, meaning the Euclidean length of the vector will be equal to 1.", "method_code_mask": "import logging\nimport unittest\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom scipy import sparse\nfrom scipy.sparse import csc_matrix\nfrom scipy.special import psi\nimport gensim.matutils as matutils\n\n\ndef manual_unitvec(vec): [MASK]\n"}
{"method_name": "get_random_state", "full_method_name": "get_random_state", "method_path": "../srcdata/Computation/gensim/gensim/utils.py", "method_code": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\ndef get_random_state(seed):\n    \"\"\"Generate :class:`numpy.random.RandomState` based on input seed.\n\n    Parameters\n    ----------\n    seed : {None, int, array_like}\n        Seed for random state.\n\n    Returns\n    -------\n    :class:`numpy.random.RandomState`\n        Random state.\n\n    Raises\n    ------\n    AttributeError\n        If seed is not {None, int, array_like}.\n\n    Notes\n    -----\n    Method originally from `maciejkula/glove-python <https://github.com/maciejkula/glove-python>`_\n    and written by `@joshloyal <https://github.com/joshloyal>`_.\n\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (numbers.Integral, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        '%r cannot be used to seed a np.random.RandomState instance' % seed)", "test_code_list": [{"test_code": "import logging\nimport numbers\nimport os\nimport unittest\nimport copy\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gensim.corpora import mmcorpus\nfrom gensim.corpora import Dictionary\nfrom gensim.models import ldamodel\nfrom gensim.models import ldamulticore\nfrom gensim import matutils\nfrom gensim import utils\nfrom gensim.test import basetmtests\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import common_texts\ndef test_random_state():\n    testcases = [np.random.seed(0), None, np.random.RandomState(0), 0]\n    for testcase in testcases:\n        assert isinstance(get_random_state(testcase), np.random.\n            RandomState)\n\ntest_random_state()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_ldamodel.py"}], "instruction": "Functionality: \nThe get_random_state function is designed to generate a numpy.random.RandomState object based on the provided seed. This allows for the creation of a random number generator that can be used to generate reproducible random numbers.\n\nInputs:\nseed : {None, int, array_like}\n    The seed used to initialize the random number generator. If None, the current numpy random state is returned. If an integer or array-like object is provided, a new numpy random state is created using the given seed.\n\nOutputs:\n:class:`numpy.random.RandomState`\n    A numpy random state object. This object can be used to generate random numbers using numpy's random module functions.\n\nRaises:\nAttributeError\n    If the seed is not of type {None, int, array_like}, an AttributeError is raised.\n\nNotes:\nThe method was originally taken from the maciejkula/glove-python repository on GitHub, written by @joshloyal, and has been adapted for use in this context.", "method_code_mask": "from contextlib import contextmanager\nimport collections.abc\nimport logging\nimport warnings\nimport numbers\nfrom html.entities import name2codepoint as n2cp\nimport pickle as _pickle\nimport re\nimport unicodedata\nimport os\nimport random\nimport itertools\nimport tempfile\nfrom functools import wraps\nimport multiprocessing\nimport shutil\nimport sys\nimport subprocess\nimport inspect\nimport heapq\nfrom copy import deepcopy\nfrom datetime import datetime\nimport platform\nimport types\nimport numpy as np\nimport scipy.sparse\nfrom smart_open import open\nfrom gensim import __version__ as gensim_version\nimport socket\n\n\ndef get_random_state(seed): [MASK]\n"}
{"method_name": "sparse2full", "full_method_name": "sparse2full", "method_path": "../srcdata/Computation/gensim/gensim/matutils.py", "method_code": "from __future__ import with_statement\nimport logging\nimport math\nfrom gensim import utils\nimport numpy as np\nimport scipy.sparse\nfrom scipy.stats import entropy\nfrom scipy.linalg import get_blas_funcs\nfrom scipy.linalg.lapack import get_lapack_funcs\nfrom scipy.special import psi\nfrom numpy import triu\nfrom scipy.linalg import triu\nfrom gensim._matutils import logsumexp\nfrom gensim._matutils import mean_absolute_difference\nfrom gensim._matutils import dirichlet_expectation\nfrom gensim.corpora._mmreader import MmReader\ndef sparse2full(doc, length):\n    \"\"\"Convert a document in Gensim bag-of-words format into a dense numpy array.\n\n    Parameters\n    ----------\n    doc : list of (int, number)\n        Document in BoW format.\n    length : int\n        Vector dimensionality. This cannot be inferred from the BoW, and you must supply it explicitly.\n        This is typically the vocabulary size or number of topics, depending on how you created `doc`.\n\n    Returns\n    -------\n    numpy.ndarray\n        Dense numpy vector for `doc`.\n\n    See Also\n    --------\n    :func:`~gensim.matutils.full2sparse`\n        Convert dense array to gensim bag-of-words format.\n\n    \"\"\"\n    result = np.zeros(length, dtype=np.float32)\n    doc = ((int(id_), float(val_)) for id_, val_ in doc)\n    doc = dict(doc)\n    result[list(doc)] = list(doc.values())\n    return result", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport math\nimport os\nimport numpy\nimport scipy\nfrom gensim import utils\nfrom gensim.corpora import Dictionary\nfrom gensim.models import word2vec\nfrom gensim.models import doc2vec\nfrom gensim.models import KeyedVectors\nfrom gensim.models import TfidfModel\nfrom gensim import matutils\nfrom gensim import similarities\nfrom gensim.models import Word2Vec\nfrom gensim.models import FastText\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import common_texts as TEXTS\nfrom gensim.test.utils import common_dictionary as DICTIONARY\nfrom gensim.test.utils import common_corpus as CORPUS\nfrom gensim.similarities import UniformTermSimilarityIndex\nfrom gensim.similarities import WordEmbeddingSimilarityIndex\nfrom gensim.similarities import SparseTermSimilarityMatrix\nfrom gensim.similarities import LevenshteinSimilarityIndex\nfrom gensim.similarities.docsim import _nlargest\nfrom gensim.similarities.fastss import editdist\nfrom gensim.similarities.annoy import AnnoyIndexer\n@unittest.skip('skipping abstract base class')\nclass _TestSimilarityABC(unittest.TestCase):\n    \"\"\"\n    Base class for SparseMatrixSimilarity and MatrixSimilarity unit tests.\n    \"\"\"\n\n    def factoryMethod(self):\n        \"\"\"Creates a SimilarityABC instance.\"\"\"\n        return self.cls(CORPUS, num_features=len(DICTIONARY))\n\n    def test_full(self, num_best=None, shardsize=100):\n        if self.cls == similarities.Similarity:\n            index = self.cls(None, CORPUS, num_features=len(DICTIONARY),\n                shardsize=shardsize)\n        else:\n            index = self.cls(CORPUS, num_features=len(DICTIONARY))\n        if isinstance(index, similarities.MatrixSimilarity):\n            expected = numpy.array([[0.57735026, 0.57735026, 0.57735026, \n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, \n                0.40824831, 0.0, 0.40824831, 0.40824831, 0.40824831, \n                0.40824831, 0.40824831, 0.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0], [0.0, 0.0, \n                0.40824831, 0.0, 0.0, 0.0, 0.81649661, 0.0, 0.40824831, 0.0,\n                0.0, 0.0], [0.0, 0.0, 0.0, 0.57735026, 0.57735026, 0.0, 0.0,\n                0.57735026, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, \n                0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, \n                0.0, 0.0, 0.0, 0.0, 0.0, 0.70710677, 0.70710677, 0.0], [0.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57735026, \n                0.57735026, 0.57735026], [0.0, 0.0, 0.0, 0.0, 0.0, \n                0.57735026, 0.0, 0.0, 0.0, 0.0, 0.57735026, 0.57735026]],\n                dtype=numpy.float32)\n            self.assertTrue(numpy.allclose(sorted(expected.flat), sorted(\n                index.index.flat)))\n        index.num_best = num_best\n        query = CORPUS[0]\n        sims = index[query]\n        expected = [(0, 0.99999994), (2, 0.28867513), (3, 0.23570226), (1, \n            0.23570226)][:num_best]\n        expected = sparse2full(expected, len(index))\n        if num_best is not None:\n            sims = sparse2full(sims, len(index))\n        self.assertTrue(numpy.allclose(expected, sims))\n        if self.cls == similarities.Similarity:\n            index.destroy()\n\n    def test_num_best(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        for num_best in [None, 0, 1, 9, 1000]:\n            self.testFull(num_best=num_best)\n\n    def test_full2sparse_clipped(self):\n        vec = [0.8, 0.2, 0.0, 0.0, -0.1, -0.15]\n        expected = [(0, 0.8), (1, 0.2), (5, -0.15)]\n        self.assertTrue(matutils.full2sparse_clipped(vec, topn=3), expected)\n\n    def test_scipy2scipy_clipped(self):\n        vec = [0.8, 0.2, 0.0, 0.0, -0.1, -0.15]\n        expected = [(0, 0.8), (1, 0.2), (5, -0.15)]\n        vec_scipy = scipy.sparse.csr_matrix(vec)\n        vec_scipy_clipped = matutils.scipy2scipy_clipped(vec_scipy, topn=3)\n        self.assertTrue(scipy.sparse.issparse(vec_scipy_clipped))\n        self.assertTrue(matutils.scipy2sparse(vec_scipy_clipped), expected)\n        vec = [0.8, 0.2, 0.0, 0.0, -0.1, -0.15]\n        expected = [(0, 0.8), (1, 0.2), (5, -0.15)]\n        matrix_scipy = scipy.sparse.csr_matrix([vec] * 3)\n        matrix_scipy_clipped = matutils.scipy2scipy_clipped(matrix_scipy,\n            topn=3)\n        self.assertTrue(scipy.sparse.issparse(matrix_scipy_clipped))\n        self.assertTrue([matutils.scipy2sparse(x) for x in\n            matrix_scipy_clipped], [expected] * 3)\n\n    def test_empty_query(self):\n        index = self.factoryMethod()\n        if isinstance(index, similarities.WmdSimilarity) and not POT_EXT:\n            self.skipTest('POT not installed')\n        query = []\n        try:\n            sims = index[query]\n            self.assertTrue(sims is not None)\n        except IndexError:\n            self.assertTrue(False)\n\n    def test_chunking(self):\n        if self.cls == similarities.Similarity:\n            index = self.cls(None, CORPUS, num_features=len(DICTIONARY),\n                shardsize=5)\n        else:\n            index = self.cls(CORPUS, num_features=len(DICTIONARY))\n        query = CORPUS[:3]\n        sims = index[query]\n        expected = numpy.array([[0.99999994, 0.23570226, 0.28867513, \n            0.23570226, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23570226, 1.0, \n            0.40824831, 0.33333334, 0.70710677, 0.0, 0.0, 0.0, 0.23570226],\n            [0.28867513, 0.40824831, 1.0, 0.61237246, 0.28867513, 0.0, 0.0,\n            0.0, 0.0]], dtype=numpy.float32)\n        self.assertTrue(numpy.allclose(expected, sims))\n        index.num_best = 3\n        sims = index[query]\n        expected = [[(0, 0.99999994), (2, 0.28867513), (1, 0.23570226)], [(\n            1, 1.0), (4, 0.70710677), (2, 0.40824831)], [(2, 1.0), (3, \n            0.61237246), (1, 0.40824831)]]\n        self.assertTrue(numpy.allclose(expected, sims))\n        if self.cls == similarities.Similarity:\n            index.destroy()\n\n    def test_iter(self):\n        if self.cls == similarities.Similarity:\n            index = self.cls(None, CORPUS, num_features=len(DICTIONARY),\n                shardsize=5)\n        else:\n            index = self.cls(CORPUS, num_features=len(DICTIONARY))\n        sims = [sim for sim in index]\n        expected = numpy.array([[0.99999994, 0.23570226, 0.28867513, \n            0.23570226, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23570226, 1.0, \n            0.40824831, 0.33333334, 0.70710677, 0.0, 0.0, 0.0, 0.23570226],\n            [0.28867513, 0.40824831, 1.0, 0.61237246, 0.28867513, 0.0, 0.0,\n            0.0, 0.0], [0.23570226, 0.33333334, 0.61237246, 1.0, 0.0, 0.0, \n            0.0, 0.0, 0.0], [0.0, 0.70710677, 0.28867513, 0.0, 0.99999994, \n            0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.70710677,\n            0.57735026, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.70710677, \n            0.99999994, 0.81649655, 0.40824828], [0.0, 0.0, 0.0, 0.0, 0.0, \n            0.57735026, 0.81649655, 0.99999994, 0.66666663], [0.0, \n            0.23570226, 0.0, 0.0, 0.0, 0.0, 0.40824828, 0.66666663, \n            0.99999994]], dtype=numpy.float32)\n        self.assertTrue(numpy.allclose(expected, sims))\n        if self.cls == similarities.Similarity:\n            index.destroy()\n\n    def test_persistency(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl')\n        index = self.factoryMethod()\n        index.save(fname)\n        index2 = self.cls.load(fname)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_persistency_compressed(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl.gz')\n        index = self.factoryMethod()\n        index.save(fname)\n        index2 = self.cls.load(fname)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_large(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        index2 = self.cls.load(fname)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_large_compressed(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl.gz')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        index2 = self.cls.load(fname, mmap=None)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_mmap(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        index2 = self.cls.load(fname, mmap='r')\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_mmap_compressed(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl.gz')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        self.assertRaises(IOError, self.cls.load, fname, mmap='r')\n\nclass TestSimilarity(_TestSimilarityABC):\n\tdef test_reopen(self):\n\t    \"\"\"test re-opening partially full shards\"\"\"\n\t    index = similarities.Similarity(None, CORPUS[:5], num_features=len(\n\t        DICTIONARY), shardsize=9)\n\t    _ = index[CORPUS[0]]\n\t    index.add_documents(CORPUS[5:])\n\t    query = CORPUS[0]\n\t    sims = index[query]\n\t    expected = [(0, 0.99999994), (2, 0.28867513), (3, 0.23570226), (1, \n\t        0.23570226)]\n\t    expected = sparse2full(expected, len(index))\n\t    self.assertTrue(numpy.allclose(expected, sims))\n\t    index.destroy()\n\t\nTestSimilarity().test_reopen()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_similarities.py"}], "instruction": "Functionality: The sparse2full function converts a document represented in Gensim's bag-of-words (BoW) format into a dense numpy array. This is useful for converting sparse representations of documents into a format more suitable for certain types of analysis or machine learning algorithms that require dense vectors.\n\nInputs:\n- doc: A list of tuples, where each tuple consists of an integer (representing the index of a word or feature) and a number (representing the frequency or weight of that word or feature in the document).\n- length: An integer specifying the intended length of the dense vector. This is typically the vocabulary size or the number of topics if the document is based on a topic model, and it must be provided explicitly.\n\nOutputs:\n- A numpy.ndarray representing the dense vector form of the input document. The length of this array will match the 'length' input parameter. Each element in the array corresponds to the frequency or weight of the word or feature at that index in the document. If a word or feature is not present in the document, its corresponding index in the array will have a value of 0.", "method_code_mask": "from __future__ import with_statement\nimport logging\nimport math\nfrom gensim import utils\nimport numpy as np\nimport scipy.sparse\nfrom scipy.stats import entropy\nfrom scipy.linalg import get_blas_funcs\nfrom scipy.linalg.lapack import get_lapack_funcs\nfrom scipy.special import psi\nfrom numpy import triu\nfrom scipy.linalg import triu\nfrom gensim._matutils import logsumexp\nfrom gensim._matutils import mean_absolute_difference\nfrom gensim._matutils import dirichlet_expectation\nfrom gensim.corpora._mmreader import MmReader\n\n\ndef sparse2full(doc, length): [MASK]\n"}
{"method_name": "pseudorandom_weak_vector", "full_method_name": "pseudorandom_weak_vector", "method_path": "../srcdata/Computation/gensim/gensim/models/keyedvectors.py", "method_code": "import logging\nimport sys\nimport itertools\nimport warnings\nfrom numbers import Integral\nfrom typing import Iterable\nfrom numpy import dot\nfrom numpy import float32 as REAL\nfrom numpy import double\nfrom numpy import zeros\nfrom numpy import vstack\nfrom numpy import ndarray\nfrom numpy import sum as np_sum\nfrom numpy import prod\nfrom numpy import argmax\nfrom numpy import dtype\nfrom numpy import ascontiguousarray\nfrom numpy import frombuffer\nimport numpy as np\nfrom scipy import stats\nfrom scipy.spatial.distance import cdist\nfrom gensim import utils\nfrom gensim import matutils\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.utils import deprecated\ndef pseudorandom_weak_vector(size, seed_string=None, hashfxn=hash):\n    \"\"\"Get a random vector, derived deterministically from `seed_string` if supplied.\n\n    Useful for initializing KeyedVectors that will be the starting projection/input layers of _2Vec models.\n\n    \"\"\"\n    if seed_string:\n        once = np.random.Generator(np.random.SFC64(hashfxn(seed_string) & \n            4294967295))\n    else:\n        once = utils.default_prng\n    return (once.random(size).astype(REAL) - 0.5) / size", "test_code_list": [{"test_code": "import functools\nimport logging\nimport unittest\nimport numpy as np\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.models.keyedvectors import REAL\nfrom gensim.models.keyedvectors import pseudorandom_weak_vector\nfrom gensim.test.utils import datapath\nimport gensim.models.keyedvectors\n\nclass TestKeyedVectors(unittest.TestCase):\n\tdef test_save_reload(self):\n\t    randkv = KeyedVectors(vector_size=100)\n\t    count = 20\n\t    keys = [str(i) for i in range(count)]\n\t    weights = [pseudorandom_weak_vector(randkv.vector_size) for _ in range(\n\t        count)]\n\t    randkv.add_vectors(keys, weights)\n\t    tmpfiletxt = gensim.test.utils.get_tmpfile('tmp_kv.txt')\n\t    randkv.save_word2vec_format(tmpfiletxt, binary=False)\n\t    reloadtxtkv = KeyedVectors.load_word2vec_format(tmpfiletxt, binary=False)\n\t    self.assertEqual(randkv.index_to_key, reloadtxtkv.index_to_key)\n\t    self.assertTrue((randkv.vectors == reloadtxtkv.vectors).all())\n\t    tmpfilebin = gensim.test.utils.get_tmpfile('tmp_kv.bin')\n\t    randkv.save_word2vec_format(tmpfilebin, binary=True)\n\t    reloadbinkv = KeyedVectors.load_word2vec_format(tmpfilebin, binary=True)\n\t    self.assertEqual(randkv.index_to_key, reloadbinkv.index_to_key)\n\t    self.assertTrue((randkv.vectors == reloadbinkv.vectors).all())\n\t\nTestKeyedVectors().test_save_reload()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_keyedvectors.py"}, {"test_code": "import functools\nimport logging\nimport unittest\nimport numpy as np\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.models.keyedvectors import REAL\nfrom gensim.models.keyedvectors import pseudorandom_weak_vector\nfrom gensim.test.utils import datapath\nimport gensim.models.keyedvectors\n\nclass TestKeyedVectors(unittest.TestCase):\n\tdef test_no_header(self):\n\t    randkv = KeyedVectors(vector_size=100)\n\t    count = 20\n\t    keys = [str(i) for i in range(count)]\n\t    weights = [pseudorandom_weak_vector(randkv.vector_size) for _ in range(\n\t        count)]\n\t    randkv.add_vectors(keys, weights)\n\t    tmpfiletxt = gensim.test.utils.get_tmpfile('tmp_kv.txt')\n\t    randkv.save_word2vec_format(tmpfiletxt, binary=False, write_header=False)\n\t    reloadtxtkv = KeyedVectors.load_word2vec_format(tmpfiletxt, binary=\n\t        False, no_header=True)\n\t    self.assertEqual(randkv.index_to_key, reloadtxtkv.index_to_key)\n\t    self.assertTrue((randkv.vectors == reloadtxtkv.vectors).all())\n\t\nTestKeyedVectors().test_no_header()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_keyedvectors.py"}], "instruction": "Functionality: The function 'pseudorandom_weak_vector' generates a pseudo-random vector of a specified size. The vector is deterministically derived from a seed string if provided, which can be useful for initializing KeyedVectors in _2Vec models.\n\nInputs: \n1. size (Integral): The size of the vector to be generated.\n2. seed_string (str, optional): A string from which the vector is deterministically derived. If not provided, a default pseudo-random generator is used.\n3. hashfxn (function, optional): The hash function used to process the seed_string. By default, it is the built-in hash function.\n\nOutputs: \n1. vector (ndarray): A pseudo-random vector of the specified size, with elements normalized by the size and adjusted by subtracting 0.5.", "method_code_mask": "import logging\nimport sys\nimport itertools\nimport warnings\nfrom numbers import Integral\nfrom typing import Iterable\nfrom numpy import dot\nfrom numpy import float32 as REAL\nfrom numpy import double\nfrom numpy import zeros\nfrom numpy import vstack\nfrom numpy import ndarray\nfrom numpy import sum as np_sum\nfrom numpy import prod\nfrom numpy import argmax\nfrom numpy import dtype\nfrom numpy import ascontiguousarray\nfrom numpy import frombuffer\nimport numpy as np\nfrom scipy import stats\nfrom scipy.spatial.distance import cdist\nfrom gensim import utils\nfrom gensim import matutils\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.utils import deprecated\n\n\ndef pseudorandom_weak_vector(size, seed_string=None, hashfxn=hash): [MASK]\n"}
{"method_name": "load_native", "full_method_name": "load_native", "method_path": "../srcdata/Computation/gensim/gensim/test/test_fasttext.py", "method_code": "from __future__ import division\nimport gzip\nimport io\nimport logging\nimport unittest\nimport os\nimport shutil\nimport subprocess\nimport struct\nimport sys\nimport numpy as np\nimport pytest\nfrom gensim import utils\nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models.fasttext import FastText as FT_gensim\nfrom gensim.models.fasttext import FastTextKeyedVectors\nfrom gensim.models.fasttext import _unpack\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import temporary_file\nfrom gensim.test.utils import common_texts as sentences\nfrom gensim.test.utils import lee_corpus_list as list_corpus\nimport gensim.models._fasttext_bin\nfrom gensim.models.fasttext_inner import compute_ngrams\nfrom gensim.models.fasttext_inner import compute_ngrams_bytes\nfrom gensim.models.fasttext_inner import ft_hash_bytes\nimport gensim.models.fasttext\ndef load_native():\n    path = datapath('toy-model.bin')\n    model = gensim.models.fasttext.load_facebook_model(path)\n    return model", "test_code_list": [{"test_code": "import gzip\nimport io\nimport logging\nimport unittest\nimport os\nimport shutil\nimport subprocess\nimport struct\nimport sys\nimport numpy as np\nimport pytest\nfrom gensim import utils\nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models.fasttext import FastText as FT_gensim\nfrom gensim.models.fasttext import FastTextKeyedVectors\nfrom gensim.models.fasttext import _unpack\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import temporary_file\nfrom gensim.test.utils import common_texts as sentences\nfrom gensim.test.utils import lee_corpus_list as list_corpus\nimport gensim.models._fasttext_bin\nfrom gensim.models.fasttext_inner import compute_ngrams\nfrom gensim.models.fasttext_inner import compute_ngrams_bytes\nfrom gensim.models.fasttext_inner import ft_hash_bytes\nimport gensim.models.fasttext\n\nclass HashCompatibilityTest(unittest.TestCase):\n\tdef test_hash_native(self):\n\t    m = load_native()\n\t    self.assertTrue(m.wv.compatible_hash)\n\t\nHashCompatibilityTest().test_hash_native()\n", "code_start": "from __future__ import division\n", "test_path": "../srcdata/Computation/gensim/gensim/test/test_fasttext.py"}], "instruction": "Functionality: The function load_native is designed to load a pre-trained FastText model stored in a binary format. This function interacts with the gensim library, specifically using the load_facebook_model method from the gensim.models.fasttext module to read the model from the designated file path and return the model object.\n\nInputs: \n- No explicit input arguments are required for this function. The file path for the pre-trained model is hardcoded within the function as 'toy-model.bin'. This string is passed to the gensim.models.fasttext.load_facebook_model method to load the model.\n\nOutputs:\n- The function returns the loaded FastText model as a gensim.models.fasttext.FastText object. This model object, once returned, can be used for various natural language processing tasks, such as word vector lookup, similarity comparisons, and other linguistic analyses.", "method_code_mask": "from __future__ import division\nimport gzip\nimport io\nimport logging\nimport unittest\nimport os\nimport shutil\nimport subprocess\nimport struct\nimport sys\nimport numpy as np\nimport pytest\nfrom gensim import utils\nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models.fasttext import FastText as FT_gensim\nfrom gensim.models.fasttext import FastTextKeyedVectors\nfrom gensim.models.fasttext import _unpack\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import temporary_file\nfrom gensim.test.utils import common_texts as sentences\nfrom gensim.test.utils import lee_corpus_list as list_corpus\nimport gensim.models._fasttext_bin\nfrom gensim.models.fasttext_inner import compute_ngrams\nfrom gensim.models.fasttext_inner import compute_ngrams_bytes\nfrom gensim.models.fasttext_inner import ft_hash_bytes\nimport gensim.models.fasttext\n\n\ndef load_native(): [MASK]\n"}
{"method_name": "_nlargest", "full_method_name": "_nlargest", "method_path": "../srcdata/Computation/gensim/gensim/similarities/docsim.py", "method_code": "import logging\nimport itertools\nimport os\nimport heapq\nimport warnings\nimport numpy\nimport scipy.sparse\nfrom gensim import interfaces\nfrom gensim import utils\nfrom gensim import matutils\nimport multiprocessing\nimport glob\ndef _nlargest(n, iterable):\n    \"\"\"Helper for extracting n documents with maximum similarity.\n\n    Parameters\n    ----------\n    n : int\n        Number of elements to be extracted\n    iterable : iterable of list of (int, float)\n        Iterable containing documents with computed similarities\n\n    Returns\n    -------\n    :class:`list`\n        List with the n largest elements from the dataset defined by iterable.\n\n    Notes\n    -----\n    Elements are compared by the absolute value of similarity, because negative value of similarity\n    does not mean some form of dissimilarity.\n\n    \"\"\"\n    return heapq.nlargest(n, itertools.chain(*iterable), key=lambda item:\n        abs(item[1]))", "test_code_list": [{"test_code": "import logging\nimport unittest\nimport math\nimport os\nimport numpy\nimport scipy\nfrom gensim import utils\nfrom gensim.corpora import Dictionary\nfrom gensim.models import word2vec\nfrom gensim.models import doc2vec\nfrom gensim.models import KeyedVectors\nfrom gensim.models import TfidfModel\nfrom gensim import matutils\nfrom gensim import similarities\nfrom gensim.models import Word2Vec\nfrom gensim.models import FastText\nfrom gensim.test.utils import datapath\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.test.utils import common_texts as TEXTS\nfrom gensim.test.utils import common_dictionary as DICTIONARY\nfrom gensim.test.utils import common_corpus as CORPUS\nfrom gensim.similarities import UniformTermSimilarityIndex\nfrom gensim.similarities import WordEmbeddingSimilarityIndex\nfrom gensim.similarities import SparseTermSimilarityMatrix\nfrom gensim.similarities import LevenshteinSimilarityIndex\nfrom gensim.similarities.docsim import _nlargest\nfrom gensim.similarities.fastss import editdist\nfrom gensim.similarities.annoy import AnnoyIndexer\n@unittest.skip('skipping abstract base class')\nclass _TestSimilarityABC(unittest.TestCase):\n    \"\"\"\n    Base class for SparseMatrixSimilarity and MatrixSimilarity unit tests.\n    \"\"\"\n\n    def factoryMethod(self):\n        \"\"\"Creates a SimilarityABC instance.\"\"\"\n        return self.cls(CORPUS, num_features=len(DICTIONARY))\n\n    def test_full(self, num_best=None, shardsize=100):\n        if self.cls == similarities.Similarity:\n            index = self.cls(None, CORPUS, num_features=len(DICTIONARY),\n                shardsize=shardsize)\n        else:\n            index = self.cls(CORPUS, num_features=len(DICTIONARY))\n        if isinstance(index, similarities.MatrixSimilarity):\n            expected = numpy.array([[0.57735026, 0.57735026, 0.57735026, \n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, \n                0.40824831, 0.0, 0.40824831, 0.40824831, 0.40824831, \n                0.40824831, 0.40824831, 0.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.0,\n                0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0], [0.0, 0.0, \n                0.40824831, 0.0, 0.0, 0.0, 0.81649661, 0.0, 0.40824831, 0.0,\n                0.0, 0.0], [0.0, 0.0, 0.0, 0.57735026, 0.57735026, 0.0, 0.0,\n                0.57735026, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, \n                0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, \n                0.0, 0.0, 0.0, 0.0, 0.0, 0.70710677, 0.70710677, 0.0], [0.0,\n                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57735026, \n                0.57735026, 0.57735026], [0.0, 0.0, 0.0, 0.0, 0.0, \n                0.57735026, 0.0, 0.0, 0.0, 0.0, 0.57735026, 0.57735026]],\n                dtype=numpy.float32)\n            self.assertTrue(numpy.allclose(sorted(expected.flat), sorted(\n                index.index.flat)))\n        index.num_best = num_best\n        query = CORPUS[0]\n        sims = index[query]\n        expected = [(0, 0.99999994), (2, 0.28867513), (3, 0.23570226), (1, \n            0.23570226)][:num_best]\n        expected = matutils.sparse2full(expected, len(index))\n        if num_best is not None:\n            sims = matutils.sparse2full(sims, len(index))\n        self.assertTrue(numpy.allclose(expected, sims))\n        if self.cls == similarities.Similarity:\n            index.destroy()\n\n    def test_num_best(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        for num_best in [None, 0, 1, 9, 1000]:\n            self.testFull(num_best=num_best)\n\n    def test_full2sparse_clipped(self):\n        vec = [0.8, 0.2, 0.0, 0.0, -0.1, -0.15]\n        expected = [(0, 0.8), (1, 0.2), (5, -0.15)]\n        self.assertTrue(matutils.full2sparse_clipped(vec, topn=3), expected)\n\n    def test_scipy2scipy_clipped(self):\n        vec = [0.8, 0.2, 0.0, 0.0, -0.1, -0.15]\n        expected = [(0, 0.8), (1, 0.2), (5, -0.15)]\n        vec_scipy = scipy.sparse.csr_matrix(vec)\n        vec_scipy_clipped = matutils.scipy2scipy_clipped(vec_scipy, topn=3)\n        self.assertTrue(scipy.sparse.issparse(vec_scipy_clipped))\n        self.assertTrue(matutils.scipy2sparse(vec_scipy_clipped), expected)\n        vec = [0.8, 0.2, 0.0, 0.0, -0.1, -0.15]\n        expected = [(0, 0.8), (1, 0.2), (5, -0.15)]\n        matrix_scipy = scipy.sparse.csr_matrix([vec] * 3)\n        matrix_scipy_clipped = matutils.scipy2scipy_clipped(matrix_scipy,\n            topn=3)\n        self.assertTrue(scipy.sparse.issparse(matrix_scipy_clipped))\n        self.assertTrue([matutils.scipy2sparse(x) for x in\n            matrix_scipy_clipped], [expected] * 3)\n\n    def test_empty_query(self):\n        index = self.factoryMethod()\n        if isinstance(index, similarities.WmdSimilarity) and not POT_EXT:\n            self.skipTest('POT not installed')\n        query = []\n        try:\n            sims = index[query]\n            self.assertTrue(sims is not None)\n        except IndexError:\n            self.assertTrue(False)\n\n    def test_chunking(self):\n        if self.cls == similarities.Similarity:\n            index = self.cls(None, CORPUS, num_features=len(DICTIONARY),\n                shardsize=5)\n        else:\n            index = self.cls(CORPUS, num_features=len(DICTIONARY))\n        query = CORPUS[:3]\n        sims = index[query]\n        expected = numpy.array([[0.99999994, 0.23570226, 0.28867513, \n            0.23570226, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23570226, 1.0, \n            0.40824831, 0.33333334, 0.70710677, 0.0, 0.0, 0.0, 0.23570226],\n            [0.28867513, 0.40824831, 1.0, 0.61237246, 0.28867513, 0.0, 0.0,\n            0.0, 0.0]], dtype=numpy.float32)\n        self.assertTrue(numpy.allclose(expected, sims))\n        index.num_best = 3\n        sims = index[query]\n        expected = [[(0, 0.99999994), (2, 0.28867513), (1, 0.23570226)], [(\n            1, 1.0), (4, 0.70710677), (2, 0.40824831)], [(2, 1.0), (3, \n            0.61237246), (1, 0.40824831)]]\n        self.assertTrue(numpy.allclose(expected, sims))\n        if self.cls == similarities.Similarity:\n            index.destroy()\n\n    def test_iter(self):\n        if self.cls == similarities.Similarity:\n            index = self.cls(None, CORPUS, num_features=len(DICTIONARY),\n                shardsize=5)\n        else:\n            index = self.cls(CORPUS, num_features=len(DICTIONARY))\n        sims = [sim for sim in index]\n        expected = numpy.array([[0.99999994, 0.23570226, 0.28867513, \n            0.23570226, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23570226, 1.0, \n            0.40824831, 0.33333334, 0.70710677, 0.0, 0.0, 0.0, 0.23570226],\n            [0.28867513, 0.40824831, 1.0, 0.61237246, 0.28867513, 0.0, 0.0,\n            0.0, 0.0], [0.23570226, 0.33333334, 0.61237246, 1.0, 0.0, 0.0, \n            0.0, 0.0, 0.0], [0.0, 0.70710677, 0.28867513, 0.0, 0.99999994, \n            0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.70710677,\n            0.57735026, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.70710677, \n            0.99999994, 0.81649655, 0.40824828], [0.0, 0.0, 0.0, 0.0, 0.0, \n            0.57735026, 0.81649655, 0.99999994, 0.66666663], [0.0, \n            0.23570226, 0.0, 0.0, 0.0, 0.0, 0.40824828, 0.66666663, \n            0.99999994]], dtype=numpy.float32)\n        self.assertTrue(numpy.allclose(expected, sims))\n        if self.cls == similarities.Similarity:\n            index.destroy()\n\n    def test_persistency(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl')\n        index = self.factoryMethod()\n        index.save(fname)\n        index2 = self.cls.load(fname)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_persistency_compressed(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl.gz')\n        index = self.factoryMethod()\n        index.save(fname)\n        index2 = self.cls.load(fname)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_large(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        index2 = self.cls.load(fname)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_large_compressed(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl.gz')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        index2 = self.cls.load(fname, mmap=None)\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_mmap(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        index2 = self.cls.load(fname, mmap='r')\n        if self.cls == similarities.Similarity:\n            self.assertTrue(len(index.shards) == len(index2.shards))\n            index.destroy()\n        else:\n            if isinstance(index, similarities.SparseMatrixSimilarity):\n                index.index = index.index.todense()\n                index2.index = index2.index.todense()\n            self.assertTrue(numpy.allclose(index.index, index2.index))\n            self.assertEqual(index.num_best, index2.num_best)\n\n    def test_mmap_compressed(self):\n        if self.cls == similarities.WmdSimilarity and not POT_EXT:\n            self.skipTest('POT not installed')\n        fname = get_tmpfile('gensim_similarities.tst.pkl.gz')\n        index = self.factoryMethod()\n        index.save(fname, sep_limit=0)\n        self.assertRaises(IOError, self.cls.load, fname, mmap='r')\n\nclass TestSimilarity(_TestSimilarityABC):\n\tdef test_nlargest(self):\n\t    sims = [(0, 0.8), (1, 0.2), (2, 0.0), (3, 0.0), (4, -0.1), (5, -0.15)],\n\t    expected = [(0, 0.8), (1, 0.2), (5, -0.15)]\n\t    self.assertTrue(_nlargest(3, sims), expected)\n\t\nTestSimilarity().test_nlargest()\n", "code_start": "", "test_path": "../srcdata/Computation/gensim/gensim/test/test_similarities.py"}], "instruction": "Functionality: The _nlargest function is designed to extract the top n documents with the maximum similarity from a given iterable. This function is particularly useful in information retrieval and natural language processing tasks where selecting the most relevant documents is crucial.\n\nInputs:\n- n : int\n    Number of elements to be extracted from the iterable. This parameter defines how many of the most similar documents you want to retrieve.\n- iterable : iterable of list of (int, float)\n    An iterable containing documents (each as a list) with computed similarities. Each list in the iterable should contain tuples where the first element is an integer representing the document index, and the second element is a float representing the similarity score.\n\nOutputs:\n- :class:`list`\n    A list with the n largest elements (tuples of document index and similarity score) from the dataset defined by the iterable. The elements are ordered by decreasing similarity score. Note that the function compares elements by the absolute value of similarity to account for potential negative similarity scores.\n", "method_code_mask": "import logging\nimport itertools\nimport os\nimport heapq\nimport warnings\nimport numpy\nimport scipy.sparse\nfrom gensim import interfaces\nfrom gensim import utils\nfrom gensim import matutils\nimport multiprocessing\nimport glob\n\n\ndef _nlargest(n, iterable): [MASK]\n"}

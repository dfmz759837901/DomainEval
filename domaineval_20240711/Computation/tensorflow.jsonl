{"method_name": "make_decorator", "full_method_name": "make_decorator", "method_path": "../srcdata/AI_and_SE/tensorflow/tensorflow/python/util/tf_decorator.py", "method_code": "import inspect\nfrom typing import Dict\nfrom typing import Any\ndef make_decorator(target, decorator_func, decorator_name=None,\n    decorator_doc='', decorator_argspec=None):\n    \"\"\"Make a decorator from a wrapper and a target.\n\n  Args:\n    target: The final callable to be wrapped.\n    decorator_func: The wrapper function.\n    decorator_name: The name of the decorator. If `None`, the name of the\n      function calling make_decorator.\n    decorator_doc: Documentation specific to this application of\n      `decorator_func` to `target`.\n    decorator_argspec: Override the signature using FullArgSpec.\n\n  Returns:\n    The `decorator_func` argument with new metadata attached.\n  \"\"\"\n    if decorator_name is None:\n        decorator_name = inspect.currentframe().f_back.f_code.co_name\n    decorator = TFDecorator(decorator_name, target, decorator_doc,\n        decorator_argspec)\n    setattr(decorator_func, '_tf_decorator', decorator)\n    if hasattr(target, '__name__'):\n        decorator_func.__name__ = target.__name__\n    if hasattr(target, '__qualname__'):\n        decorator_func.__qualname__ = target.__qualname__\n    if hasattr(target, '__module__'):\n        decorator_func.__module__ = target.__module__\n    if hasattr(target, '__dict__'):\n        for name in target.__dict__:\n            if name not in decorator_func.__dict__:\n                decorator_func.__dict__[name] = target.__dict__[name]\n    if hasattr(target, '__doc__'):\n        decorator_func.__doc__ = decorator.__doc__\n    decorator_func.__wrapped__ = target\n    decorator_func.__original_wrapped__ = target\n    if decorator_argspec:\n        decorator_func.__signature__ = fullargspec_to_signature(\n            decorator_argspec)\n    elif callable(target):\n        try:\n            signature = inspect.signature(target)\n        except (TypeError, ValueError):\n            pass\n        else:\n            bound_instance = _get_bound_instance(target)\n            if bound_instance and 'self' in signature.parameters:\n                signature = inspect.Signature(list(signature.parameters.\n                    values())[1:])\n                decorator_func.__self__ = bound_instance\n            decorator_func.__signature__ = signature\n    return decorator_func", "test_code_list": [{"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef assert_no_new_pyobjects_executing_eagerly() ->Callable[\n    [Callable[..., Any]], Callable[..., None]]:\n    \"\"\"Decorator for asserting that no new Python objects persist after a test.\n\n  Returns a decorator that runs the test multiple times executing eagerly,\n  first as a warmup and then to let objects accumulate. The warmup helps ignore\n  caches which do not grow as the test is run repeatedly.\n\n  Useful for checking that there are no missing Py_DECREFs in the C exercised by\n  a bit of Python.\n\n  Args:\n    warmup_iters: The numer of warmup iterations, excluded from measuring.\n\n  Returns:\n    A decorator function which can be applied to the test function.\n  \"\"\"\n\n    def wrap_f(f: Callable[..., Any]) ->Callable[..., None]:\n\n        def decorator(self: 'TensorFlowTestCase', *args, **kwargs) ->None:\n            \"\"\"Warms up, gets object counts, runs the test, checks for new objects.\"\"\"\n            with context.eager_mode():\n                gc.disable()\n                test_errors = None\n                test_skipped = None\n                if hasattr(self._outcome, 'errors'):\n                    test_errors = self._outcome.errors\n                    test_skipped = self._outcome.skipped\n                else:\n                    test_errors = self._outcome.result.errors\n                    test_skipped = self._outcome.result.skipped\n                for _ in range(warmup_iters):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                obj_count_by_type = _get_object_count_by_type()\n                gc.collect()\n                registered_function_names = context.context(\n                    ).list_function_names()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped))\n                if ops.has_default_graph():\n                    collection_sizes_before = {collection: len(ops.\n                        get_collection(collection)) for collection in ops.\n                        get_default_graph().collections}\n                for _ in range(3):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                if ops.has_default_graph():\n                    for collection_key in ops.get_default_graph().collections:\n                        collection = ops.get_collection(collection_key)\n                        size_before = collection_sizes_before.get(\n                            collection_key, 0)\n                        if len(collection) > size_before:\n                            raise AssertionError(\n                                'Collection %s increased in size from %d to %d (current items %s).'\n                                 % (collection_key, size_before, len(\n                                collection), collection))\n                        del collection\n                        del collection_key\n                        del size_before\n                    del collection_sizes_before\n                gc.collect()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped)\n                    ) - obj_count_by_type\n                leftover_functions = context.context().list_function_names(\n                    ) - registered_function_names\n                assert not leftover_functions, 'The following functions were newly created: %s' % leftover_functions\n                assert not obj_count_by_type, 'The following objects were newly created: %s' % str(\n                    obj_count_by_type)\n                gc.enable()\n        return make_decorator(f, decorator)\n    return wrap_f\n\nassert_no_new_pyobjects_executing_eagerly()\n", "code_start": "", "test_path": "../srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}, {"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef run_in_v1_v2(\n    bool=False) ->Callable[[Callable[..., Any]], Callable[..., None]]:\n    \"\"\"Execute the decorated test in v1 and v2 modes.\n\n  The overall execution is similar to that of `run_in_graph_and_eager_mode`.\n\n  Args:\n    device_to_use: A string in the following format: \"/device:CPU:0\".\n    assert_no_eager_garbage: If True, sets DEBUG_SAVEALL on the garbage\n      collector and asserts that no extra garbage has been created when running\n      the test with eager execution enabled. This will fail if there are\n      reference cycles (e.g. a = []; a.append(a)). Off by default because some\n      tests may create garbage for legitimate reasons (e.g. they define a class\n      which inherits from `object`), and because DEBUG_SAVEALL is sticky in some\n      Python interpreters (meaning that tests which rely on objects being\n      collected elsewhere in the unit test file will not work). Additionally,\n      checks that nothing still has a reference to Tensors that the test\n      allocated.\n\n  Returns:\n    A decorator that runs a given test in v1 and v2 modes.\n  \"\"\"\n\n    def decorator(f: Callable[..., Any]) ->Callable[..., None]:\n        decorator_tag = 'wrapped_with_v1_v2_decorator'\n        if hasattr(f, decorator_tag):\n            return f\n\n        def decorated(self: 'TensorFlowTestCase', *args, **kwargs) ->None:\n            logging.info('Running %s in V1 mode.', f.__name__)\n            try:\n                with self.subTest('V1_mode'):\n                    v2_compat.disable_v2_behavior()\n                    f(self, *args, **kwargs)\n            except unittest.case.SkipTest:\n                pass\n\n            def run_v2(self: 'TensorFlowTestCase', **kwargs) ->None:\n                logging.info('Running %s in V2 mode.', f.__name__)\n                if device_to_use:\n                    with ops.device(device_to_use):\n                        f(self, *args, **kwargs)\n                else:\n                    f(self, *args, **kwargs)\n            if assert_no_eager_garbage:\n                ops.reset_default_graph()\n                run_v2 = assert_no_new_tensors(assert_no_garbage_created(\n                    run_v2))\n            self.tearDown()\n            self._tempdir = None\n            ops.reset_default_graph()\n            v2_compat.enable_v2_behavior()\n            with self.subTest('V2_mode'):\n                self.setUp()\n                run_v2(self, **kwargs)\n        tf_decorated = make_decorator(f, decorated)\n        tf_decorated.__dict__[decorator_tag] = True\n        return tf_decorated\n    return decorator\n\nrun_in_v1_v2()\n", "code_start": "", "test_path": "../srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}, {"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef run_without_tensor_float_32() ->Callable[[Callable[...,\n    Any]], Callable[..., None]]:\n    \"\"\"Execute test with TensorFloat-32 disabled.\n\n  While almost every real-world deep learning model runs fine with\n  TensorFloat-32, many tests use assertAllClose or similar methods.\n  TensorFloat-32 matmuls typically will cause such methods to fail with the\n  default tolerances.\n\n  Args:\n    description: A description used for documentation purposes, describing why\n      the test requires TensorFloat-32 to be disabled.\n\n  Returns:\n    Decorator which runs a test with TensorFloat-32 disabled.\n  \"\"\"\n\n    def decorator(f: Callable[..., Any]) ->Callable[..., None]:\n\n        @functools.wraps(f)\n        def decorated(*args, **kwargs):\n            allowed = config.tensor_float_32_execution_enabled()\n            try:\n                config.enable_tensor_float_32_execution(False)\n                f(*args, **kwargs)\n            finally:\n                config.enable_tensor_float_32_execution(allowed)\n        return make_decorator(f, decorated)\n    return decorator\n\nrun_without_tensor_float_32()\n", "code_start": "", "test_path": "../srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}], "instruction": "Functionality: Create a decorator from a wrapper function and a target callable. The make_decorator function takes a target callable that is to be wrapped, a decorator function that wraps the target, and optional arguments like the decorator's name, documentation, and arguments specification. It returns the decorator function with metadata attached that reflects the target's properties such as name, docstring, and signature. The purpose is to allow decorator functions to retain or override the metadata of the target functions they decorate.\n\nInputs:\n- target: The callable to be wrapped by the decorator.\n- decorator_func: The wrapper function that serves as the decorator.\n- decorator_name (optional): The name of the decorator. If not provided, it is derived from the function calling make_decorator.\n- decorator_doc (optional): Documentation specific to the application of the decorator to the target.\n- decorator_argspec (optional): Override the signature using FullArgSpec.\n\nOutputs:\n- The decorated function `decorator_func` with the metadata of the `target` applied, including its name, docstring, signature, and properties as attributes of the decorator function.", "method_code_mask": "import inspect\nfrom typing import Dict\nfrom typing import Any\n\n\ndef make_decorator(target, decorator_func, decorator_name=None,\n    decorator_doc='', decorator_argspec=None): [MASK]\n"}
{"method_name": "_get_object_count_by_type", "full_method_name": "_get_object_count_by_type", "method_path": "../srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py", "method_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef _get_object_count_by_type(exclude: Iterable[Any]=()) ->collections.Counter[\n    str]:\n    return collections.Counter([type(obj).__name__ for obj in gc.get_objects()]\n        ) - collections.Counter([type(obj).__name__ for obj in exclude])", "test_code_list": [{"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef assert_no_new_pyobjects_executing_eagerly() ->Callable[\n    [Callable[..., Any]], Callable[..., None]]:\n    \"\"\"Decorator for asserting that no new Python objects persist after a test.\n\n  Returns a decorator that runs the test multiple times executing eagerly,\n  first as a warmup and then to let objects accumulate. The warmup helps ignore\n  caches which do not grow as the test is run repeatedly.\n\n  Useful for checking that there are no missing Py_DECREFs in the C exercised by\n  a bit of Python.\n\n  Args:\n    warmup_iters: The numer of warmup iterations, excluded from measuring.\n\n  Returns:\n    A decorator function which can be applied to the test function.\n  \"\"\"\n\n    def wrap_f(f: Callable[..., Any]) ->Callable[..., None]:\n\n        def decorator(self: 'TensorFlowTestCase', *args, **kwargs) ->None:\n            \"\"\"Warms up, gets object counts, runs the test, checks for new objects.\"\"\"\n            with context.eager_mode():\n                gc.disable()\n                test_errors = None\n                test_skipped = None\n                if hasattr(self._outcome, 'errors'):\n                    test_errors = self._outcome.errors\n                    test_skipped = self._outcome.skipped\n                else:\n                    test_errors = self._outcome.result.errors\n                    test_skipped = self._outcome.result.skipped\n                for _ in range(warmup_iters):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                obj_count_by_type = _get_object_count_by_type()\n                gc.collect()\n                registered_function_names = context.context(\n                    ).list_function_names()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped))\n                if ops.has_default_graph():\n                    collection_sizes_before = {collection: len(ops.\n                        get_collection(collection)) for collection in ops.\n                        get_default_graph().collections}\n                for _ in range(3):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                if ops.has_default_graph():\n                    for collection_key in ops.get_default_graph().collections:\n                        collection = ops.get_collection(collection_key)\n                        size_before = collection_sizes_before.get(\n                            collection_key, 0)\n                        if len(collection) > size_before:\n                            raise AssertionError(\n                                'Collection %s increased in size from %d to %d (current items %s).'\n                                 % (collection_key, size_before, len(\n                                collection), collection))\n                        del collection\n                        del collection_key\n                        del size_before\n                    del collection_sizes_before\n                gc.collect()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped)\n                    ) - obj_count_by_type\n                leftover_functions = context.context().list_function_names(\n                    ) - registered_function_names\n                assert not leftover_functions, 'The following functions were newly created: %s' % leftover_functions\n                assert not obj_count_by_type, 'The following objects were newly created: %s' % str(\n                    obj_count_by_type)\n                gc.enable()\n        return tf_decorator.make_decorator(f, decorator)\n    return wrap_f\n\nassert_no_new_pyobjects_executing_eagerly()\n", "code_start": "", "test_path": "../srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}], "instruction": "Functionality: The function _get_object_count_by_type is designed to count the number of objects of each type currently existing in the Python interpreter's memory. It accomplishes this by retrieving all objects in memory, determining their types, and then counting how many of each type exist. The counts are returned in a collections.Counter object which is a dictionary subclass for counting hashable objects.\n\nInputs: \n- exclude: This is an optional parameter of type Iterable[Any] that contains objects you want to exclude from the count. By default, it is an empty tuple. If you provide objects to this parameter, they will not be counted in the resulting Counter.\n\nOutputs:\n- collections.Counter[str]: The function returns a collections.Counter object. This object's keys are the names of the types as strings, and the values are the counts of how many objects of those types are in memory, excluding the ones specified in the 'exclude' parameter.", "method_code_mask": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\n\n\ndef _get_object_count_by_type(exclude: Iterable[Any]=()) ->collections.Counter[\n    str]: [MASK]\n"}

{"method_name": "detect", "full_method_name": "detect", "method_path": "../srcdata/Basic/charset_normalizer/charset_normalizer/legacy.py", "method_code": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import Optional\nfrom warnings import warn\nfrom typing_extensions import TypedDict\ndef detect(byte_str: bytes, should_rename_legacy: bool=False, **kwargs: Any\n    ) ->ResultDict:\n    \"\"\"\n    chardet legacy method\n    Detect the encoding of the given byte string. It should be mostly backward-compatible.\n    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)\n    This function is deprecated and should be used to migrate your project easily, consult the documentation for\n    further information. Not planned for removal.\n\n    :param byte_str:     The byte sequence to examine.\n    :param should_rename_legacy:  Should we rename legacy encodings\n                                  to their more modern equivalents?\n    \"\"\"\n    if len(kwargs):\n        warn(\n            f\"charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()\"\n            )\n    if not isinstance(byte_str, (bytearray, bytes)):\n        raise TypeError('Expected object of type bytes or bytearray, got: {0}'\n            .format(type(byte_str)))\n    if isinstance(byte_str, bytearray):\n        byte_str = bytes(byte_str)\n    r = from_bytes(byte_str).best()\n    encoding = r.encoding if r is not None else None\n    language = r.language if r is not None and r.language != 'Unknown' else ''\n    confidence = 1.0 - r.chaos if r is not None else None\n    if r is not None and encoding == 'utf_8' and r.bom:\n        encoding += '_sig'\n    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:\n        encoding = CHARDET_CORRESPONDENCE[encoding]\n    return {'encoding': encoding, 'language': language, 'confidence':\n        confidence}", "test_code_list": [{"test_code": "import unittest\nfrom charset_normalizer.legacy import detect\n\nclass TestDetectLegacy(unittest.TestCase):\n\tdef test_detect_dict_keys(self):\n\t    r = detect((u'\\ufeff' + '\u6211\u6ca1\u6709\u57cb\u6028\uff0c\u78cb\u7823\u7684\u53ea\u662f\u4e00\u4e9b\u65f6\u95f4\u3002').encode('gb18030'))\n\t    with self.subTest('encoding key present'):\n\t        self.assertIn('encoding', r.keys())\n\t    with self.subTest('language key present'):\n\t        self.assertIn('language', r.keys())\n\t    with self.subTest('confidence key present'):\n\t        self.assertIn('confidence', r.keys())\n\t\nTestDetectLegacy().test_detect_dict_keys()\n", "code_start": "", "test_path": "../srcdata/Basic/charset_normalizer/tests/test_detect_legacy.py"}, {"test_code": "import unittest\nfrom charset_normalizer.legacy import detect\n\nclass TestDetectLegacy(unittest.TestCase):\n\tdef test_detect_dict_value_type(self):\n\t    r = detect('\u6211\u6ca1\u6709\u57cb\u6028\uff0c\u78cb\u7823\u7684\u53ea\u662f\u4e00\u4e9b\u65f6\u95f4\u3002'.encode('utf_8'))\n\t    with self.subTest('encoding instance of str'):\n\t        self.assertIsInstance(r['encoding'], str)\n\t    with self.subTest('language instance of str'):\n\t        self.assertIsInstance(r['language'], str)\n\t    with self.subTest('confidence instance of float'):\n\t        self.assertIsInstance(r['confidence'], float)\n\t\nTestDetectLegacy().test_detect_dict_value_type()\n", "code_start": "", "test_path": "../srcdata/Basic/charset_normalizer/tests/test_detect_legacy.py"}, {"test_code": "import unittest\nfrom charset_normalizer.legacy import detect\n\nclass TestDetectLegacy(unittest.TestCase):\n\tdef test_detect_dict_value(self):\n\t    r = detect('\u6211\u6ca1\u6709\u57cb\u6028\uff0c\u78cb\u7823\u7684\u53ea\u662f\u4e00\u4e9b\u65f6\u95f4\u3002'.encode('utf_32'))\n\t    with self.subTest('encoding is equal to utf_32'):\n\t        self.assertEqual(r['encoding'], 'UTF-32')\n\t\nTestDetectLegacy().test_detect_dict_value()\n", "code_start": "", "test_path": "../srcdata/Basic/charset_normalizer/tests/test_detect_legacy.py"}, {"test_code": "import unittest\nfrom charset_normalizer.legacy import detect\n\nclass TestDetectLegacy(unittest.TestCase):\n\tdef test_utf8_sig_not_striped(self):\n\t    r = detect('Hello World'.encode('utf-8-sig'))\n\t    with self.subTest(\n\t        'Verify that UTF-8-SIG is returned when using legacy detect'):\n\t        self.assertEqual(r['encoding'], 'UTF-8-SIG')\n\t\nTestDetectLegacy().test_utf8_sig_not_striped()\n", "code_start": "", "test_path": "../srcdata/Basic/charset_normalizer/tests/test_detect_legacy.py"}], "instruction": "Functionality: The 'detect' function is designed to determine the encoding of a given byte string. It is a legacy method aiming for backward compatibility. The function will return the detected encoding, a language tag if applicable, and a confidence level indicating the reliability of the detection.\n\nInputs: \n- byte_str: A byte sequence (bytes or bytearray) to be examined.\n- should_rename_legacy: An optional boolean parameter that, when True, will rename legacy encoding names to their more modern equivalents. Default is False.\n\nOutputs:\n- A dictionary containing the following information:\n  - 'encoding': The detected encoding name. If detection is not possible, None is returned.\n  - 'language': A language tag associated with the encoding if it can be determined. Returns an empty string if the language is unknown.\n  - 'confidence': A floating-point number representing the confidence level of the detection, ranging from 0 to 1. If detection is not possible, None is returned.", "method_code_mask": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import Optional\nfrom warnings import warn\nfrom typing_extensions import TypedDict\n\n\ndef detect(byte_str: bytes, should_rename_legacy: bool=False, **kwargs: Any\n    ) ->ResultDict: [MASK]\n"}

{"model_name": "std", "repository": "nltk", "full_method_name": "_wikipedia_example_hmm", "method_path": "../srcdata/Computation/nltk/nltk/test/unit/test_hmm.py", "response": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\ndef _wikipedia_example_hmm():\n    states = ['rain', 'no rain']\n    symbols = ['umbrella', 'no umbrella']\n    A = [[0.7, 0.3], [0.3, 0.7]]\n    B = [[0.9, 0.1], [0.2, 0.8]]\n    pi = [0.5, 0.5]\n    seq = ['umbrella', 'umbrella', 'no umbrella', 'umbrella', 'umbrella']\n    seq = list(zip(seq, [None] * len(seq)))\n    model = hmm._create_hmm_tagger(states, symbols, A, B, pi)\n    return model, states, symbols, seq", "test_code_list": [{"test_code": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\ndef test_forward_probability2():\n    from numpy.testing import assert_array_almost_equal\n    model, states, symbols, seq = _wikipedia_example_hmm()\n    fp = 2 ** model._forward_probability(seq)\n    fp = (fp.T / fp.sum(axis=1)).T\n    wikipedia_results = [[0.8182, 0.1818], [0.8834, 0.1166], [0.1907, \n        0.8093], [0.7308, 0.2692], [0.8673, 0.1327]]\n    assert_array_almost_equal(wikipedia_results, fp, 4)\n\ntest_forward_probability2()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_hmm.py"}, {"test_code": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\ndef test_backward_probability():\n    from numpy.testing import assert_array_almost_equal\n    model, states, symbols, seq = _wikipedia_example_hmm()\n    bp = 2 ** model._backward_probability(seq)\n    bp = (bp.T / bp.sum(axis=1)).T\n    wikipedia_results = [[0.5923, 0.4077], [0.3763, 0.6237], [0.6533, \n        0.3467], [0.6273, 0.3727], [0.5, 0.5]]\n    assert_array_almost_equal(wikipedia_results, bp, 4)\n\ntest_backward_probability()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_hmm.py"}], "method_code_mask": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\n\n\ndef _wikipedia_example_hmm(): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "corpus_ribes", "method_path": "../srcdata/Computation/nltk/nltk/translate/ribes_score.py", "response": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\ndef corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.1):\n    \"\"\"\n    This function \"calculates RIBES for a system output (hypothesis) with\n    multiple references, and returns \"best\" score among multi-references and\n    individual scores. The scores are corpus-wise, i.e., averaged by the number\n    of sentences.\" (c.f. RIBES version 1.03.1 code).\n\n    Different from BLEU's micro-average precision, RIBES calculates the\n    macro-average precision by averaging the best RIBES score for each pair of\n    hypothesis and its corresponding references\n\n    >>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n    ...         'ensures', 'that', 'the', 'military', 'always',\n    ...         'obeys', 'the', 'commands', 'of', 'the', 'party']\n    >>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n    ...          'ensures', 'that', 'the', 'military', 'will', 'forever',\n    ...          'heed', 'Party', 'commands']\n    >>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n    ...          'guarantees', 'the', 'military', 'forces', 'always',\n    ...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']\n    >>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n    ...          'army', 'always', 'to', 'heed', 'the', 'directions',\n    ...          'of', 'the', 'party']\n\n    >>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n    ...         'interested', 'in', 'world', 'history']\n    >>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',\n    ...          'because', 'he', 'read', 'the', 'book']\n\n    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n    >>> hypotheses = [hyp1, hyp2]\n    >>> round(corpus_ribes(list_of_references, hypotheses),4)\n    0.3597\n\n    :param references: a corpus of lists of reference sentences, w.r.t. hypotheses\n    :type references: list(list(list(str)))\n    :param hypotheses: a list of hypothesis sentences\n    :type hypotheses: list(list(str))\n    :param alpha: hyperparameter used as a prior for the unigram precision.\n    :type alpha: float\n    :param beta: hyperparameter used as a prior for the brevity penalty.\n    :type beta: float\n    :return: The best ribes score from one of the references.\n    :rtype: float\n    \"\"\"\n    corpus_best_ribes = 0.0\n    for references, hypothesis in zip(list_of_references, hypotheses):\n        corpus_best_ribes += sentence_ribes(references, hypothesis, alpha, beta\n            )\n    return corpus_best_ribes / len(hypotheses)", "test_code_list": [{"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_empty_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's neat and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == []\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_empty_worder()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_one_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == [3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_one_worder()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_two_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference is different\".split(\n        )\n    assert word_rank_alignment(ref, hyp) == [9, 3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_two_worder()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes():\n    hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures',\n        'that', 'the', 'military', 'always', 'obeys', 'the', 'commands',\n        'of', 'the', 'party']\n    ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures',\n        'that', 'the', 'military', 'will', 'forever', 'heed', 'Party',\n        'commands']\n    ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n        'guarantees', 'the', 'military', 'forces', 'always', 'being',\n        'under', 'the', 'command', 'of', 'the', 'Party']\n    ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', 'army',\n        'always', 'to', 'heed', 'the', 'directions', 'of', 'the', 'party']\n    hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n        'interested', 'in', 'world', 'history']\n    ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because',\n        'he', 'read', 'the', 'book']\n    list_of_refs = [[ref1a, ref1b, ref1c], [ref2a]]\n    hypotheses = [hyp1, hyp2]\n    score = corpus_ribes(list_of_refs, hypotheses)\n    assert round(score, 4) == 0.3597\n\ntest_ribes()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_no_zero_div():\n    hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures',\n        'that', 'the', 'military', 'always', 'obeys', 'the', 'commands',\n        'of', 'the', 'party']\n    ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures',\n        'that', 'the', 'military', 'will', 'forever', 'heed', 'Party',\n        'commands']\n    ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n        'guarantees', 'the', 'military', 'forces', 'always', 'being',\n        'under', 'the', 'command', 'of', 'the', 'Party']\n    ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', 'army',\n        'always', 'to', 'heed', 'the', 'directions', 'of', 'the', 'party']\n    hyp2 = ['he', 'read', 'the']\n    ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because',\n        'he']\n    list_of_refs = [[ref1a, ref1b, ref1c], [ref2a]]\n    hypotheses = [hyp1, hyp2]\n    score = corpus_ribes(list_of_refs, hypotheses)\n    assert round(score, 4) == 0.1688\n\ntest_no_zero_div()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}], "method_code_mask": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\n\ndef corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.1): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "word_rank_alignment", "method_path": "../srcdata/Computation/nltk/nltk/translate/ribes_score.py", "response": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\ndef word_rank_alignment(reference, hypothesis, character_based=False):\n    \"\"\"\n    This is the word rank alignment algorithm described in the paper to produce\n    the *worder* list, i.e. a list of word indices of the hypothesis word orders\n    w.r.t. the list of reference words.\n\n    Below is (H0, R0) example from the Isozaki et al. 2010 paper,\n    note the examples are indexed from 1 but the results here are indexed from 0:\n\n        >>> ref = str('he was interested in world history because he '\n        ... 'read the book').split()\n        >>> hyp = str('he read the book because he was interested in world '\n        ... 'history').split()\n        >>> word_rank_alignment(ref, hyp)\n        [7, 8, 9, 10, 6, 0, 1, 2, 3, 4, 5]\n\n    The (H1, R1) example from the paper, note the 0th index:\n\n        >>> ref = 'John hit Bob yesterday'.split()\n        >>> hyp = 'Bob hit John yesterday'.split()\n        >>> word_rank_alignment(ref, hyp)\n        [2, 1, 0, 3]\n\n    Here is the (H2, R2) example from the paper, note the 0th index here too:\n\n        >>> ref = 'the boy read the book'.split()\n        >>> hyp = 'the book was read by the boy'.split()\n        >>> word_rank_alignment(ref, hyp)\n        [3, 4, 2, 0, 1]\n\n    :param reference: a reference sentence\n    :type reference: list(str)\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: list(str)\n    \"\"\"\n    worder = []\n    hyp_len = len(hypothesis)\n    ref_ngrams = []\n    hyp_ngrams = []\n    for n in range(1, len(reference) + 1):\n        for ng in ngrams(reference, n):\n            ref_ngrams.append(ng)\n        for ng in ngrams(hypothesis, n):\n            hyp_ngrams.append(ng)\n    for i, h_word in enumerate(hypothesis):\n        if h_word not in reference:\n            continue\n        elif hypothesis.count(h_word) == reference.count(h_word) == 1:\n            worder.append(reference.index(h_word))\n        else:\n            max_window_size = max(i, hyp_len - i + 1)\n            for window in range(1, max_window_size):\n                if i + window < hyp_len:\n                    right_context_ngram = tuple(islice(hypothesis, i, i +\n                        window + 1))\n                    num_times_in_ref = ref_ngrams.count(right_context_ngram)\n                    num_times_in_hyp = hyp_ngrams.count(right_context_ngram)\n                    if num_times_in_ref == num_times_in_hyp == 1:\n                        pos = position_of_ngram(right_context_ngram, reference)\n                        worder.append(pos)\n                        break\n                if window <= i:\n                    left_context_ngram = tuple(islice(hypothesis, i -\n                        window, i + 1))\n                    num_times_in_ref = ref_ngrams.count(left_context_ngram)\n                    num_times_in_hyp = hyp_ngrams.count(left_context_ngram)\n                    if num_times_in_ref == num_times_in_hyp == 1:\n                        pos = position_of_ngram(left_context_ngram, reference)\n                        worder.append(pos + len(left_context_ngram) - 1)\n                        break\n    return worder", "test_code_list": [{"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_empty_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's neat and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == []\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_empty_worder()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_one_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == [3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_one_worder()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_two_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference is different\".split(\n        )\n    assert word_rank_alignment(ref, hyp) == [9, 3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_two_worder()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}], "method_code_mask": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\n\ndef word_rank_alignment(reference, hypothesis, character_based=False): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "_pair_iter", "method_path": "../srcdata/Computation/nltk/nltk/tokenize/punkt.py", "response": "import math\nimport re\nimport string\nfrom collections import defaultdict\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Match\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize.api import TokenizerI\ndef _pair_iter(iterator):\n    \"\"\"\n    Yields pairs of tokens from the given iterator such that each input\n    token will appear as the first element in a yielded tuple. The last\n    pair will have None as its second element.\n    \"\"\"\n    iterator = iter(iterator)\n    try:\n        prev = next(iterator)\n    except StopIteration:\n        return\n    for el in iterator:\n        yield prev, el\n        prev = el\n    yield prev, None", "test_code_list": [{"test_code": "from typing import List\nfrom typing import Tuple\nimport pytest\nfrom nltk.tokenize import LegalitySyllableTokenizer\nfrom nltk.tokenize import StanfordSegmenter\nfrom nltk.tokenize import SyllableTokenizer\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import punkt\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.simple import CharTokenizer\nfrom nltk.corpus import words\n\nclass TestTokenize():\n\tdef test_punkt_pair_iter(self):\n\t    test_cases = [('12', [('1', '2'), ('2', None)]), ('123', [('1', '2'), (\n\t        '2', '3'), ('3', None)]), ('1234', [('1', '2'), ('2', '3'), ('3',\n\t        '4'), ('4', None)])]\n\t    for test_input, expected_output in test_cases:\n\t        actual_output = [x for x in _pair_iter(test_input)]\n\t        assert actual_output == expected_output\n\t\nTestTokenize().test_punkt_pair_iter()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/test_tokenize.py"}], "method_code_mask": "import math\nimport re\nimport string\nfrom collections import defaultdict\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Match\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize.api import TokenizerI\n\n\ndef _pair_iter(iterator): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "padded_everygram_pipeline", "method_path": "../srcdata/Computation/nltk/nltk/lm/preprocessing.py", "response": "from functools import partial\nfrom itertools import chain\nfrom nltk.util import everygrams\nfrom nltk.util import pad_sequence\ndef padded_everygram_pipeline(order, text):\n    \"\"\"Default preprocessing for a sequence of sentences.\n\n    Creates two iterators:\n\n    - sentences padded and turned into sequences of `nltk.util.everygrams`\n    - sentences padded as above and chained together for a flat stream of words\n\n    :param order: Largest ngram length produced by `everygrams`.\n    :param text: Text to iterate over. Expected to be an iterable of sentences.\n    :type text: Iterable[Iterable[str]]\n    :return: iterator over text as ngrams, iterator over text as vocabulary data\n    \"\"\"\n    padding_fn = partial(pad_both_ends, n=order)\n    return (everygrams(list(padding_fn(sent)), max_len=order) for sent in text\n        ), flatten(map(padding_fn, text))", "test_code_list": [{"test_code": "import unittest\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_padded_everygram_pipeline(self):\n\t    expected_train = [[('<s>',), ('<s>', 'a'), ('a',), ('a', 'b'), ('b',),\n\t        ('b', 'c'), ('c',), ('c', '</s>'), ('</s>',)]]\n\t    expected_vocab = ['<s>', 'a', 'b', 'c', '</s>']\n\t    train_data, vocab_data = padded_everygram_pipeline(2, [['a', 'b', 'c']])\n\t    self.assertEqual([list(sent) for sent in train_data], expected_train)\n\t    self.assertEqual(list(vocab_data), expected_vocab)\n\t\nTestPreprocessing().test_padded_everygram_pipeline()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/lm/test_preprocessing.py"}], "method_code_mask": "from functools import partial\nfrom itertools import chain\nfrom nltk.util import everygrams\nfrom nltk.util import pad_sequence\n\n\ndef padded_everygram_pipeline(order, text): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "brevity_penalty", "method_path": "../srcdata/Computation/nltk/nltk/translate/bleu_score.py", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\ndef brevity_penalty(closest_ref_len, hyp_len):\n    \"\"\"\n    Calculate brevity penalty.\n\n    As the modified n-gram precision still has the problem from the short\n    length sentence, brevity penalty is used to modify the overall BLEU\n    score according to length.\n\n    An example from the paper. There are three references with length 12, 15\n    and 17. And a concise hypothesis of the length 12. The brevity penalty is 1.\n\n    >>> reference1 = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12\n    >>> reference2 = list('aaaaaaaaaaaaaaa')   # i.e. ['a'] * 15\n    >>> reference3 = list('aaaaaaaaaaaaaaaaa') # i.e. ['a'] * 17\n    >>> hypothesis = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12\n    >>> references = [reference1, reference2, reference3]\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len)\n    1.0\n\n    In case a hypothesis translation is shorter than the references, penalty is\n    applied.\n\n    >>> references = [['a'] * 28, ['a'] * 28]\n    >>> hypothesis = ['a'] * 12\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len)\n    0.2635971381157267\n\n    The length of the closest reference is used to compute the penalty. If the\n    length of a hypothesis is 12, and the reference lengths are 13 and 2, the\n    penalty is applied because the hypothesis length (12) is less then the\n    closest reference length (13).\n\n    >>> references = [['a'] * 13, ['a'] * 2]\n    >>> hypothesis = ['a'] * 12\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS\n    0.9200...\n\n    The brevity penalty doesn't depend on reference order. More importantly,\n    when two reference sentences are at the same distance, the shortest\n    reference sentence length is used.\n\n    >>> references = [['a'] * 13, ['a'] * 11]\n    >>> hypothesis = ['a'] * 12\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> bp1 = brevity_penalty(closest_ref_len, hyp_len)\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(reversed(references), hyp_len)\n    >>> bp2 = brevity_penalty(closest_ref_len, hyp_len)\n    >>> bp1 == bp2 == 1\n    True\n\n    A test example from mteval-v13a.pl (starting from the line 705):\n\n    >>> references = [['a'] * 11, ['a'] * 8]\n    >>> hypothesis = ['a'] * 7\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS\n    0.8668...\n\n    >>> references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n    >>> hypothesis = ['a'] * 7\n    >>> hyp_len = len(hypothesis)\n    >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n    >>> brevity_penalty(closest_ref_len, hyp_len)\n    1.0\n\n    :param hyp_len: The length of the hypothesis for a single sentence OR the\n        sum of all the hypotheses' lengths for a corpus\n    :type hyp_len: int\n    :param closest_ref_len: The length of the closest reference for a single\n        hypothesis OR the sum of all the closest references for every hypotheses.\n    :type closest_ref_len: int\n    :return: BLEU's brevity penalty.\n    :rtype: float\n    \"\"\"\n    if hyp_len > closest_ref_len:\n        return 1\n    elif hyp_len == 0:\n        return 0\n    else:\n        return math.exp(1 - closest_ref_len / hyp_len)", "test_code_list": [{"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_brevity_penalty(self):\n\t    references = [['a'] * 11, ['a'] * 8]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    self.assertAlmostEqual(brevity_penalty(closest_ref_len, hyp_len), \n\t        0.8669, places=4)\n\t    references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    assert brevity_penalty(closest_ref_len, hyp_len) == 1.0\n\t\nTestBLEU().test_brevity_penalty()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}], "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef brevity_penalty(closest_ref_len, hyp_len): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "closest_ref_length", "method_path": "../srcdata/Computation/nltk/nltk/translate/bleu_score.py", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\ndef closest_ref_length(references, hyp_len):\n    \"\"\"\n    This function finds the reference that is the closest length to the\n    hypothesis. The closest reference length is referred to as *r* variable\n    from the brevity penalty formula in Papineni et. al. (2002)\n\n    :param references: A list of reference translations.\n    :type references: list(list(str))\n    :param hyp_len: The length of the hypothesis.\n    :type hyp_len: int\n    :return: The length of the reference that's closest to the hypothesis.\n    :rtype: int\n    \"\"\"\n    ref_lens = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len -\n        hyp_len), ref_len))\n    return closest_ref_len", "test_code_list": [{"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_brevity_penalty(self):\n\t    references = [['a'] * 11, ['a'] * 8]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    self.assertAlmostEqual(brevity_penalty(closest_ref_len, hyp_len), \n\t        0.8669, places=4)\n\t    references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    assert brevity_penalty(closest_ref_len, hyp_len) == 1.0\n\t\nTestBLEU().test_brevity_penalty()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}], "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef closest_ref_length(references, hyp_len): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "sentence_bleu", "method_path": "../srcdata/Computation/nltk/nltk/translate/bleu_score.py", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None, auto_reweigh=False):\n    \"\"\"\n    Calculate BLEU score (Bilingual Evaluation Understudy) from\n    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\n    \"BLEU: a method for automatic evaluation of machine translation.\"\n    In Proceedings of ACL. https://www.aclweb.org/anthology/P02-1040.pdf\n\n    >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n    ...               'ensures', 'that', 'the', 'military', 'always',\n    ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\n\n    >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\n    ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\n    ...               'that', 'party', 'direct']\n\n    >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n    ...               'ensures', 'that', 'the', 'military', 'will', 'forever',\n    ...               'heed', 'Party', 'commands']\n\n    >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n    ...               'guarantees', 'the', 'military', 'forces', 'always',\n    ...               'being', 'under', 'the', 'command', 'of', 'the',\n    ...               'Party']\n\n    >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n    ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n    ...               'of', 'the', 'party']\n\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS\n    0.5045...\n\n    If there is no ngrams overlap for any order of n-grams, BLEU returns the\n    value 0. This is because the precision for the order of n-grams without\n    overlap is 0, and the geometric mean in the final BLEU score computation\n    multiplies the 0 with the precision of other n-grams. This results in 0\n    (independently of the precision of the other n-gram orders). The following\n    example has zero 3-gram and 4-gram overlaps:\n\n    >>> round(sentence_bleu([reference1, reference2, reference3], hypothesis2),4) # doctest: +ELLIPSIS\n    0.0\n\n    To avoid this harsh behaviour when no ngram overlaps are found a smoothing\n    function can be used.\n\n    >>> chencherry = SmoothingFunction()\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis2,\n    ...     smoothing_function=chencherry.method1) # doctest: +ELLIPSIS\n    0.0370...\n\n    The default BLEU calculates a score for up to 4-grams using uniform\n    weights (this is called BLEU-4). To evaluate your translations with\n    higher/lower order ngrams, use customized weights. E.g. when accounting\n    for up to 5-grams with uniform weights (this is called BLEU-5) use:\n\n    >>> weights = (1./5., 1./5., 1./5., 1./5., 1./5.)\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\n    0.3920...\n\n    Multiple BLEU scores can be computed at once, by supplying a list of weights.\n    E.g. for computing BLEU-2, BLEU-3 *and* BLEU-4 in one computation, use:\n    >>> weights = [\n    ...     (1./2., 1./2.),\n    ...     (1./3., 1./3., 1./3.),\n    ...     (1./4., 1./4., 1./4., 1./4.)\n    ... ]\n    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\n    [0.7453..., 0.6240..., 0.5045...]\n\n    :param references: reference sentences\n    :type references: list(list(str))\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: list(str)\n    :param weights: weights for unigrams, bigrams, trigrams and so on (one or a list of weights)\n    :type weights: tuple(float) / list(tuple(float))\n    :param smoothing_function:\n    :type smoothing_function: SmoothingFunction\n    :param auto_reweigh: Option to re-normalize the weights uniformly.\n    :type auto_reweigh: bool\n    :return: The sentence-level BLEU score. Returns a list if multiple weights were supplied.\n    :rtype: float / list(float)\n    \"\"\"\n    return corpus_bleu([references], [hypothesis], weights,\n        smoothing_function, auto_reweigh)", "test_code_list": [{"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_zero_matches(self):\n\t    references = ['The candidate has no alignment to any of the references'\n\t        .split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    for n in range(1, len(hypothesis)):\n\t        weights = (1.0 / n,) * n\n\t        assert sentence_bleu(references, hypothesis, weights) == 0\n\t\nTestBLEU().test_zero_matches()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_full_matches(self):\n\t    references = ['John loves Mary'.split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    for n in range(1, len(hypothesis)):\n\t        weights = (1.0 / n,) * n\n\t        assert sentence_bleu(references, hypothesis, weights) == 1.0\n\t\nTestBLEU().test_full_matches()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_partial_matches_hypothesis_longer_than_reference(self):\n\t    references = ['John loves Mary'.split()]\n\t    hypothesis = 'John loves Mary who loves Mike'.split()\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4\n\t        )\n\t    try:\n\t        self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n\t    except AttributeError:\n\t        pass\n\t\nTestBLEU().test_partial_matches_hypothesis_longer_than_reference()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_case_where_n_is_bigger_than_hypothesis_length(self):\n\t    references = ['John loves Mary ?'.split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    n = len(hypothesis) + 1\n\t    weights = (1.0 / n,) * n\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis, weights), \n\t        0.0, places=4)\n\t    try:\n\t        self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n\t    except AttributeError:\n\t        pass\n\t    references = ['John loves Mary'.split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis, weights), \n\t        0.0, places=4)\n\t\nTestBLEUFringeCases().test_case_where_n_is_bigger_than_hypothesis_length()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_empty_hypothesis(self):\n\t    references = ['The candidate has no alignment to any of the references'\n\t        .split()]\n\t    hypothesis = []\n\t    assert sentence_bleu(references, hypothesis) == 0\n\t\nTestBLEUFringeCases().test_empty_hypothesis()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_empty_references(self):\n\t    references = [[]]\n\t    hypothesis = 'John loves Mary'.split()\n\t    assert sentence_bleu(references, hypothesis) == 0\n\t\nTestBLEUFringeCases().test_empty_references()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_empty_references_and_hypothesis(self):\n\t    references = [[]]\n\t    hypothesis = []\n\t    assert sentence_bleu(references, hypothesis) == 0\n\t\nTestBLEUFringeCases().test_empty_references_and_hypothesis()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_reference_or_hypothesis_shorter_than_fourgrams(self):\n\t    references = ['let it go'.split()]\n\t    hypothesis = 'let go it'.split()\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4\n\t        )\n\t    try:\n\t        self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n\t    except AttributeError:\n\t        pass\n\t\nTestBLEUFringeCases().test_reference_or_hypothesis_shorter_than_fourgrams()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}], "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None, auto_reweigh=False): [MASK]\n"}
{"model_name": "std", "repository": "nltk", "full_method_name": "grow_diag_final_and", "method_path": "../srcdata/Computation/nltk/nltk/translate/gdfa.py", "response": "from collections import defaultdict\ndef grow_diag_final_and(srclen, trglen, e2f, f2e):\n    \"\"\"\n    This module symmetrisatizes the source-to-target and target-to-source\n    word alignment output and produces, aka. GDFA algorithm (Koehn, 2005).\n\n    Step 1: Find the intersection of the bidirectional alignment.\n\n    Step 2: Search for additional neighbor alignment points to be added, given\n            these criteria: (i) neighbor alignments points are not in the\n            intersection and (ii) neighbor alignments are in the union.\n\n    Step 3: Add all other alignment points that are not in the intersection, not in\n            the neighboring alignments that met the criteria but in the original\n            forward/backward alignment outputs.\n\n        >>> forw = ('0-0 2-1 9-2 21-3 10-4 7-5 11-6 9-7 12-8 1-9 3-10 '\n        ...         '4-11 17-12 17-13 25-14 13-15 24-16 11-17 28-18')\n        >>> back = ('0-0 1-9 2-9 3-10 4-11 5-12 6-6 7-5 8-6 9-7 10-4 '\n        ...         '11-6 12-8 13-12 15-12 17-13 18-13 19-12 20-13 '\n        ...         '21-3 22-12 23-14 24-17 25-15 26-17 27-18 28-18')\n        >>> srctext = (\"\u3053\u306e \u3088\u3046 \u306a \u30cf\u30ed\u30fc \u767d\u8272 \u308f\u3044 \u661f \u306e \uff2c \u95a2\u6570 \"\n        ...            \"\u306f \uff2c \u3068 \u5171 \u306b \u4e0d\u9023\u7d9a \u306b \u5897\u52a0 \u3059\u308b \u3053\u3068 \u304c \"\n        ...            \"\u671f\u5f85 \u3055 \u308c\u308b \u3053\u3068 \u3092 \u793a\u3057 \u305f \u3002\")\n        >>> trgtext = (\"Therefore , we expect that the luminosity function \"\n        ...            \"of such halo white dwarfs increases discontinuously \"\n        ...            \"with the luminosity .\")\n        >>> srclen = len(srctext.split())\n        >>> trglen = len(trgtext.split())\n        >>>\n        >>> gdfa = grow_diag_final_and(srclen, trglen, forw, back)\n        >>> gdfa == sorted(set([(28, 18), (6, 6), (24, 17), (2, 1), (15, 12), (13, 12),\n        ...         (2, 9), (3, 10), (26, 17), (25, 15), (8, 6), (9, 7), (20,\n        ...         13), (18, 13), (0, 0), (10, 4), (13, 15), (23, 14), (7, 5),\n        ...         (25, 14), (1, 9), (17, 13), (4, 11), (11, 17), (9, 2), (22,\n        ...         12), (27, 18), (24, 16), (21, 3), (19, 12), (17, 12), (5,\n        ...         12), (11, 6), (12, 8)]))\n        True\n\n    References:\n    Koehn, P., A. Axelrod, A. Birch, C. Callison, M. Osborne, and D. Talbot.\n    2005. Edinburgh System Description for the 2005 IWSLT Speech\n    Translation Evaluation. In MT Eval Workshop.\n\n    :type srclen: int\n    :param srclen: the number of tokens in the source language\n    :type trglen: int\n    :param trglen: the number of tokens in the target language\n    :type e2f: str\n    :param e2f: the forward word alignment outputs from source-to-target\n                language (in pharaoh output format)\n    :type f2e: str\n    :param f2e: the backward word alignment outputs from target-to-source\n                language (in pharaoh output format)\n    :rtype: set(tuple(int))\n    :return: the symmetrized alignment points from the GDFA algorithm\n    \"\"\"\n    e2f = [tuple(map(int, a.split('-'))) for a in e2f.split()]\n    f2e = [tuple(map(int, a.split('-'))) for a in f2e.split()]\n    neighbors = [(-1, 0), (0, -1), (1, 0), (0, 1), (-1, -1), (-1, 1), (1, -\n        1), (1, 1)]\n    alignment = set(e2f).intersection(set(f2e))\n    union = set(e2f).union(set(f2e))\n    aligned = defaultdict(set)\n    for i, j in alignment:\n        aligned['e'].add(i)\n        aligned['f'].add(j)\n\n    def grow_diag():\n        \"\"\"\n        Search for the neighbor points and them to the intersected alignment\n        points if criteria are met.\n        \"\"\"\n        prev_len = len(alignment) - 1\n        while prev_len < len(alignment):\n            no_new_points = True\n            for e in range(srclen):\n                for f in range(trglen):\n                    if (e, f) in alignment:\n                        for neighbor in neighbors:\n                            neighbor = tuple(i + j for i, j in zip((e, f),\n                                neighbor))\n                            e_new, f_new = neighbor\n                            if (e_new not in aligned and f_new not in aligned\n                                ) and neighbor in union:\n                                alignment.add(neighbor)\n                                aligned['e'].add(e_new)\n                                aligned['f'].add(f_new)\n                                prev_len += 1\n                                no_new_points = False\n            if no_new_points:\n                break\n\n    def final_and(a):\n        \"\"\"\n        Adds remaining points that are not in the intersection, not in the\n        neighboring alignments but in the original *e2f* and *f2e* alignments\n        \"\"\"\n        for e_new in range(srclen):\n            for f_new in range(trglen):\n                if e_new not in aligned and f_new not in aligned and (e_new,\n                    f_new) in union:\n                    alignment.add((e_new, f_new))\n                    aligned['e'].add(e_new)\n                    aligned['f'].add(f_new)\n    grow_diag()\n    final_and(e2f)\n    final_and(f2e)\n    return sorted(alignment)", "test_code_list": [{"test_code": "import unittest\nfrom nltk.translate.gdfa import grow_diag_final_and\n\nclass TestGDFA(unittest.TestCase):\n\tdef test_from_eflomal_outputs(self):\n\t    \"\"\"\n\t        Testing GDFA with first 10 eflomal outputs from issue #1829\n\t        https://github.com/nltk/nltk/issues/1829\n\t        \"\"\"\n\t    forwards = ['0-0 1-2', '0-0 1-1',\n\t        '0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 7-8 9-9 10-10 9-11 11-12 12-13 13-14',\n\t        '0-0 1-1 1-2 2-3 3-4 4-5 4-6 5-7 6-8 8-9 9-10',\n\t        '0-0 14-1 15-2 16-3 20-5 21-6 22-7 5-8 6-9 7-10 8-11 9-12 10-13 11-14 12-15 13-16 14-17 17-18 18-19 19-20 20-21 23-22 24-23 25-24 26-25 27-27 28-28 29-29 30-30 31-31'\n\t        , '0-0 1-1 0-2 2-3', '0-0 2-2 4-4',\n\t        '0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-20'\n\t        ,\n\t        '3-0 4-1 6-2 5-3 6-4 7-5 8-6 9-7 10-8 11-9 16-10 9-12 10-13 12-14',\n\t        '1-0']\n\t    backwards = ['0-0 1-2', '0-0 1-1',\n\t        '0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 10-10 11-12 12-11 13-13',\n\t        '0-0 1-2 2-3 3-4 4-6 6-8 7-5 8-7 9-8',\n\t        '0-0 1-8 2-9 3-10 4-11 5-12 6-11 8-13 9-14 10-15 11-16 12-17 13-18 14-19 15-20 16-21 17-22 18-23 19-24 20-29 21-30 22-31 23-2 24-3 25-4 26-5 27-5 28-6 29-7 30-28 31-31'\n\t        , '0-0 1-1 2-3', '0-0 1-1 2-3 4-4',\n\t        '0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-16 21-18'\n\t        ,\n\t        '0-0 1-1 3-2 4-1 5-3 6-4 7-5 8-6 9-7 10-8 11-9 12-8 13-9 14-8 15-9 16-10'\n\t        , '1-0']\n\t    source_lens = [2, 3, 3, 15, 11, 33, 4, 6, 23, 18]\n\t    target_lens = [2, 4, 3, 16, 12, 33, 5, 6, 22, 16]\n\t    expected = [[(0, 0), (1, 2)], [(0, 0), (1, 1)], [(0, 0), (2, 1), (3, 2),\n\t        (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (10, 10), (11, 12)], [(0, 0\n\t        ), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (4, 6), (5, 7), (6, 8),\n\t        (7, 5), (8, 7), (8, 9), (9, 8), (9, 10)], [(0, 0), (1, 8), (2, 9),\n\t        (3, 10), (4, 11), (5, 8), (6, 9), (6, 11), (7, 10), (8, 11), (31, \n\t        31)], [(0, 0), (0, 2), (1, 1), (2, 3)], [(0, 0), (1, 1), (2, 2), (2,\n\t        3), (4, 4)], [(0, 0), (1, 1), (2, 3), (3, 4), (5, 5), (7, 6), (8, 7\n\t        ), (9, 8), (10, 9), (11, 10), (12, 11), (13, 12), (14, 13), (15, 14\n\t        ), (16, 16), (17, 17), (18, 18), (19, 19)], [(0, 0), (1, 1), (3, 0),\n\t        (3, 2), (4, 1), (5, 3), (6, 2), (6, 4), (7, 5), (8, 6), (9, 7), (9,\n\t        12), (10, 8), (10, 13), (11, 9), (12, 8), (12, 14), (13, 9), (14, 8\n\t        ), (15, 9), (16, 10)], [(1, 0)], [(0, 0), (1, 1), (3, 2), (4, 3), (\n\t        5, 4), (6, 5), (7, 6), (9, 10), (10, 12), (11, 13), (12, 14), (13, 15)]\n\t        ]\n\t    for fw, bw, src_len, trg_len, expect in zip(forwards, backwards,\n\t        source_lens, target_lens, expected):\n\t        self.assertListEqual(expect, grow_diag_final_and(src_len, trg_len,\n\t            fw, bw))\n\t\nTestGDFA().test_from_eflomal_outputs()\n", "code_start": "", "test_path": "../srcdata/Computation/nltk/nltk/test/unit/translate/test_gdfa.py"}], "method_code_mask": "from collections import defaultdict\n\n\ndef grow_diag_final_and(srclen, trglen, e2f, f2e): [MASK]\n"}

{"model_name": "gpt-4o-mini", "repository": "tensorflow", "full_method_name": "make_decorator", "method_path": "/home/zhuqiming2022/work/srcdata/AI_and_SE/tensorflow/tensorflow/python/util/tf_decorator.py", "response": "import inspect\nfrom typing import Dict\nfrom typing import Any\n\n\ndef make_decorator(target, decorator_func, decorator_name=None,\n    decorator_doc='', decorator_argspec=None):\n    \n    if decorator_name is None:\n        decorator_name = decorator_func.__name__\n    \n    if decorator_doc:\n        decorator_func.__doc__ = decorator_doc\n    else:\n        decorator_func.__doc__ = target.__doc__\n    \n    if decorator_argspec is not None:\n        signature = inspect.signature(decorator_argspec)\n    else:\n        signature = inspect.signature(target)\n    \n    decorator_func.__name__ = decorator_name\n    decorator_func.__signature__ = signature\n    \n    def wrapper(*args, **kwargs):\n        return decorator_func(target, *args, **kwargs)\n    \n    wrapper.__name__ = target.__name__\n    wrapper.__doc__ = target.__doc__\n    wrapper.__signature__ = inspect.signature(target)\n    \n    return wrapper", "test_code_list": [{"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef assert_no_new_pyobjects_executing_eagerly() ->Callable[\n    [Callable[..., Any]], Callable[..., None]]:\n    \"\"\"Decorator for asserting that no new Python objects persist after a test.\n\n  Returns a decorator that runs the test multiple times executing eagerly,\n  first as a warmup and then to let objects accumulate. The warmup helps ignore\n  caches which do not grow as the test is run repeatedly.\n\n  Useful for checking that there are no missing Py_DECREFs in the C exercised by\n  a bit of Python.\n\n  Args:\n    warmup_iters: The numer of warmup iterations, excluded from measuring.\n\n  Returns:\n    A decorator function which can be applied to the test function.\n  \"\"\"\n\n    def wrap_f(f: Callable[..., Any]) ->Callable[..., None]:\n\n        def decorator(self: 'TensorFlowTestCase', *args, **kwargs) ->None:\n            \"\"\"Warms up, gets object counts, runs the test, checks for new objects.\"\"\"\n            with context.eager_mode():\n                gc.disable()\n                test_errors = None\n                test_skipped = None\n                if hasattr(self._outcome, 'errors'):\n                    test_errors = self._outcome.errors\n                    test_skipped = self._outcome.skipped\n                else:\n                    test_errors = self._outcome.result.errors\n                    test_skipped = self._outcome.result.skipped\n                for _ in range(warmup_iters):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                obj_count_by_type = _get_object_count_by_type()\n                gc.collect()\n                registered_function_names = context.context(\n                    ).list_function_names()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped))\n                if ops.has_default_graph():\n                    collection_sizes_before = {collection: len(ops.\n                        get_collection(collection)) for collection in ops.\n                        get_default_graph().collections}\n                for _ in range(3):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                if ops.has_default_graph():\n                    for collection_key in ops.get_default_graph().collections:\n                        collection = ops.get_collection(collection_key)\n                        size_before = collection_sizes_before.get(\n                            collection_key, 0)\n                        if len(collection) > size_before:\n                            raise AssertionError(\n                                'Collection %s increased in size from %d to %d (current items %s).'\n                                 % (collection_key, size_before, len(\n                                collection), collection))\n                        del collection\n                        del collection_key\n                        del size_before\n                    del collection_sizes_before\n                gc.collect()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped)\n                    ) - obj_count_by_type\n                leftover_functions = context.context().list_function_names(\n                    ) - registered_function_names\n                assert not leftover_functions, 'The following functions were newly created: %s' % leftover_functions\n                assert not obj_count_by_type, 'The following objects were newly created: %s' % str(\n                    obj_count_by_type)\n                gc.enable()\n        return make_decorator(f, decorator)\n    return wrap_f\n\nassert_no_new_pyobjects_executing_eagerly()\n", "code_start": "", "test_path": "/home/zhuqiming2022/work/srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}, {"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef run_in_v1_v2(\n    bool=False) ->Callable[[Callable[..., Any]], Callable[..., None]]:\n    \"\"\"Execute the decorated test in v1 and v2 modes.\n\n  The overall execution is similar to that of `run_in_graph_and_eager_mode`.\n\n  Args:\n    device_to_use: A string in the following format: \"/device:CPU:0\".\n    assert_no_eager_garbage: If True, sets DEBUG_SAVEALL on the garbage\n      collector and asserts that no extra garbage has been created when running\n      the test with eager execution enabled. This will fail if there are\n      reference cycles (e.g. a = []; a.append(a)). Off by default because some\n      tests may create garbage for legitimate reasons (e.g. they define a class\n      which inherits from `object`), and because DEBUG_SAVEALL is sticky in some\n      Python interpreters (meaning that tests which rely on objects being\n      collected elsewhere in the unit test file will not work). Additionally,\n      checks that nothing still has a reference to Tensors that the test\n      allocated.\n\n  Returns:\n    A decorator that runs a given test in v1 and v2 modes.\n  \"\"\"\n\n    def decorator(f: Callable[..., Any]) ->Callable[..., None]:\n        decorator_tag = 'wrapped_with_v1_v2_decorator'\n        if hasattr(f, decorator_tag):\n            return f\n\n        def decorated(self: 'TensorFlowTestCase', *args, **kwargs) ->None:\n            logging.info('Running %s in V1 mode.', f.__name__)\n            try:\n                with self.subTest('V1_mode'):\n                    v2_compat.disable_v2_behavior()\n                    f(self, *args, **kwargs)\n            except unittest.case.SkipTest:\n                pass\n\n            def run_v2(self: 'TensorFlowTestCase', **kwargs) ->None:\n                logging.info('Running %s in V2 mode.', f.__name__)\n                if device_to_use:\n                    with ops.device(device_to_use):\n                        f(self, *args, **kwargs)\n                else:\n                    f(self, *args, **kwargs)\n            if assert_no_eager_garbage:\n                ops.reset_default_graph()\n                run_v2 = assert_no_new_tensors(assert_no_garbage_created(\n                    run_v2))\n            self.tearDown()\n            self._tempdir = None\n            ops.reset_default_graph()\n            v2_compat.enable_v2_behavior()\n            with self.subTest('V2_mode'):\n                self.setUp()\n                run_v2(self, **kwargs)\n        tf_decorated = make_decorator(f, decorated)\n        tf_decorated.__dict__[decorator_tag] = True\n        return tf_decorated\n    return decorator\n\nrun_in_v1_v2()\n", "code_start": "", "test_path": "/home/zhuqiming2022/work/srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}, {"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef run_without_tensor_float_32() ->Callable[[Callable[...,\n    Any]], Callable[..., None]]:\n    \"\"\"Execute test with TensorFloat-32 disabled.\n\n  While almost every real-world deep learning model runs fine with\n  TensorFloat-32, many tests use assertAllClose or similar methods.\n  TensorFloat-32 matmuls typically will cause such methods to fail with the\n  default tolerances.\n\n  Args:\n    description: A description used for documentation purposes, describing why\n      the test requires TensorFloat-32 to be disabled.\n\n  Returns:\n    Decorator which runs a test with TensorFloat-32 disabled.\n  \"\"\"\n\n    def decorator(f: Callable[..., Any]) ->Callable[..., None]:\n\n        @functools.wraps(f)\n        def decorated(*args, **kwargs):\n            allowed = config.tensor_float_32_execution_enabled()\n            try:\n                config.enable_tensor_float_32_execution(False)\n                f(*args, **kwargs)\n            finally:\n                config.enable_tensor_float_32_execution(allowed)\n        return make_decorator(f, decorated)\n    return decorator\n\nrun_without_tensor_float_32()\n", "code_start": "", "test_path": "/home/zhuqiming2022/work/srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}], "method_code_mask": "import inspect\nfrom typing import Dict\nfrom typing import Any\n\n\ndef make_decorator(target, decorator_func, decorator_name=None,\n    decorator_doc='', decorator_argspec=None): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "tensorflow", "full_method_name": "_get_object_count_by_type", "method_path": "/home/zhuqiming2022/work/srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py", "response": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\n\n\ndef _get_object_count_by_type(exclude: Iterable[Any]=()) -> collections.Counter[str]:\n    # Create a set from the exclude iterable for faster lookups\n    exclude_set = set(exclude)\n    \n    # Create a Counter to hold the counts of each type\n    type_counter = collections.Counter()\n    \n    # Use the garbage collector to get all objects in memory\n    all_objects = gc.get_objects()\n    \n    for obj in all_objects:\n        # Check if the object is in the exclude set\n        if obj in exclude_set:\n            continue\n        \n        # Get the type of the object and increment the counter\n        obj_type = type(obj).__name__\n        type_counter[obj_type] += 1\n    \n    return type_counter", "test_code_list": [{"test_code": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\ndef assert_no_new_pyobjects_executing_eagerly() ->Callable[\n    [Callable[..., Any]], Callable[..., None]]:\n    \"\"\"Decorator for asserting that no new Python objects persist after a test.\n\n  Returns a decorator that runs the test multiple times executing eagerly,\n  first as a warmup and then to let objects accumulate. The warmup helps ignore\n  caches which do not grow as the test is run repeatedly.\n\n  Useful for checking that there are no missing Py_DECREFs in the C exercised by\n  a bit of Python.\n\n  Args:\n    warmup_iters: The numer of warmup iterations, excluded from measuring.\n\n  Returns:\n    A decorator function which can be applied to the test function.\n  \"\"\"\n\n    def wrap_f(f: Callable[..., Any]) ->Callable[..., None]:\n\n        def decorator(self: 'TensorFlowTestCase', *args, **kwargs) ->None:\n            \"\"\"Warms up, gets object counts, runs the test, checks for new objects.\"\"\"\n            with context.eager_mode():\n                gc.disable()\n                test_errors = None\n                test_skipped = None\n                if hasattr(self._outcome, 'errors'):\n                    test_errors = self._outcome.errors\n                    test_skipped = self._outcome.skipped\n                else:\n                    test_errors = self._outcome.result.errors\n                    test_skipped = self._outcome.result.skipped\n                for _ in range(warmup_iters):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                obj_count_by_type = _get_object_count_by_type()\n                gc.collect()\n                registered_function_names = context.context(\n                    ).list_function_names()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped))\n                if ops.has_default_graph():\n                    collection_sizes_before = {collection: len(ops.\n                        get_collection(collection)) for collection in ops.\n                        get_default_graph().collections}\n                for _ in range(3):\n                    f(self, *args, **kwargs)\n                self.doCleanups()\n                if ops.has_default_graph():\n                    for collection_key in ops.get_default_graph().collections:\n                        collection = ops.get_collection(collection_key)\n                        size_before = collection_sizes_before.get(\n                            collection_key, 0)\n                        if len(collection) > size_before:\n                            raise AssertionError(\n                                'Collection %s increased in size from %d to %d (current items %s).'\n                                 % (collection_key, size_before, len(\n                                collection), collection))\n                        del collection\n                        del collection_key\n                        del size_before\n                    del collection_sizes_before\n                gc.collect()\n                obj_count_by_type = _get_object_count_by_type(exclude=gc.\n                    get_referents(test_errors, test_skipped)\n                    ) - obj_count_by_type\n                leftover_functions = context.context().list_function_names(\n                    ) - registered_function_names\n                assert not leftover_functions, 'The following functions were newly created: %s' % leftover_functions\n                assert not obj_count_by_type, 'The following objects were newly created: %s' % str(\n                    obj_count_by_type)\n                gc.enable()\n        return tf_decorator.make_decorator(f, decorator)\n    return wrap_f\n\nassert_no_new_pyobjects_executing_eagerly()\n", "code_start": "", "test_path": "/home/zhuqiming2022/work/srcdata/AI_and_SE/tensorflow/tensorflow/python/framework/test_util.py"}], "method_code_mask": "import collections\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Callable\nfrom collections.abc import Collection\nfrom collections.abc import Sequence\nimport contextlib\nimport functools\nimport gc\nimport itertools\nimport math\nimport os\nimport random\nimport re\nimport tempfile\nimport threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TypeVar\nimport unittest\nimport numpy as np\n\n\ndef _get_object_count_by_type(exclude: Iterable[Any]=()) ->collections.Counter[\n    str]: [MASK]\n"}

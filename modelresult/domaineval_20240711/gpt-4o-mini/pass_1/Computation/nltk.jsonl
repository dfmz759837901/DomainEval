{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "_wikipedia_example_hmm", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_hmm.py", "response": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\n\n\ndef _wikipedia_example_hmm():\n    # Define the states and symbols\n    states = ['rain', 'no rain']\n    symbols = ['umbrella', 'no umbrella']\n\n    # Define the transition probabilities\n    transition_probs = {\n        'rain': {'rain': 0.7, 'no rain': 0.3},\n        'no rain': {'rain': 0.4, 'no rain': 0.6}\n    }\n\n    # Define the emission probabilities\n    emission_probs = {\n        'rain': {'umbrella': 0.9, 'no umbrella': 0.1},\n        'no rain': {'umbrella': 0.2, 'no umbrella': 0.8}\n    }\n\n    # Define the initial state probabilities\n    initial_probs = {'rain': 0.6, 'no rain': 0.4}\n\n    # Create the HMM model\n    model = hmm.HiddenMarkovModelTrainer(states, symbols).train([([], [])], transition_probs, emission_probs, initial_probs)\n\n    # Define the sequence of observations\n    seq = [('umbrella', None), ('no umbrella', None), ('umbrella', None)]\n\n    return model, states, symbols, seq", "test_code_list": [{"test_code": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\ndef test_forward_probability2():\n    from numpy.testing import assert_array_almost_equal\n    model, states, symbols, seq = _wikipedia_example_hmm()\n    fp = 2 ** model._forward_probability(seq)\n    fp = (fp.T / fp.sum(axis=1)).T\n    wikipedia_results = [[0.8182, 0.1818], [0.8834, 0.1166], [0.1907, \n        0.8093], [0.7308, 0.2692], [0.8673, 0.1327]]\n    assert_array_almost_equal(wikipedia_results, fp, 4)\n\ntest_forward_probability2()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_hmm.py"}, {"test_code": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\ndef test_backward_probability():\n    from numpy.testing import assert_array_almost_equal\n    model, states, symbols, seq = _wikipedia_example_hmm()\n    bp = 2 ** model._backward_probability(seq)\n    bp = (bp.T / bp.sum(axis=1)).T\n    wikipedia_results = [[0.5923, 0.4077], [0.3763, 0.6237], [0.6533, \n        0.3467], [0.6273, 0.3727], [0.5, 0.5]]\n    assert_array_almost_equal(wikipedia_results, bp, 4)\n\ntest_backward_probability()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_hmm.py"}], "method_code_mask": "import pytest\nfrom nltk.tag import hmm\nfrom numpy.testing import assert_array_almost_equal\n\n\ndef _wikipedia_example_hmm(): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "corpus_ribes", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/translate/ribes_score.py", "response": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\ndef corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.1):\n    def ribes(reference, hypothesis, alpha, beta):\n        # Calculate unigram precision\n        ref_ngrams = set(ngrams(reference, 1))\n        hyp_ngrams = set(ngrams(hypothesis, 1))\n        precision = len(ref_ngrams.intersection(hyp_ngrams)) / max(len(hyp_ngrams), 1)\n        \n        # Calculate brevity penalty\n        ref_length = len(reference)\n        hyp_length = len(hypothesis)\n        brevity_penalty = math.exp(1 - (ref_length / hyp_length)) if hyp_length < ref_length else 1.0\n        \n        # Calculate RIBES score\n        return (1 - alpha) * precision + alpha * brevity_penalty\n\n    total_score = 0.0\n    for hypothesis in hypotheses:\n        best_score = float('-inf')\n        for references in list_of_references:\n            for reference in references:\n                score = ribes(reference.split(), hypothesis.split(), alpha, beta)\n                best_score = max(best_score, score)\n        total_score += best_score\n\n    return total_score / max(len(hypotheses), 1)", "test_code_list": [{"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_empty_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's neat and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == []\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_empty_worder()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_one_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == [3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_one_worder()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_two_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference is different\".split(\n        )\n    assert word_rank_alignment(ref, hyp) == [9, 3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_two_worder()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes():\n    hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures',\n        'that', 'the', 'military', 'always', 'obeys', 'the', 'commands',\n        'of', 'the', 'party']\n    ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures',\n        'that', 'the', 'military', 'will', 'forever', 'heed', 'Party',\n        'commands']\n    ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n        'guarantees', 'the', 'military', 'forces', 'always', 'being',\n        'under', 'the', 'command', 'of', 'the', 'Party']\n    ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', 'army',\n        'always', 'to', 'heed', 'the', 'directions', 'of', 'the', 'party']\n    hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n        'interested', 'in', 'world', 'history']\n    ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because',\n        'he', 'read', 'the', 'book']\n    list_of_refs = [[ref1a, ref1b, ref1c], [ref2a]]\n    hypotheses = [hyp1, hyp2]\n    score = corpus_ribes(list_of_refs, hypotheses)\n    assert round(score, 4) == 0.3597\n\ntest_ribes()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_no_zero_div():\n    hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures',\n        'that', 'the', 'military', 'always', 'obeys', 'the', 'commands',\n        'of', 'the', 'party']\n    ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures',\n        'that', 'the', 'military', 'will', 'forever', 'heed', 'Party',\n        'commands']\n    ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n        'guarantees', 'the', 'military', 'forces', 'always', 'being',\n        'under', 'the', 'command', 'of', 'the', 'Party']\n    ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', 'army',\n        'always', 'to', 'heed', 'the', 'directions', 'of', 'the', 'party']\n    hyp2 = ['he', 'read', 'the']\n    ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because',\n        'he']\n    list_of_refs = [[ref1a, ref1b, ref1c], [ref2a]]\n    hypotheses = [hyp1, hyp2]\n    score = corpus_ribes(list_of_refs, hypotheses)\n    assert round(score, 4) == 0.1688\n\ntest_no_zero_div()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}], "method_code_mask": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\n\ndef corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.1): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "word_rank_alignment", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/translate/ribes_score.py", "response": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\ndef word_rank_alignment(reference, hypothesis, character_based=False):\n    worder = []\n    reference_indices = {word: [] for word in reference}\n\n    # Create a mapping of words in the reference to their indices\n    for index, word in enumerate(reference):\n        reference_indices[word].append(index)\n\n    # Iterate through the hypothesis and find the corresponding indices in the reference\n    for word in hypothesis:\n        if word in reference_indices and reference_indices[word]:\n            worder.append(reference_indices[word].pop(0))  # Get the first occurrence and remove it\n\n    return worder", "test_code_list": [{"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_empty_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's neat and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == []\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_empty_worder()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_one_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference's different\".split()\n    assert word_rank_alignment(ref, hyp) == [3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_one_worder()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}, {"test_code": "from nltk.translate.ribes_score import corpus_ribes\nfrom nltk.translate.ribes_score import word_rank_alignment\ndef test_ribes_two_worder():\n    hyp = 'This is a nice sentence which I quite like'.split()\n    ref = \"Okay well that's nice and all but the reference is different\".split(\n        )\n    assert word_rank_alignment(ref, hyp) == [9, 3]\n    list_of_refs = [[ref]]\n    hypotheses = [hyp]\n    assert corpus_ribes(list_of_refs, hypotheses) == 0.0\n\ntest_ribes_two_worder()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_ribes.py"}], "method_code_mask": "import math\nfrom itertools import islice\nfrom nltk.util import choose\nfrom nltk.util import ngrams\n\n\ndef word_rank_alignment(reference, hypothesis, character_based=False): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "_pair_iter", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/tokenize/punkt.py", "response": "import math\nimport re\nimport string\nfrom collections import defaultdict\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Match\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize.api import TokenizerI\n\n\ndef _pair_iter(iterator: Iterator) -> Iterator[Tuple[Any, Optional[Any]]]:\n    it = iter(iterator)\n    current = next(it, None)\n    for next_item in it:\n        yield (current, next_item)\n        current = next_item\n    yield (current, None)", "test_code_list": [{"test_code": "from typing import List\nfrom typing import Tuple\nimport pytest\nfrom nltk.tokenize import LegalitySyllableTokenizer\nfrom nltk.tokenize import StanfordSegmenter\nfrom nltk.tokenize import SyllableTokenizer\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import punkt\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.simple import CharTokenizer\nfrom nltk.corpus import words\n\nclass TestTokenize():\n\tdef test_punkt_pair_iter(self):\n\t    test_cases = [('12', [('1', '2'), ('2', None)]), ('123', [('1', '2'), (\n\t        '2', '3'), ('3', None)]), ('1234', [('1', '2'), ('2', '3'), ('3',\n\t        '4'), ('4', None)])]\n\t    for test_input, expected_output in test_cases:\n\t        actual_output = [x for x in _pair_iter(test_input)]\n\t        assert actual_output == expected_output\n\t\nTestTokenize().test_punkt_pair_iter()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/test_tokenize.py"}], "method_code_mask": "import math\nimport re\nimport string\nfrom collections import defaultdict\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Match\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize.api import TokenizerI\n\n\ndef _pair_iter(iterator): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "padded_everygram_pipeline", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/lm/preprocessing.py", "response": "from functools import partial\nfrom itertools import chain\nfrom nltk.util import everygrams\nfrom nltk.util import pad_sequence\n\n\ndef padded_everygram_pipeline(order, text):\n    # Create a function to pad each sentence\n    padder = partial(pad_sequence, pad_left=True, pad_right=True, n=order)\n    \n    # Create an iterator for everygrams\n    everygram_iterator = (\n        everygrams(padder(sentence), max_len=order) for sentence in text\n    )\n    \n    # Create an iterator for flat stream of words\n    flat_word_iterator = chain.from_iterable(padder(sentence) for sentence in text)\n    \n    return everygram_iterator, flat_word_iterator", "test_code_list": [{"test_code": "import unittest\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\n\nclass TestPreprocessing(unittest.TestCase):\n\tdef test_padded_everygram_pipeline(self):\n\t    expected_train = [[('<s>',), ('<s>', 'a'), ('a',), ('a', 'b'), ('b',),\n\t        ('b', 'c'), ('c',), ('c', '</s>'), ('</s>',)]]\n\t    expected_vocab = ['<s>', 'a', 'b', 'c', '</s>']\n\t    train_data, vocab_data = padded_everygram_pipeline(2, [['a', 'b', 'c']])\n\t    self.assertEqual([list(sent) for sent in train_data], expected_train)\n\t    self.assertEqual(list(vocab_data), expected_vocab)\n\t\nTestPreprocessing().test_padded_everygram_pipeline()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/lm/test_preprocessing.py"}], "method_code_mask": "from functools import partial\nfrom itertools import chain\nfrom nltk.util import everygrams\nfrom nltk.util import pad_sequence\n\n\ndef padded_everygram_pipeline(order, text): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "brevity_penalty", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/translate/bleu_score.py", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef brevity_penalty(closest_ref_len, hyp_len):\n    if hyp_len > closest_ref_len:\n        return 1.0\n    elif hyp_len == 0:\n        return 0.0\n    else:\n        return math.exp(1 - closest_ref_len / hyp_len)", "test_code_list": [{"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_brevity_penalty(self):\n\t    references = [['a'] * 11, ['a'] * 8]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    self.assertAlmostEqual(brevity_penalty(closest_ref_len, hyp_len), \n\t        0.8669, places=4)\n\t    references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    assert brevity_penalty(closest_ref_len, hyp_len) == 1.0\n\t\nTestBLEU().test_brevity_penalty()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}], "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef brevity_penalty(closest_ref_len, hyp_len): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "closest_ref_length", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/translate/bleu_score.py", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef closest_ref_length(references, hyp_len):\n    closest_ref_len = min(references, key=lambda ref: abs(len(ref) - hyp_len))\n    return len(closest_ref_len)", "test_code_list": [{"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_brevity_penalty(self):\n\t    references = [['a'] * 11, ['a'] * 8]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    self.assertAlmostEqual(brevity_penalty(closest_ref_len, hyp_len), \n\t        0.8669, places=4)\n\t    references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n\t    hypothesis = ['a'] * 7\n\t    hyp_len = len(hypothesis)\n\t    closest_ref_len = closest_ref_length(references, hyp_len)\n\t    assert brevity_penalty(closest_ref_len, hyp_len) == 1.0\n\t\nTestBLEU().test_brevity_penalty()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}], "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef closest_ref_length(references, hyp_len): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "sentence_bleu", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/translate/bleu_score.py", "response": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None, auto_reweigh=False):\n    \n    # Check if weights sum to 1\n    if auto_reweigh:\n        weights = tuple(w / sum(weights) for w in weights)\n\n    # Count n-grams in the hypothesis\n    hypothesis_ngrams = [Counter(ngrams(hypothesis, n)) for n in range(1, len(weights) + 1)]\n    \n    # Count n-grams in the references\n    reference_ngrams = [Counter() for _ in range(len(references))]\n    for ref in references:\n        for n in range(1, len(weights) + 1):\n            reference_ngrams[n-1] += Counter(ngrams(ref, n))\n\n    # Calculate precision for each n-gram\n    precisions = []\n    for i, hyp_ngram in enumerate(hypothesis_ngrams):\n        ref_ngram_counts = Counter()\n        for ref_count in reference_ngrams:\n            ref_ngram_counts |= ref_count  # Union of counts\n        overlap = sum((hyp_ngram & ref_ngram_counts).values())\n        total = sum(hyp_ngram.values())\n        precision = overlap / total if total > 0 else 0\n        precisions.append(precision)\n\n    # Calculate the geometric mean of the precisions\n    if any(p == 0 for p in precisions):\n        bleu_score = 0\n    else:\n        bleu_score = math.exp(sum(w * math.log(p) for w, p in zip(weights, precisions)))\n\n    # Apply brevity penalty\n    reference_lengths = [len(ref) for ref in references]\n    closest_ref_length = min(reference_lengths, key=lambda ref_len: abs(ref_len - len(hypothesis)))\n    if len(hypothesis) < closest_ref_length:\n        bp = math.exp(1 - closest_ref_length / len(hypothesis))\n    else:\n        bp = 1.0\n\n    return bleu_score * bp", "test_code_list": [{"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_zero_matches(self):\n\t    references = ['The candidate has no alignment to any of the references'\n\t        .split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    for n in range(1, len(hypothesis)):\n\t        weights = (1.0 / n,) * n\n\t        assert sentence_bleu(references, hypothesis, weights) == 0\n\t\nTestBLEU().test_zero_matches()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_full_matches(self):\n\t    references = ['John loves Mary'.split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    for n in range(1, len(hypothesis)):\n\t        weights = (1.0 / n,) * n\n\t        assert sentence_bleu(references, hypothesis, weights) == 1.0\n\t\nTestBLEU().test_full_matches()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEU(unittest.TestCase):\n\tdef test_partial_matches_hypothesis_longer_than_reference(self):\n\t    references = ['John loves Mary'.split()]\n\t    hypothesis = 'John loves Mary who loves Mike'.split()\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4\n\t        )\n\t    try:\n\t        self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n\t    except AttributeError:\n\t        pass\n\t\nTestBLEU().test_partial_matches_hypothesis_longer_than_reference()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_case_where_n_is_bigger_than_hypothesis_length(self):\n\t    references = ['John loves Mary ?'.split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    n = len(hypothesis) + 1\n\t    weights = (1.0 / n,) * n\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis, weights), \n\t        0.0, places=4)\n\t    try:\n\t        self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n\t    except AttributeError:\n\t        pass\n\t    references = ['John loves Mary'.split()]\n\t    hypothesis = 'John loves Mary'.split()\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis, weights), \n\t        0.0, places=4)\n\t\nTestBLEUFringeCases().test_case_where_n_is_bigger_than_hypothesis_length()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_empty_hypothesis(self):\n\t    references = ['The candidate has no alignment to any of the references'\n\t        .split()]\n\t    hypothesis = []\n\t    assert sentence_bleu(references, hypothesis) == 0\n\t\nTestBLEUFringeCases().test_empty_hypothesis()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_empty_references(self):\n\t    references = [[]]\n\t    hypothesis = 'John loves Mary'.split()\n\t    assert sentence_bleu(references, hypothesis) == 0\n\t\nTestBLEUFringeCases().test_empty_references()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_empty_references_and_hypothesis(self):\n\t    references = [[]]\n\t    hypothesis = []\n\t    assert sentence_bleu(references, hypothesis) == 0\n\t\nTestBLEUFringeCases().test_empty_references_and_hypothesis()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}, {"test_code": "import unittest\nimport numpy as np\nfrom nltk.data import find\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import brevity_penalty\nfrom nltk.translate.bleu_score import closest_ref_length\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import modified_precision\nfrom nltk.translate.bleu_score import sentence_bleu\n\nclass TestBLEUFringeCases(unittest.TestCase):\n\tdef test_reference_or_hypothesis_shorter_than_fourgrams(self):\n\t    references = ['let it go'.split()]\n\t    hypothesis = 'let go it'.split()\n\t    self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4\n\t        )\n\t    try:\n\t        self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)\n\t    except AttributeError:\n\t        pass\n\t\nTestBLEUFringeCases().test_reference_or_hypothesis_shorter_than_fourgrams()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_bleu.py"}], "method_code_mask": "import math\nimport sys\nimport warnings\nfrom collections import Counter\nfrom fractions import Fraction as _Fraction\nfrom nltk.util import ngrams\n\n\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n    smoothing_function=None, auto_reweigh=False): [MASK]\n"}
{"model_name": "gpt-4o-mini", "repository": "nltk", "full_method_name": "grow_diag_final_and", "method_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/translate/gdfa.py", "response": "from collections import defaultdict\n\n\ndef grow_diag_final_and(srclen, trglen, e2f, f2e):\n    # Parse the forward alignment\n    e2f_pairs = set()\n    for pair in e2f.strip().split():\n        src, trg = map(int, pair.split('-'))\n        e2f_pairs.add((src, trg))\n\n    # Parse the backward alignment\n    f2e_pairs = set()\n    for pair in f2e.strip().split():\n        trg, src = map(int, pair.split('-'))\n        f2e_pairs.add((src, trg))\n\n    # Initialize the symmetrized alignment set\n    symmetrized_pairs = set()\n\n    # Step 1: Add intersection alignments\n    for src, trg in e2f_pairs:\n        if (trg, src) in f2e_pairs:\n            symmetrized_pairs.add((src, trg))\n\n    # Step 2: Add neighbor alignments\n    for src, trg in e2f_pairs:\n        if (src, trg) not in symmetrized_pairs:\n            # Check for neighbors in the backward alignment\n            for (b_src, b_trg) in f2e_pairs:\n                if b_trg == trg and abs(b_src - src) == 1:\n                    symmetrized_pairs.add((src, trg))\n                if b_src == src and abs(b_trg - trg) == 1:\n                    symmetrized_pairs.add((src, trg))\n\n    # Step 3: Add additional alignments\n    for src, trg in e2f_pairs:\n        if (src, trg) not in symmetrized_pairs:\n            if (trg, src) not in f2e_pairs:\n                symmetrized_pairs.add((src, trg))\n\n    # Convert to sorted list of tuples\n    return sorted(symmetrized_pairs)", "test_code_list": [{"test_code": "import unittest\nfrom nltk.translate.gdfa import grow_diag_final_and\n\nclass TestGDFA(unittest.TestCase):\n\tdef test_from_eflomal_outputs(self):\n\t    \"\"\"\n\t        Testing GDFA with first 10 eflomal outputs from issue #1829\n\t        https://github.com/nltk/nltk/issues/1829\n\t        \"\"\"\n\t    forwards = ['0-0 1-2', '0-0 1-1',\n\t        '0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 7-8 9-9 10-10 9-11 11-12 12-13 13-14',\n\t        '0-0 1-1 1-2 2-3 3-4 4-5 4-6 5-7 6-8 8-9 9-10',\n\t        '0-0 14-1 15-2 16-3 20-5 21-6 22-7 5-8 6-9 7-10 8-11 9-12 10-13 11-14 12-15 13-16 14-17 17-18 18-19 19-20 20-21 23-22 24-23 25-24 26-25 27-27 28-28 29-29 30-30 31-31'\n\t        , '0-0 1-1 0-2 2-3', '0-0 2-2 4-4',\n\t        '0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-20'\n\t        ,\n\t        '3-0 4-1 6-2 5-3 6-4 7-5 8-6 9-7 10-8 11-9 16-10 9-12 10-13 12-14',\n\t        '1-0']\n\t    backwards = ['0-0 1-2', '0-0 1-1',\n\t        '0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 10-10 11-12 12-11 13-13',\n\t        '0-0 1-2 2-3 3-4 4-6 6-8 7-5 8-7 9-8',\n\t        '0-0 1-8 2-9 3-10 4-11 5-12 6-11 8-13 9-14 10-15 11-16 12-17 13-18 14-19 15-20 16-21 17-22 18-23 19-24 20-29 21-30 22-31 23-2 24-3 25-4 26-5 27-5 28-6 29-7 30-28 31-31'\n\t        , '0-0 1-1 2-3', '0-0 1-1 2-3 4-4',\n\t        '0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-16 21-18'\n\t        ,\n\t        '0-0 1-1 3-2 4-1 5-3 6-4 7-5 8-6 9-7 10-8 11-9 12-8 13-9 14-8 15-9 16-10'\n\t        , '1-0']\n\t    source_lens = [2, 3, 3, 15, 11, 33, 4, 6, 23, 18]\n\t    target_lens = [2, 4, 3, 16, 12, 33, 5, 6, 22, 16]\n\t    expected = [[(0, 0), (1, 2)], [(0, 0), (1, 1)], [(0, 0), (2, 1), (3, 2),\n\t        (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (10, 10), (11, 12)], [(0, 0\n\t        ), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (4, 6), (5, 7), (6, 8),\n\t        (7, 5), (8, 7), (8, 9), (9, 8), (9, 10)], [(0, 0), (1, 8), (2, 9),\n\t        (3, 10), (4, 11), (5, 8), (6, 9), (6, 11), (7, 10), (8, 11), (31, \n\t        31)], [(0, 0), (0, 2), (1, 1), (2, 3)], [(0, 0), (1, 1), (2, 2), (2,\n\t        3), (4, 4)], [(0, 0), (1, 1), (2, 3), (3, 4), (5, 5), (7, 6), (8, 7\n\t        ), (9, 8), (10, 9), (11, 10), (12, 11), (13, 12), (14, 13), (15, 14\n\t        ), (16, 16), (17, 17), (18, 18), (19, 19)], [(0, 0), (1, 1), (3, 0),\n\t        (3, 2), (4, 1), (5, 3), (6, 2), (6, 4), (7, 5), (8, 6), (9, 7), (9,\n\t        12), (10, 8), (10, 13), (11, 9), (12, 8), (12, 14), (13, 9), (14, 8\n\t        ), (15, 9), (16, 10)], [(1, 0)], [(0, 0), (1, 1), (3, 2), (4, 3), (\n\t        5, 4), (6, 5), (7, 6), (9, 10), (10, 12), (11, 13), (12, 14), (13, 15)]\n\t        ]\n\t    for fw, bw, src_len, trg_len, expect in zip(forwards, backwards,\n\t        source_lens, target_lens, expected):\n\t        self.assertListEqual(expect, grow_diag_final_and(src_len, trg_len,\n\t            fw, bw))\n\t\nTestGDFA().test_from_eflomal_outputs()\n", "code_start": "", "test_path": "/home/zhuqiming2022/workspace/srcdata/Computation/nltk/nltk/test/unit/translate/test_gdfa.py"}], "method_code_mask": "from collections import defaultdict\n\n\ndef grow_diag_final_and(srclen, trglen, e2f, f2e): [MASK]\n"}
